{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"This is ansemjo's small wiki. It contains all sorts of more or less useful information. Unless otherwise noted, the content on this site is licensed under the Creative Commons Attribution-ShareAlike 4.0 , which also applies to my website .","title":"Home"},{"location":"software.html","text":"Software Collaboration A collection of nice software that can be used for collaborative tasks, preferrably self-hosted. Name Description self-hosted Airtable Cloud-hosted Database with a beautiful UI and API integration no CodiMD Free fork of HackMD, realtime collaborative text editor similar to etherpad yes Archival An overview of archival software for different purposes. E-Mail Name Type Notes ansemjo/imapfetch Python fetches from IMAP, config via ini , archived in maildir for mutt raymii/NoPriv Python open formats, no search, combination with imapbox and calaca possible, exports to HTML tree mailpiler PHP many dependencies, only accepts via SMTP receiver Mailstore Windows App requires Windows, no scheduling in the free Home version Mailarchiva Java werid licensing model, cumbersome installation Documents Name Server Client OCR Notes mayanEDMS Docker Web yes not sure why I initially dismissed this .. beautiful, has OCR, easily installed with docker-compose .. ecodms Debian / Docker Web / Windows / Linux yes not very pretty; simple installation logicalDoc Docker Web not in CE nice feature set and relatively easy to set up; lacks OCR in community edition ambar huge docker-compose Web yes ONLY for searching; good for temporary projects seeddms LAMP stack Web ? rather simple, fewer professional features nuxeo Debian / Docker Web no very beautiful; resource intensive and not very intuitive / usable for smaller deployments agorum ? Web yes not successfully demo-ed yet; looks promising though","title":"Software"},{"location":"software.html#software","text":"","title":"Software"},{"location":"software.html#collaboration","text":"A collection of nice software that can be used for collaborative tasks, preferrably self-hosted. Name Description self-hosted Airtable Cloud-hosted Database with a beautiful UI and API integration no CodiMD Free fork of HackMD, realtime collaborative text editor similar to etherpad yes","title":"Collaboration"},{"location":"software.html#archival","text":"An overview of archival software for different purposes.","title":"Archival"},{"location":"software.html#e-mail","text":"Name Type Notes ansemjo/imapfetch Python fetches from IMAP, config via ini , archived in maildir for mutt raymii/NoPriv Python open formats, no search, combination with imapbox and calaca possible, exports to HTML tree mailpiler PHP many dependencies, only accepts via SMTP receiver Mailstore Windows App requires Windows, no scheduling in the free Home version Mailarchiva Java werid licensing model, cumbersome installation","title":"E-Mail"},{"location":"software.html#documents","text":"Name Server Client OCR Notes mayanEDMS Docker Web yes not sure why I initially dismissed this .. beautiful, has OCR, easily installed with docker-compose .. ecodms Debian / Docker Web / Windows / Linux yes not very pretty; simple installation logicalDoc Docker Web not in CE nice feature set and relatively easy to set up; lacks OCR in community edition ambar huge docker-compose Web yes ONLY for searching; good for temporary projects seeddms LAMP stack Web ? rather simple, fewer professional features nuxeo Debian / Docker Web no very beautiful; resource intensive and not very intuitive / usable for smaller deployments agorum ? Web yes not successfully demo-ed yet; looks promising though","title":"Documents"},{"location":"personal/l\u00f6schungsantrag.html","text":"DSGVO L\u00f6schungsantrag Den folgenden Text habe ich am 2018-08-10 an change.org geschickt, um mein Konto zu Schlie\u00dfen und eine vollst\u00e4ndige L\u00f6schung meiner Daten zu beantragen. Ich glaube, dass er sich gut als Vorlage f\u00fcr zuk\u00fcnftige L\u00f6schungsersuchen eignet: Sehr geehrte Damen und Herren, ich m\u00f6chte Sie hiermit auffordern alle personenbezogenen Daten zu meiner Person, die Sie gespeichert haben vollst\u00e4ndig zu l\u00f6schen (\u00a717 DSGVO) und mein Konto zu schlie\u00dfen. Sollte eine vollst\u00e4ndige L\u00f6schung aufgrund von Mindestaufbewahrungsfristen nicht m\u00f6glich sein, so widerspreche ich hiermit mindestens aber einer weiteren Verwendung meiner Daten zu Werbe- und Marktforschungszwecken und der \u00dcbermittlung an Dritte. In diesem Fall bitte ich um Auskunft \u00fcber die verbleibenden gespeicherten Daten. Bitte best\u00e4tigen Sie mir die L\u00f6schung schriftlich per E-Mail. Mit freundlichen Gr\u00fc\u00dfen, Anton Semjonov","title":"DSGVO L\u00f6schungsantrag"},{"location":"personal/l\u00f6schungsantrag.html#dsgvo-loschungsantrag","text":"Den folgenden Text habe ich am 2018-08-10 an change.org geschickt, um mein Konto zu Schlie\u00dfen und eine vollst\u00e4ndige L\u00f6schung meiner Daten zu beantragen. Ich glaube, dass er sich gut als Vorlage f\u00fcr zuk\u00fcnftige L\u00f6schungsersuchen eignet: Sehr geehrte Damen und Herren, ich m\u00f6chte Sie hiermit auffordern alle personenbezogenen Daten zu meiner Person, die Sie gespeichert haben vollst\u00e4ndig zu l\u00f6schen (\u00a717 DSGVO) und mein Konto zu schlie\u00dfen. Sollte eine vollst\u00e4ndige L\u00f6schung aufgrund von Mindestaufbewahrungsfristen nicht m\u00f6glich sein, so widerspreche ich hiermit mindestens aber einer weiteren Verwendung meiner Daten zu Werbe- und Marktforschungszwecken und der \u00dcbermittlung an Dritte. In diesem Fall bitte ich um Auskunft \u00fcber die verbleibenden gespeicherten Daten. Bitte best\u00e4tigen Sie mir die L\u00f6schung schriftlich per E-Mail. Mit freundlichen Gr\u00fc\u00dfen, Anton Semjonov","title":"DSGVO L\u00f6schungsantrag"},{"location":"personal/versicherungen.html","text":"Notizen zur privaten Haftpflichtversicherung G\u00fcnstige Tarife gibt es bspw. bei der HUK-Coburg oder deren Direktversicherungs-Tochter HUK24 . Je nach Selbstbeteiligung und Zusatzleistungen landet man dort bei 40 bis 55 \u20ac. Ein anderer, sehr gut bewerteter Versicherer ist die VHV Gruppe .","title":"Versicherungen"},{"location":"personal/versicherungen.html#notizen-zur-privaten-haftpflichtversicherung","text":"G\u00fcnstige Tarife gibt es bspw. bei der HUK-Coburg oder deren Direktversicherungs-Tochter HUK24 . Je nach Selbstbeteiligung und Zusatzleistungen landet man dort bei 40 bis 55 \u20ac. Ein anderer, sehr gut bewerteter Versicherer ist die VHV Gruppe .","title":"Notizen zur privaten Haftpflichtversicherung"},{"location":"rechenzentrum/index.html","text":"Overview \"Rechenzentrum\" is the name I gave my little deskside homelab. This section contains information on: booting systems over the network performing automated installations managing a small number of machines various general sysadmin tasks","title":"Overview"},{"location":"rechenzentrum/index.html#overview","text":"\"Rechenzentrum\" is the name I gave my little deskside homelab. This section contains information on: booting systems over the network performing automated installations managing a small number of machines various general sysadmin tasks","title":"Overview"},{"location":"rechenzentrum/bootstrap.html","text":"Note This page needs tidying up. Network Booting Combined DHCP responses Last time I changed my PXE procedure to use custom iPXE scripts and compiled iPXE binaries, I ran into the problem that the bootloop needs to be broken somehow, if you specify the iPXE binary as the boot target via DHCP and do not want to recompile your binary with new scripts embedded every time. This assumed dumb DHCP servers, which cannot react to different client classes. And while OpenWRT and the underlying dnsmasq are not exactly dumb , the necessary settings are not comfortably exposed in Luci. So I used an unused DHCP option to specify the real PXE boot target and an embedded iPXE script which reads this option and then chainloads. Whew! Today I learned that clients can assemble responses from multiple DHCP servers and dnsmasq can act as a proxy just fine, meaning it will only serve the settings relevant to PXE booting and leave all the IP assignments, DNS settings, etc. to the main DHCP server in the network. Yay! This means that the same server that shall act as the target for PXE booting (possibly even containing a gRPC-enabled matchbox ) can also serve the relevant DHCP settings so clients will use it. CoreOS containers The CoreOS team provides containers for both matchbox and dnsmasq : quay.io/coreos/matchbox quay.io/coreos/dnsmasq Together with the sample systemd service files provided in matchbox releases in the contrib/systemd/ subdirectory, these can easily be used to bootstrap a fully functional network boot target on top of CoreOS. [Unit] Description = CoreOS matchbox Server Documentation = https://github.com/coreos/matchbox [Service] Environment = \"IMAGE=quay.io/coreos/matchbox\" Environment = \"VERSION=v0.7.1\" Environment = \"MATCHBOX_ADDRESS=0.0.0.0:8080\" Environment = \"MATCHBOX_RPC_ADDRESS=0.0.0.0:8081\" Environment = \"MATCHBOX_LOG_LEVEL=debug\" ExecStartPre = /usr/bin/mkdir -p /etc/matchbox ExecStartPre = /usr/bin/mkdir -p /var/lib/matchbox/assets ExecStart = /usr/bin/rkt run \\ --net=host \\ --inherit-env \\ --trust-keys-from-https \\ --mount volume=data,target=/var/lib/matchbox \\ --mount volume=config,target=/etc/matchbox \\ --volume data,kind=host,source=/var/lib/matchbox \\ --volume config,kind=host,source=/etc/matchbox \\ ${IMAGE}:${VERSION} [Install] WantedBy = multi-user.target [Unit] Description = CoreOS dnsmasq DHCP proxy and TFTP server Documentation = https://github.com/coreos/matchbox [Service] Environment = \"IMAGE=quay.io/coreos/dnsmasq\" Environment = \"VERSION=v0.5.0\" Environment = \"NETWORK=172.26.63.1\" Environment = \"MATCHBOX=172.26.63.242:8080\" # replace with %H ? ExecStart = /usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy = multi-user.target Bootstrap Process This is an overview of the necessary procedures to bootstrap the entire Rechenzentrum\ufffd: bootstrap the network boot target 'matchbox' boot and install CoreOS to disk generate tls keys for matchbox gRPC enable systemd services for matchbox and dnsmasq optionally add custom iPXE scripts in matchbox ' assets and tweak boot option optionally download and store kernel and initramfs locally in assets configure infrastructure with terraform use providers for matchbox and vsphere / libvirt /... configure profiles for machines that bootstrap until you can ssh in terraform apply to bring them up and have them boot from pxe use ansible to do proper deployments and configuration management bonus points if you use terraform state as inventory for ansible Matchbox Boot CoreOS into RAM through your preferred method, e.g. using the CoreOS ISO or netboot.xyz . Then install to disk with coreos-install : curl - Lo install . json https : // ks . surge . sh / matchbox . json sudo coreos - install - d / dev / sda - i install . json sudo reboot Append -o vmware_raw to the installation command if you're installing on VMware and replace /dev/sda with /dev/vda if you're installing on KVM (or use scsi-virtio disks). Reboot twice for the autologin to take effect. Then add this matchbox server in your DNS. Install CentOS over Serial Cable Simply using netboot.xyz.kpxe is not sufficient when installing from network with only a serial connection (e.g. on my X10SBA) because it does not allow you to edit the kernel commandline - which does not include console=ttyS0 by default. Thus boot into an iPXE shell and use the following script to start an interactive CentOS install over serial: imgfree set repo http://mirror.23media.de/centos/7/os/x86_64 kernel ${ repo } /images/pxeboot/vmlinuz initrd ${ repo } /images/pxeboot/initrd.img imgargs vmlinuz ramdisk_size=8192 console=ttyS0,115200 text method= ${ repo } / boot Careful when copy-pasting though. I have had incomplete pastes which lead to a missing _64 in the repo URL, etc. Squid proxy Instead of mirroring multiple repositories locally, you could run a Squid proxy in your network and point all yum clients at it. With some URL rewriting you can cache the same packages from many different mirrors in a deduplicated fashion. I used the sameersbn/squid image on a CoreOS host, started with the following arguments: docker run - d --net host --name squid \\ - v / var / spool / squid : / var / spool / squid \\ - v / etc / squid : / etc / squid sameersbn / squid The configuration files cache all .rpm 's from any mirrors. This is probably too open and broad to be used as a general proxy, so be careful. /etc/squid/storeid.db : # / etc / squid / squid . conf acl safe_ports port 80 21 443 acl tls_ports port 443 acl CONNECT method CONNECT http_access deny ! safe_ports http_access deny CONNECT ! tls_ports http_access allow localhost http_access allow all http_port 3128 # cache yum / rpm downloads : # http : // ma . ttwagner . com / lazy - distro - mirrors - with - squid / # https : // serverfault . com / questions / 837291 / squid - and - caching - of - dnf - yum - downloads cache_replacement_policy heap LFUDA # least - frequently - used cache_dir aufs / var / spool / squid 20000 16 256 # 20 GB disk cache maximum_object_size 4096 MB # up to 4 GB files store_id_program / usr / lib / squid / storeid_file_rewrite / etc / squid / storeid . db # rewrite all centos mirror urls coredump_dir / var / spool / squid refresh_pattern - i \\. ( deb | rpm | tar | tar . gz | tgz ) $ 10080 90 % 43200 override - expire ignore - no - cache ignore - no - store ignore - private refresh_pattern . 0 20 % 4320 /etc/squid/storeid.db : \\ / ([ 0 - 9 \\ . \\ - ] + ) \\ / ([ a - z ] + ) \\ / ( x86_64 | i386 ) \\ / (. * \\ . d ? rpm ) http : // rpmcache . squid . internal / $ 1 / $ 2 / $ 3 / $ 4 Note: the storeid.db rules need tabs as whitespace, not spaces! And during initial creation I found out that squid sends a quoted string to the helper, so using any regular expression with (.*\\.rpm)$ at the end did not match. Use debug_options ALL,5 and grep for storeId if you're having problems. Finally, simply add proxy=http://url.to.your.proxy:3128 in your clients' /etc/yum.conf . reverse proxy With a small addition to your squid.conf you can also make Squid act as a reverse proxy (or \"accelerator\" in Squid terms) for a mirror of your choice, using the same cache . Although the wiki still says that cache_peer requests do not pass through the storeid helper, it in fact seems to do just that. At the time of this writing, the latest container uses Squid Cache: Version 3.5.27 from Ubuntu's repositories. ... http_port 3128 http_port 80 accel defaultsite = ftp . halifax . rwth - aachen . de no - vhost cache_peer ftp . halifax . rwth - aachen . de parent 80 0 no - query originserver name = mirror acl mirror dstdomain ftp . halifax . rwth - aachen . de http_access allow mirror cache_peer_access mirror allow mirror cache_peer_access mirror deny all ... With this configuration, you could also use Squid as a mirror to install your machines via kickstart, because also the .../os/x86_64/images/pxeboot/* files will be cached. You might want to amend your refresh_pattern rules to account for that.","title":"Bootstrap"},{"location":"rechenzentrum/bootstrap.html#network-booting","text":"","title":"Network Booting"},{"location":"rechenzentrum/bootstrap.html#combined-dhcp-responses","text":"Last time I changed my PXE procedure to use custom iPXE scripts and compiled iPXE binaries, I ran into the problem that the bootloop needs to be broken somehow, if you specify the iPXE binary as the boot target via DHCP and do not want to recompile your binary with new scripts embedded every time. This assumed dumb DHCP servers, which cannot react to different client classes. And while OpenWRT and the underlying dnsmasq are not exactly dumb , the necessary settings are not comfortably exposed in Luci. So I used an unused DHCP option to specify the real PXE boot target and an embedded iPXE script which reads this option and then chainloads. Whew! Today I learned that clients can assemble responses from multiple DHCP servers and dnsmasq can act as a proxy just fine, meaning it will only serve the settings relevant to PXE booting and leave all the IP assignments, DNS settings, etc. to the main DHCP server in the network. Yay! This means that the same server that shall act as the target for PXE booting (possibly even containing a gRPC-enabled matchbox ) can also serve the relevant DHCP settings so clients will use it.","title":"Combined DHCP responses"},{"location":"rechenzentrum/bootstrap.html#coreos-containers","text":"The CoreOS team provides containers for both matchbox and dnsmasq : quay.io/coreos/matchbox quay.io/coreos/dnsmasq Together with the sample systemd service files provided in matchbox releases in the contrib/systemd/ subdirectory, these can easily be used to bootstrap a fully functional network boot target on top of CoreOS. [Unit] Description = CoreOS matchbox Server Documentation = https://github.com/coreos/matchbox [Service] Environment = \"IMAGE=quay.io/coreos/matchbox\" Environment = \"VERSION=v0.7.1\" Environment = \"MATCHBOX_ADDRESS=0.0.0.0:8080\" Environment = \"MATCHBOX_RPC_ADDRESS=0.0.0.0:8081\" Environment = \"MATCHBOX_LOG_LEVEL=debug\" ExecStartPre = /usr/bin/mkdir -p /etc/matchbox ExecStartPre = /usr/bin/mkdir -p /var/lib/matchbox/assets ExecStart = /usr/bin/rkt run \\ --net=host \\ --inherit-env \\ --trust-keys-from-https \\ --mount volume=data,target=/var/lib/matchbox \\ --mount volume=config,target=/etc/matchbox \\ --volume data,kind=host,source=/var/lib/matchbox \\ --volume config,kind=host,source=/etc/matchbox \\ ${IMAGE}:${VERSION} [Install] WantedBy = multi-user.target [Unit] Description = CoreOS dnsmasq DHCP proxy and TFTP server Documentation = https://github.com/coreos/matchbox [Service] Environment = \"IMAGE=quay.io/coreos/dnsmasq\" Environment = \"VERSION=v0.5.0\" Environment = \"NETWORK=172.26.63.1\" Environment = \"MATCHBOX=172.26.63.242:8080\" # replace with %H ? ExecStart = /usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy = multi-user.target","title":"CoreOS containers"},{"location":"rechenzentrum/bootstrap.html#bootstrap-process","text":"This is an overview of the necessary procedures to bootstrap the entire Rechenzentrum\ufffd: bootstrap the network boot target 'matchbox' boot and install CoreOS to disk generate tls keys for matchbox gRPC enable systemd services for matchbox and dnsmasq optionally add custom iPXE scripts in matchbox ' assets and tweak boot option optionally download and store kernel and initramfs locally in assets configure infrastructure with terraform use providers for matchbox and vsphere / libvirt /... configure profiles for machines that bootstrap until you can ssh in terraform apply to bring them up and have them boot from pxe use ansible to do proper deployments and configuration management bonus points if you use terraform state as inventory for ansible","title":"Bootstrap Process"},{"location":"rechenzentrum/bootstrap.html#matchbox","text":"Boot CoreOS into RAM through your preferred method, e.g. using the CoreOS ISO or netboot.xyz . Then install to disk with coreos-install : curl - Lo install . json https : // ks . surge . sh / matchbox . json sudo coreos - install - d / dev / sda - i install . json sudo reboot Append -o vmware_raw to the installation command if you're installing on VMware and replace /dev/sda with /dev/vda if you're installing on KVM (or use scsi-virtio disks). Reboot twice for the autologin to take effect. Then add this matchbox server in your DNS.","title":"Matchbox"},{"location":"rechenzentrum/bootstrap.html#install-centos-over-serial-cable","text":"Simply using netboot.xyz.kpxe is not sufficient when installing from network with only a serial connection (e.g. on my X10SBA) because it does not allow you to edit the kernel commandline - which does not include console=ttyS0 by default. Thus boot into an iPXE shell and use the following script to start an interactive CentOS install over serial: imgfree set repo http://mirror.23media.de/centos/7/os/x86_64 kernel ${ repo } /images/pxeboot/vmlinuz initrd ${ repo } /images/pxeboot/initrd.img imgargs vmlinuz ramdisk_size=8192 console=ttyS0,115200 text method= ${ repo } / boot Careful when copy-pasting though. I have had incomplete pastes which lead to a missing _64 in the repo URL, etc.","title":"Install CentOS over Serial Cable"},{"location":"rechenzentrum/bootstrap.html#squid-proxy","text":"Instead of mirroring multiple repositories locally, you could run a Squid proxy in your network and point all yum clients at it. With some URL rewriting you can cache the same packages from many different mirrors in a deduplicated fashion. I used the sameersbn/squid image on a CoreOS host, started with the following arguments: docker run - d --net host --name squid \\ - v / var / spool / squid : / var / spool / squid \\ - v / etc / squid : / etc / squid sameersbn / squid The configuration files cache all .rpm 's from any mirrors. This is probably too open and broad to be used as a general proxy, so be careful. /etc/squid/storeid.db : # / etc / squid / squid . conf acl safe_ports port 80 21 443 acl tls_ports port 443 acl CONNECT method CONNECT http_access deny ! safe_ports http_access deny CONNECT ! tls_ports http_access allow localhost http_access allow all http_port 3128 # cache yum / rpm downloads : # http : // ma . ttwagner . com / lazy - distro - mirrors - with - squid / # https : // serverfault . com / questions / 837291 / squid - and - caching - of - dnf - yum - downloads cache_replacement_policy heap LFUDA # least - frequently - used cache_dir aufs / var / spool / squid 20000 16 256 # 20 GB disk cache maximum_object_size 4096 MB # up to 4 GB files store_id_program / usr / lib / squid / storeid_file_rewrite / etc / squid / storeid . db # rewrite all centos mirror urls coredump_dir / var / spool / squid refresh_pattern - i \\. ( deb | rpm | tar | tar . gz | tgz ) $ 10080 90 % 43200 override - expire ignore - no - cache ignore - no - store ignore - private refresh_pattern . 0 20 % 4320 /etc/squid/storeid.db : \\ / ([ 0 - 9 \\ . \\ - ] + ) \\ / ([ a - z ] + ) \\ / ( x86_64 | i386 ) \\ / (. * \\ . d ? rpm ) http : // rpmcache . squid . internal / $ 1 / $ 2 / $ 3 / $ 4 Note: the storeid.db rules need tabs as whitespace, not spaces! And during initial creation I found out that squid sends a quoted string to the helper, so using any regular expression with (.*\\.rpm)$ at the end did not match. Use debug_options ALL,5 and grep for storeId if you're having problems. Finally, simply add proxy=http://url.to.your.proxy:3128 in your clients' /etc/yum.conf .","title":"Squid proxy"},{"location":"rechenzentrum/bootstrap.html#reverse-proxy","text":"With a small addition to your squid.conf you can also make Squid act as a reverse proxy (or \"accelerator\" in Squid terms) for a mirror of your choice, using the same cache . Although the wiki still says that cache_peer requests do not pass through the storeid helper, it in fact seems to do just that. At the time of this writing, the latest container uses Squid Cache: Version 3.5.27 from Ubuntu's repositories. ... http_port 3128 http_port 80 accel defaultsite = ftp . halifax . rwth - aachen . de no - vhost cache_peer ftp . halifax . rwth - aachen . de parent 80 0 no - query originserver name = mirror acl mirror dstdomain ftp . halifax . rwth - aachen . de http_access allow mirror cache_peer_access mirror allow mirror cache_peer_access mirror deny all ... With this configuration, you could also use Squid as a mirror to install your machines via kickstart, because also the .../os/x86_64/images/pxeboot/* files will be cached. You might want to amend your refresh_pattern rules to account for that.","title":"reverse proxy"},{"location":"rechenzentrum/docker-in-kvm.html","text":"Docker in QEMU/KVM Some applications may require a properly isolated Docker engine where users of the API have every freedom but when they must not be able to compromise the host security. Since access to the Docker socket is equivalent to being root ( or worse ) we must preferably run the engine on a seperate machine. Long story short: virtualization with QEMU/KVM provides all the required isolation and CoreOS is easy to deploy and bundles Docker by default. The following steps are designed for a CentOS 7.6 hypervisor. Prerequisites First of all, we need to prepare our hypervisor, so install QEMU and libvirt. yum install qemu - kvm libvirt modprobe kvm systemctl enable --now libvirtd Info We are going to use virt-install as well, however the version in EPEL is not recent enough to use the kernel= and initrd= arguments with --location . Thus prefer a local manager and append --connect qemu+ssh://root@hypervisor/system to virsh or virt-install commands. Boot a Virtual Machine There is a guide on how to boot CoreOS with libvirt but I prefer to perform a clean installation to disk. Therefore we need to boot CoreOS to RAM and deploy using an Ignition configuration. Via Text Console If you don't want to bother with VNC connections and would prefer to install via a text console on the hypervisor itself, you can download and run the CoreOS vmlinuz and cpio.gz directly: virt-install --name runner --memory 4096 --vcpus 4 \\ --disk size = 20 ,bus = virtio --hvm --rng /dev/urandom \\ --autostart --nographics --console pty,target_type = virtio \\ --location \"https://stable.release.core-os.net/amd64-usr/current/,kernel=coreos_production_pxe.vmlinuz,initrd=coreos_production_pxe_image.cpio.gz\" \\ --extra-args \"coreos.autologin console=ttyS0\" \\ --os-variant virtio26 Note Remember to append a --connect string if you are connecting to a remote libvirt socket. If you prefer to download and verify an ISO locally instead, you can substitute: --location /tmp/coreos.iso,kernel=/coreos/vmlinuz,initrd=/coreos/cpio.gz \\ Hint You can find files inside an ISO with isoinfo -Jf -i /path/to/iso . Via VNC Viewer Apart from using VNC, we are going to use netboot.xyt in this approach. This is possible with virt-install version 1.5.0 on CentOS 7. First, download the netboot image: cd / var / lib / libvirt / boot curl - LO https : // boot . netboot . xyz / ipxe / netboot . xyz . iso Now create the virtual machine with virt-install : virt - install --name runner --memory 4096 --vcpus 4 \\ --disk size=20,bus=virtio --hvm --rng /dev/urandom \\ --autostart --cdrom /var/lib/libvirt/boot/netboot.xyz.iso \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --os-variant virtio26 This should start the installation process and enable a VNC console. You can check the port with virsh vncdisplay runner and verify with ss -tln . In my case :0 corresponds to port 5900 on the host, so temporarily open that port in the firewall: firewall - cmd --add-port 5900/tcp Connect with your favourite VNC client and complete the installation. Hint You can't currently change the keyboard map on the console. Set a password with sudo passwd core and connect with ssh instead if you run into problems. Install CoreOS to Disk Prepare Ignition By now you should have prepared an Ignition configuration. There is of course a lot of variation possible here but most importantly you should enable rngd.service and docker.service and make sure that you can connect with SSH public keys. Mine looks somewhat like this: --- # enable docker service systemd : units : - name : rngd.service enabled : yes - name : docker.service enabled : yes # ssh public keys passwd : users : - name : core ssh_authorized_keys : - # add your keys here # automatic updates during maintenance window locksmith : reboot_strategy : reboot window_start : 04:00 window_length : 3h # enable console autologin storage : filesystems : - name : OEM mount : device : /dev/disk/by-label/OEM format : ext4 files : - filesystem : OEM path : /grub.cfg mode : 0644 append : true contents : inline : | set linux_append=\"$linux_append coreos.autologin\" Installation After transpiling, I am using surge.sh to host small static files quickly. Download the configuration and finally install CoreOS to disk: curl -LO \"https://ks.surge.sh/coreos/docker.json\" sudo coreos-install -d /dev/vda -i docker.json sudo udevadm settle sudo reboot Miscellaneous Fixed DHCP Address You can add a fixed address for this virtual machine by creating an IP assignment for its MAC address with virsh : virsh net-dhcp-leases default # see current leases virsh net-update default add-last ip-dhcp-host \\ --xml \"<host mac='52:54:00:e7:b6:4d' ip='192.168.122.2' />\" \\ --live --config SSH Client Configuration Add an appropriate SSH config on the hypervisor: Host runner User core HostName 192 . 168 . 122 . 2 StrictHostKeyChecking no UserKnownHostsFile / dev / null","title":"Docker in QEMU/KVM"},{"location":"rechenzentrum/docker-in-kvm.html#docker-in-qemukvm","text":"Some applications may require a properly isolated Docker engine where users of the API have every freedom but when they must not be able to compromise the host security. Since access to the Docker socket is equivalent to being root ( or worse ) we must preferably run the engine on a seperate machine. Long story short: virtualization with QEMU/KVM provides all the required isolation and CoreOS is easy to deploy and bundles Docker by default. The following steps are designed for a CentOS 7.6 hypervisor.","title":"Docker in QEMU/KVM"},{"location":"rechenzentrum/docker-in-kvm.html#prerequisites","text":"First of all, we need to prepare our hypervisor, so install QEMU and libvirt. yum install qemu - kvm libvirt modprobe kvm systemctl enable --now libvirtd Info We are going to use virt-install as well, however the version in EPEL is not recent enough to use the kernel= and initrd= arguments with --location . Thus prefer a local manager and append --connect qemu+ssh://root@hypervisor/system to virsh or virt-install commands.","title":"Prerequisites"},{"location":"rechenzentrum/docker-in-kvm.html#boot-a-virtual-machine","text":"There is a guide on how to boot CoreOS with libvirt but I prefer to perform a clean installation to disk. Therefore we need to boot CoreOS to RAM and deploy using an Ignition configuration.","title":"Boot a Virtual Machine"},{"location":"rechenzentrum/docker-in-kvm.html#via-text-console","text":"If you don't want to bother with VNC connections and would prefer to install via a text console on the hypervisor itself, you can download and run the CoreOS vmlinuz and cpio.gz directly: virt-install --name runner --memory 4096 --vcpus 4 \\ --disk size = 20 ,bus = virtio --hvm --rng /dev/urandom \\ --autostart --nographics --console pty,target_type = virtio \\ --location \"https://stable.release.core-os.net/amd64-usr/current/,kernel=coreos_production_pxe.vmlinuz,initrd=coreos_production_pxe_image.cpio.gz\" \\ --extra-args \"coreos.autologin console=ttyS0\" \\ --os-variant virtio26 Note Remember to append a --connect string if you are connecting to a remote libvirt socket. If you prefer to download and verify an ISO locally instead, you can substitute: --location /tmp/coreos.iso,kernel=/coreos/vmlinuz,initrd=/coreos/cpio.gz \\ Hint You can find files inside an ISO with isoinfo -Jf -i /path/to/iso .","title":"Via Text Console"},{"location":"rechenzentrum/docker-in-kvm.html#via-vnc-viewer","text":"Apart from using VNC, we are going to use netboot.xyt in this approach. This is possible with virt-install version 1.5.0 on CentOS 7. First, download the netboot image: cd / var / lib / libvirt / boot curl - LO https : // boot . netboot . xyz / ipxe / netboot . xyz . iso Now create the virtual machine with virt-install : virt - install --name runner --memory 4096 --vcpus 4 \\ --disk size=20,bus=virtio --hvm --rng /dev/urandom \\ --autostart --cdrom /var/lib/libvirt/boot/netboot.xyz.iso \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --os-variant virtio26 This should start the installation process and enable a VNC console. You can check the port with virsh vncdisplay runner and verify with ss -tln . In my case :0 corresponds to port 5900 on the host, so temporarily open that port in the firewall: firewall - cmd --add-port 5900/tcp Connect with your favourite VNC client and complete the installation. Hint You can't currently change the keyboard map on the console. Set a password with sudo passwd core and connect with ssh instead if you run into problems.","title":"Via VNC Viewer"},{"location":"rechenzentrum/docker-in-kvm.html#install-coreos-to-disk","text":"","title":"Install CoreOS to Disk"},{"location":"rechenzentrum/docker-in-kvm.html#prepare-ignition","text":"By now you should have prepared an Ignition configuration. There is of course a lot of variation possible here but most importantly you should enable rngd.service and docker.service and make sure that you can connect with SSH public keys. Mine looks somewhat like this: --- # enable docker service systemd : units : - name : rngd.service enabled : yes - name : docker.service enabled : yes # ssh public keys passwd : users : - name : core ssh_authorized_keys : - # add your keys here # automatic updates during maintenance window locksmith : reboot_strategy : reboot window_start : 04:00 window_length : 3h # enable console autologin storage : filesystems : - name : OEM mount : device : /dev/disk/by-label/OEM format : ext4 files : - filesystem : OEM path : /grub.cfg mode : 0644 append : true contents : inline : | set linux_append=\"$linux_append coreos.autologin\"","title":"Prepare Ignition"},{"location":"rechenzentrum/docker-in-kvm.html#installation","text":"After transpiling, I am using surge.sh to host small static files quickly. Download the configuration and finally install CoreOS to disk: curl -LO \"https://ks.surge.sh/coreos/docker.json\" sudo coreos-install -d /dev/vda -i docker.json sudo udevadm settle sudo reboot","title":"Installation"},{"location":"rechenzentrum/docker-in-kvm.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"rechenzentrum/docker-in-kvm.html#fixed-dhcp-address","text":"You can add a fixed address for this virtual machine by creating an IP assignment for its MAC address with virsh : virsh net-dhcp-leases default # see current leases virsh net-update default add-last ip-dhcp-host \\ --xml \"<host mac='52:54:00:e7:b6:4d' ip='192.168.122.2' />\" \\ --live --config","title":"Fixed DHCP Address"},{"location":"rechenzentrum/docker-in-kvm.html#ssh-client-configuration","text":"Add an appropriate SSH config on the hypervisor: Host runner User core HostName 192 . 168 . 122 . 2 StrictHostKeyChecking no UserKnownHostsFile / dev / null","title":"SSH Client Configuration"},{"location":"rechenzentrum/flynn.html","text":"Installing Flynn on Debian Flynn is mostly compatible with Debian. Some packages require the contrib repository though. Installing a single-node Flynn \"cluster\" is as easy as: Perform a clean Debian 9 Stretch installation. Download the official script from dl.flynn.io/install-flynn Patch the function is_ubuntu_xenial() to also check for Debian GNU/Linux 9 . Enable or add the contrib repository in /etc/apt/sources.list . Run the install-flynn script. Start and enable flynn-host.service . Export CLUSTER_DOMAIN to the appropriate FQDN. Apps will be $app.CLUSTER_DOMAIN . Bootstrap with flynn-host bootstrap .","title":"Flynn"},{"location":"rechenzentrum/flynn.html#installing-flynn-on-debian","text":"Flynn is mostly compatible with Debian. Some packages require the contrib repository though. Installing a single-node Flynn \"cluster\" is as easy as: Perform a clean Debian 9 Stretch installation. Download the official script from dl.flynn.io/install-flynn Patch the function is_ubuntu_xenial() to also check for Debian GNU/Linux 9 . Enable or add the contrib repository in /etc/apt/sources.list . Run the install-flynn script. Start and enable flynn-host.service . Export CLUSTER_DOMAIN to the appropriate FQDN. Apps will be $app.CLUSTER_DOMAIN . Bootstrap with flynn-host bootstrap .","title":"Installing Flynn on Debian"},{"location":"rechenzentrum/kvm-on-alpine.html","text":"KVM on Alpine Installing a KVM hypervisor with absolutely minimal footprint: Install Alpine Install Alpine Linux by booting from ISO or via netboot and running setup-alpine , choosing sys as the disktype. Packages Install KVM packages: apk add qemu - system - x86_64 libvirt libvirt - daemon dbus polkit qemu - img Load Modules Reboot or just load necessary kernel modules: modprobe kvm - intel br_netfilter Hint br_netfilter is required for the network bridge below. Bridge Interface Add a bridge configuration in /etc/network/interfaces : auto lo iface lo inet loopback auto br0 iface br0 inet dhcp pre - up modprobe br_netfilter pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - arptables pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - iptables pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - ip6tables bridge_ports eth0","title":"KVM on Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#kvm-on-alpine","text":"Installing a KVM hypervisor with absolutely minimal footprint:","title":"KVM on Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#install-alpine","text":"Install Alpine Linux by booting from ISO or via netboot and running setup-alpine , choosing sys as the disktype.","title":"Install Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#packages","text":"Install KVM packages: apk add qemu - system - x86_64 libvirt libvirt - daemon dbus polkit qemu - img","title":"Packages"},{"location":"rechenzentrum/kvm-on-alpine.html#load-modules","text":"Reboot or just load necessary kernel modules: modprobe kvm - intel br_netfilter Hint br_netfilter is required for the network bridge below.","title":"Load Modules"},{"location":"rechenzentrum/kvm-on-alpine.html#bridge-interface","text":"Add a bridge configuration in /etc/network/interfaces : auto lo iface lo inet loopback auto br0 iface br0 inet dhcp pre - up modprobe br_netfilter pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - arptables pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - iptables pre - up echo 0 > / proc / sys / net / bridge / bridge - nf - call - ip6tables bridge_ports eth0","title":"Bridge Interface"},{"location":"rechenzentrum/netboot.html","text":"Network Boot Todo This page is a work in progress. See the coreos dnsmasq image for details on how to create a simple dnsmasq container. The service that is started with rkt on my matchbox host is: # from : coreos / matchbox --> contrib / systemd / matchbox - for - tectonic . service # from : coreos / matchbox --> contrib / dnsmasq / README . md [ Unit ] Description = CoreOS dnsmasq DHCP proxy and TFTP server Documentation = https : // github . com / coreos / matchbox [ Service ] Environment = \" IMAGE=quay.io/coreos/dnsmasq \" Environment = \" VERSION=v0.5.0 \" Environment = \" NETWORK=172.26.63.1 \" Environment = \" MATCHBOX=%H:8080 \" ExecStart =/ usr / bin / rkt run \\ -- net = host \\ -- trust - keys - from - https \\ ${ IMAGE }:${ VERSION } \\ -- caps - retain = CAP_NET_ADMIN , CAP_NET_BIND_SERVICE , CAP_SETGID , CAP_SETUID , CAP_NET_RAW \\ -- - d - q \\ -- dhcp - range = ${ NETWORK }, proxy , 255 . 255 . 255 . 0 \\ -- enable - tftp -- tftp - root =/ var / lib / tftpboot \\ -- dhcp - userclass = set : ipxe , iPXE \\ -- pxe - service = tag :# ipxe , x86PC , \" PXE chainload to iPXE \" , undionly . kpxe \\ -- pxe - service = tag : ipxe , x86PC , \" iPXE \" , http : // ${ MATCHBOX } / boot . ipxe \\ -- log - queries \\ -- log - dhcp [ Install ] WantedBy = multi - user . target Important bits are probably --net host and the userclass , pxe-service and dhcp-range stuff in dnsmasq's options. I also need to build iPXE binaries: undionly.kpxe (bios -> ipxe) ipxe.efi (uefi -> ipxe) With the two files above present in /var/tftp the following seems to work: dnsmasq - d - q --port 0 \\ --dhcp-range=172.26.63.0,proxy --enable-tftp --tftp-root=/var/tftp \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"chainload bios --> ipxe\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,BC_EFI,\"chainload bc_efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,BC_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,x86-64_EFI,\"chainload efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,x86-64_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe Hint Add --log-dhcp to get more verbose information about served DHCP requests.","title":"Network Boot"},{"location":"rechenzentrum/netboot.html#network-boot","text":"Todo This page is a work in progress. See the coreos dnsmasq image for details on how to create a simple dnsmasq container. The service that is started with rkt on my matchbox host is: # from : coreos / matchbox --> contrib / systemd / matchbox - for - tectonic . service # from : coreos / matchbox --> contrib / dnsmasq / README . md [ Unit ] Description = CoreOS dnsmasq DHCP proxy and TFTP server Documentation = https : // github . com / coreos / matchbox [ Service ] Environment = \" IMAGE=quay.io/coreos/dnsmasq \" Environment = \" VERSION=v0.5.0 \" Environment = \" NETWORK=172.26.63.1 \" Environment = \" MATCHBOX=%H:8080 \" ExecStart =/ usr / bin / rkt run \\ -- net = host \\ -- trust - keys - from - https \\ ${ IMAGE }:${ VERSION } \\ -- caps - retain = CAP_NET_ADMIN , CAP_NET_BIND_SERVICE , CAP_SETGID , CAP_SETUID , CAP_NET_RAW \\ -- - d - q \\ -- dhcp - range = ${ NETWORK }, proxy , 255 . 255 . 255 . 0 \\ -- enable - tftp -- tftp - root =/ var / lib / tftpboot \\ -- dhcp - userclass = set : ipxe , iPXE \\ -- pxe - service = tag :# ipxe , x86PC , \" PXE chainload to iPXE \" , undionly . kpxe \\ -- pxe - service = tag : ipxe , x86PC , \" iPXE \" , http : // ${ MATCHBOX } / boot . ipxe \\ -- log - queries \\ -- log - dhcp [ Install ] WantedBy = multi - user . target Important bits are probably --net host and the userclass , pxe-service and dhcp-range stuff in dnsmasq's options. I also need to build iPXE binaries: undionly.kpxe (bios -> ipxe) ipxe.efi (uefi -> ipxe) With the two files above present in /var/tftp the following seems to work: dnsmasq - d - q --port 0 \\ --dhcp-range=172.26.63.0,proxy --enable-tftp --tftp-root=/var/tftp \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"chainload bios --> ipxe\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,BC_EFI,\"chainload bc_efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,BC_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,x86-64_EFI,\"chainload efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,x86-64_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe Hint Add --log-dhcp to get more verbose information about served DHCP requests.","title":"Network Boot"},{"location":"rechenzentrum/tls-host-aliases.html","text":"TLS for Host Aliases You can request certificates for host aliases without creating a seperate host entry with no enrolled machine. Minimum version According to this discussion at least FreeIPA version 4.5 is required. Add Principal Alias In FreeIPA on the Identity > Services page add a HTTP service for an actual, enrolled host. Edit that service and add a principal alias: Service Settings Principal alias HTTP / ifrit . rz . semjonov . de @RZ . SEMJONOV . DE [ Delete ] HTTP / s3 . rz . semjonov . de @RZ . SEMJONOV . DE [ Delete ] [ Add ] Request Certificate Request the certificate for the alias with ipa-getcert : $ export S3 = s3.rz.semjonov.de $ export tls = /etc/pki/tls $ ipa-getcert request \\ -k $tls /private/ $S3 .key \\ -f $tls /certs/ $S3 .crt \\ -D $S3 \\ -N CN = ifrit.rz.semjonov.de \\ -K HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE \\ -I $S3 $ ipa-getcert list Number of certificates and requests being tracked: 1 . Request ID 's3.rz.semjonov.de' : [ ... ] subject: CN = ifrit.rz.semjonov.de,OU = Rechenzentrum,O = rz.semjonov.de,C = DE expires: 2020 -07-26 22 :04:55 UTC dns: s3.rz.semjonov.de,ifrit.rz.semjonov.de principal name: HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [ ... ]","title":"TLS for Host Aliases"},{"location":"rechenzentrum/tls-host-aliases.html#tls-for-host-aliases","text":"You can request certificates for host aliases without creating a seperate host entry with no enrolled machine. Minimum version According to this discussion at least FreeIPA version 4.5 is required.","title":"TLS for Host Aliases"},{"location":"rechenzentrum/tls-host-aliases.html#add-principal-alias","text":"In FreeIPA on the Identity > Services page add a HTTP service for an actual, enrolled host. Edit that service and add a principal alias: Service Settings Principal alias HTTP / ifrit . rz . semjonov . de @RZ . SEMJONOV . DE [ Delete ] HTTP / s3 . rz . semjonov . de @RZ . SEMJONOV . DE [ Delete ] [ Add ]","title":"Add Principal Alias"},{"location":"rechenzentrum/tls-host-aliases.html#request-certificate","text":"Request the certificate for the alias with ipa-getcert : $ export S3 = s3.rz.semjonov.de $ export tls = /etc/pki/tls $ ipa-getcert request \\ -k $tls /private/ $S3 .key \\ -f $tls /certs/ $S3 .crt \\ -D $S3 \\ -N CN = ifrit.rz.semjonov.de \\ -K HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE \\ -I $S3 $ ipa-getcert list Number of certificates and requests being tracked: 1 . Request ID 's3.rz.semjonov.de' : [ ... ] subject: CN = ifrit.rz.semjonov.de,OU = Rechenzentrum,O = rz.semjonov.de,C = DE expires: 2020 -07-26 22 :04:55 UTC dns: s3.rz.semjonov.de,ifrit.rz.semjonov.de principal name: HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [ ... ]","title":"Request Certificate"},{"location":"security/net-disk-decryption.html","text":"Network Disk Decryption Recently RedHat 7.4 introduced the possibility to bind your encrypted disks to a network presence. It is called Network-Bound Disk Encryption and uses the projects tang and clevis . In essence, an encrypted payload is transformed with some ECDH key exchange magic with the tang server and the disk is decrypted automatically. If however the tang server is unavailable, this method fails and you must fall back to manually entering a passphrase. I decided this is a nice addition to my systemd decryption target , so here's how I implemented it: tang server First of all, install a release of the tang server. There's a package for CentOS and a package in the AUR for Arch Linux. There are probably others, too. But it shouldn't be too hard to build it yourself either. yum install tang I didn't like tang running on port 80 by default, so I changed it with systemctl edit tangd.socket : [Socket] ListenStream = ListenStream = 51653 Start and enable the service. Make sure to open the firewall on that port. systemctl enable --now tangd.socket You can now make sure that a key is present and print the public key: tang - show - keys 51653 4 yWvhO4ZthpAGHDmdMn78Pe2Bg0 clevis client Now for the client part on my fileserver. There is a nice post on the RedHat blog describing the entire procedure. I'm assuming you already have some encrypted disks that you want to set up for network-bound decryption. First, install clevis: yum install clevis clevis - luks And make sure that you can reach your tang server: curl - f tang . yourdomain : 51653 / adv | jq . Now we bind a secret to this tang server and add it as a new key on our LUKS disks. This will use the luksmeta storage, so you might want to take header backups on all disks to avoid data loss: cryptsetup luksHeaderBackup / dev / disk / by - id / ata - ... --header-backup-file ... Then bind the disks to the tang server: clevis luks bind - d / dev / disk / by - id / ata - ... \\ tang ' {\"url\":\"http://tang.yourdomain:51653\"} ' The advertisement contains the following signing keys : 4 yWvhO4ZthpAGHDmdMn78Pe2Bg0 Do you wish to trust these keys ? [ ynYN ] y ... Make sure the key matches the tang-show-keys output above! You'll be asked to initialize the LUKS metadata storage and must then enter an existing passphrase to add the newly bound secret as a new encryption key to your disk. I didn't bother to setup automatic decryption on boot since I already have a semi-automatic decryption environment in place with my systemd decryption target. I'm fine with decrypting disks with clevis manually: clevis luks unlock - d / dev / disk / by - id / ata - ... - n mappername However, I automated this for all four disks in my array with a small script, which reads the disks and names from /etc/crypttab and then starts continue.service : 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/sh # unlock disks with tang and clevis echo \"+ unlock disks\" while read name disk opts ; do echo \" $disk \" clevis luks unlock -d \" $disk \" -n \" $name \" done < /etc/crypttab # continue system startup sleep 2 echo \"+ continue startup\" systemctl start continue .service","title":"Network Disk Decryption"},{"location":"security/net-disk-decryption.html#network-disk-decryption","text":"Recently RedHat 7.4 introduced the possibility to bind your encrypted disks to a network presence. It is called Network-Bound Disk Encryption and uses the projects tang and clevis . In essence, an encrypted payload is transformed with some ECDH key exchange magic with the tang server and the disk is decrypted automatically. If however the tang server is unavailable, this method fails and you must fall back to manually entering a passphrase. I decided this is a nice addition to my systemd decryption target , so here's how I implemented it:","title":"Network Disk Decryption"},{"location":"security/net-disk-decryption.html#tang-server","text":"First of all, install a release of the tang server. There's a package for CentOS and a package in the AUR for Arch Linux. There are probably others, too. But it shouldn't be too hard to build it yourself either. yum install tang I didn't like tang running on port 80 by default, so I changed it with systemctl edit tangd.socket : [Socket] ListenStream = ListenStream = 51653 Start and enable the service. Make sure to open the firewall on that port. systemctl enable --now tangd.socket You can now make sure that a key is present and print the public key: tang - show - keys 51653 4 yWvhO4ZthpAGHDmdMn78Pe2Bg0","title":"tang server"},{"location":"security/net-disk-decryption.html#clevis-client","text":"Now for the client part on my fileserver. There is a nice post on the RedHat blog describing the entire procedure. I'm assuming you already have some encrypted disks that you want to set up for network-bound decryption. First, install clevis: yum install clevis clevis - luks And make sure that you can reach your tang server: curl - f tang . yourdomain : 51653 / adv | jq . Now we bind a secret to this tang server and add it as a new key on our LUKS disks. This will use the luksmeta storage, so you might want to take header backups on all disks to avoid data loss: cryptsetup luksHeaderBackup / dev / disk / by - id / ata - ... --header-backup-file ... Then bind the disks to the tang server: clevis luks bind - d / dev / disk / by - id / ata - ... \\ tang ' {\"url\":\"http://tang.yourdomain:51653\"} ' The advertisement contains the following signing keys : 4 yWvhO4ZthpAGHDmdMn78Pe2Bg0 Do you wish to trust these keys ? [ ynYN ] y ... Make sure the key matches the tang-show-keys output above! You'll be asked to initialize the LUKS metadata storage and must then enter an existing passphrase to add the newly bound secret as a new encryption key to your disk. I didn't bother to setup automatic decryption on boot since I already have a semi-automatic decryption environment in place with my systemd decryption target. I'm fine with decrypting disks with clevis manually: clevis luks unlock - d / dev / disk / by - id / ata - ... - n mappername However, I automated this for all four disks in my array with a small script, which reads the disks and names from /etc/crypttab and then starts continue.service : 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/sh # unlock disks with tang and clevis echo \"+ unlock disks\" while read name disk opts ; do echo \" $disk \" clevis luks unlock -d \" $disk \" -n \" $name \" done < /etc/crypttab # continue system startup sleep 2 echo \"+ continue startup\" systemctl start continue .service","title":"clevis client"},{"location":"security/secureboot.html","text":"Secureboot These are guides to install your system with your own secureboot keys and enforce signed Linux kernels. Tools Some useful tools for this job: name description ansemjo/mkefikeys generate signing keys ansemjo/mksignkernels bundle and sign kernel images Guides Arch Linux TODO Fedora Installation Do a somewhat standard installation on an UEFI system. I used the Fedora 28 Server netinst image. During partitioning, make sure to select at least \"custom\" partitioning and add a seperate EFI system partition in /boot/efi . Tick the boxes to encrypt your / and any swap partitions you might create. Technically, a seperate /boot partition is not required with the bundled kernels we are going to use but Anaconda complains otherwise and you will not be able to boot the system after installation. You could probably do all these steps from within a live rescue system but I haven't tried that route yet. Required Packages You will need to additionally (after a minimal setup) install: git make binutils sbsigntools Then clone and install the above two tools: mkefikeys and mksignkernels . cd / tmp / ... git clone https : // github . com / ansemjo / mkefikeys git clone https : // github . com / ansemjo / mksignkernels ( cd mkefikeys & amp ; & amp ; make - f install . mk install ) ( cd mksignkernels & amp ; & amp ; make - f install . mk install ) Signing keys Create a set of signing keys in /etc/efikeys : mkdir / etc / efikeys & amp ; & amp ; cd / etc / efikeys mkefikeys certs der The der target is required to output DER-encoded certificates in case you need to install those in your firmware. This is the case for OVMF, i.e. KVM machines. My Thinkpad needs authenticated \"efi signature lists\" .. generate them with mkefikeys auth . Copy files required for installation to the unencrypted ESP: cp / etc / efikeys /*.cer /boot/efi Installing them in your firmware is out of the scope of this entry. Warning Do not enable Secureboot yet. We haven't signed anything yet and your system will fail to boot. Sign your kernels Now we need to sign the kernels. Simply running mksignkernels will probably fail with a not-so-useful error message because something will be missing. On virtual machines the Intel microcode is usually not useful and thus not present. Add an empty MICROCODE = line in /etc/mksignkernels.mk . Additionally, you'll want to use the same kernel commandline as is used for your default installation. You can get the commandline of currently running kernel from cat /proc/cmdline . # blablabla # ------- custom targets -------- MICROCODE = CMDLINE = whatever_your_default_kernel_uses Next, we need to create the output directory: mkdir / boot / efi / EFI / Linux Running mksignkernels should succeed now. Otherwise check all the prerequisites: EFI stub in /usr/lib/systemd/boot/efi/linuxx64.efi.stub Signing keys in /etc/efikeys/DatabaseKey.{key,crt} Kernel in /boot/vmlinuz-* Initramfs in corresponding /boot/initramfs-*.img Use systemd-boot Check that systemd-boot is installed and you are indeed running UEFI, yadda yadda .. bootctl status To install it as the default bootloader, simply issue: bootctl install To enable the selection prompt uncomment the timeout in /boot/efi/loader/loader.conf . Otherwise it directly boots the default kernel. Sign your bootloader Before you reboot and attempt to enable Secureboot now, you need to sign the bootloader itself: mksignkernels sign SIGN =/ boot / efi / systemd / systemd - bootx64 . efi mksignkernels sign SIGN =/ boot / efi / BOOT / BOOTX64 . EFI I actually do not know if both are necessary, I just signed both just in case. Reboot When you reboot you should see systemd-boot's selection prompt instead of GRUB. If that is the case, there should be an option to \"Reboot into Firmware Setup\". Do that and enable Secureboot now. If all went fine you should be able to normally boot your system now. Starting your machine via GRUB should fail though, as neither GRUB nor any of the kernels it tries to boot are signed.","title":"Secureboot"},{"location":"security/secureboot.html#secureboot","text":"These are guides to install your system with your own secureboot keys and enforce signed Linux kernels.","title":"Secureboot"},{"location":"security/secureboot.html#tools","text":"Some useful tools for this job: name description ansemjo/mkefikeys generate signing keys ansemjo/mksignkernels bundle and sign kernel images","title":"Tools"},{"location":"security/secureboot.html#guides","text":"","title":"Guides"},{"location":"security/secureboot.html#arch-linux","text":"TODO","title":"Arch Linux"},{"location":"security/secureboot.html#fedora","text":"","title":"Fedora"},{"location":"security/secureboot.html#installation","text":"Do a somewhat standard installation on an UEFI system. I used the Fedora 28 Server netinst image. During partitioning, make sure to select at least \"custom\" partitioning and add a seperate EFI system partition in /boot/efi . Tick the boxes to encrypt your / and any swap partitions you might create. Technically, a seperate /boot partition is not required with the bundled kernels we are going to use but Anaconda complains otherwise and you will not be able to boot the system after installation. You could probably do all these steps from within a live rescue system but I haven't tried that route yet.","title":"Installation"},{"location":"security/secureboot.html#required-packages","text":"You will need to additionally (after a minimal setup) install: git make binutils sbsigntools Then clone and install the above two tools: mkefikeys and mksignkernels . cd / tmp / ... git clone https : // github . com / ansemjo / mkefikeys git clone https : // github . com / ansemjo / mksignkernels ( cd mkefikeys & amp ; & amp ; make - f install . mk install ) ( cd mksignkernels & amp ; & amp ; make - f install . mk install )","title":"Required Packages"},{"location":"security/secureboot.html#signing-keys","text":"Create a set of signing keys in /etc/efikeys : mkdir / etc / efikeys & amp ; & amp ; cd / etc / efikeys mkefikeys certs der The der target is required to output DER-encoded certificates in case you need to install those in your firmware. This is the case for OVMF, i.e. KVM machines. My Thinkpad needs authenticated \"efi signature lists\" .. generate them with mkefikeys auth . Copy files required for installation to the unencrypted ESP: cp / etc / efikeys /*.cer /boot/efi Installing them in your firmware is out of the scope of this entry. Warning Do not enable Secureboot yet. We haven't signed anything yet and your system will fail to boot.","title":"Signing keys"},{"location":"security/secureboot.html#sign-your-kernels","text":"Now we need to sign the kernels. Simply running mksignkernels will probably fail with a not-so-useful error message because something will be missing. On virtual machines the Intel microcode is usually not useful and thus not present. Add an empty MICROCODE = line in /etc/mksignkernels.mk . Additionally, you'll want to use the same kernel commandline as is used for your default installation. You can get the commandline of currently running kernel from cat /proc/cmdline . # blablabla # ------- custom targets -------- MICROCODE = CMDLINE = whatever_your_default_kernel_uses Next, we need to create the output directory: mkdir / boot / efi / EFI / Linux Running mksignkernels should succeed now. Otherwise check all the prerequisites: EFI stub in /usr/lib/systemd/boot/efi/linuxx64.efi.stub Signing keys in /etc/efikeys/DatabaseKey.{key,crt} Kernel in /boot/vmlinuz-* Initramfs in corresponding /boot/initramfs-*.img","title":"Sign your kernels"},{"location":"security/secureboot.html#use-systemd-boot","text":"Check that systemd-boot is installed and you are indeed running UEFI, yadda yadda .. bootctl status To install it as the default bootloader, simply issue: bootctl install To enable the selection prompt uncomment the timeout in /boot/efi/loader/loader.conf . Otherwise it directly boots the default kernel.","title":"Use systemd-boot"},{"location":"security/secureboot.html#sign-your-bootloader","text":"Before you reboot and attempt to enable Secureboot now, you need to sign the bootloader itself: mksignkernels sign SIGN =/ boot / efi / systemd / systemd - bootx64 . efi mksignkernels sign SIGN =/ boot / efi / BOOT / BOOTX64 . EFI I actually do not know if both are necessary, I just signed both just in case.","title":"Sign your bootloader"},{"location":"security/secureboot.html#reboot","text":"When you reboot you should see systemd-boot's selection prompt instead of GRUB. If that is the case, there should be an option to \"Reboot into Firmware Setup\". Do that and enable Secureboot now. If all went fine you should be able to normally boot your system now. Starting your machine via GRUB should fail though, as neither GRUB nor any of the kernels it tries to boot are signed.","title":"Reboot"},{"location":"security/systemd-decryption-target.html","text":"Systemd Disk Decryption Target In order to delay most of your systems services during boot until a bunch of harddisks are decrypted but still bring up enough to allow for remote unlocking via ssh you'll need to use a systemd.target . Most of the information here is based a mail on Debian's mailinglist by Christian Seiler. crypttab entries Add entries for your disks to /etc/crypttab . Use - to signal interactive passwords and add noauto to avoid hanging at early boot: # format : < name > < disk > < keyfile > < options > HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHK98JS - part1 - luks , noauto HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHLZA1S - part1 - luks , noauto WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683_LUKS / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683 - part1 - luks , noauto WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927_LUKS / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927 - part1 - luks , noauto Systemd Units Then you need to add a few unit files to create proper dependencies. unlockme.target Mostly just copy and edit /usr/lib/systemd/system/multi-user.target : [Unit] Description = System waiting for decryption of disks Requires = basic.target Conflicts = rescue.service rescue.target After = basic.target rescue.service rescue.target AllowIsolate = yes Copy wanted symlinks from /usr/lib : mkcd / etc / systemd / system / unlockme . target . wants / for w in / usr / lib / systemd / system / multi - user . target . wants /*; do ln -s $(readlink -f $w) done And add any services that you might require to be able to login via ssh : Warning Do not forget required networking services! ln - s / usr / lib / systemd / system / sshd . service ln - s / usr / lib / systemd / system / NetworkManager . service unlocked.target This target depends on all disks to be decrypted. The service's instance name is the <name> from your crypttab. [Unit] Description = Decrypted all disks Conflicts = systemd-ask-password-console.path systemd-ask-password-console.service Conflicts = systemd-ask-password-plymouth.path systemd-ask-password-plymouth.service Requires = unlockme.target \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS.service \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0284683_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0496927_LUKS.service Hint Remember to escape any names which might contain special characters in systemd's sense, i.e. run names though systemd-escape first. Note I needed a little override in /etc/systemd/system/systemd-cryptsetup@.service.d/dependencies.conf to make sure that this target properly waits for all systemd-cryptsetup@***.service units without specifying them all in After= manually: [Unit] DefaultDependencies = yes continue.service Add the service which depends on those targets and then kicks off the rest of the startup procedures: [Unit] Description = Continue system startup after disk decryption Requires = unlocked.target After = unlocked.target [Service] Type = oneshot ExecStart = /usr/bin/systemctl --no-block start multi-user.target Change Default Target Finally, change your default target to unlockme.target to use this procedure: systemctl set - default unlockme . target Now reboot, login with ssh and then start the continuation service to unlock your disks and carry on: systemctl start continue You should be asked to enter the passwords on the commandline directly. Hint If you have trouble booting after any changes, apped init=/sysroot/bin/sh to your kernel commandline!","title":"Systemd Disk Decryption Target"},{"location":"security/systemd-decryption-target.html#systemd-disk-decryption-target","text":"In order to delay most of your systems services during boot until a bunch of harddisks are decrypted but still bring up enough to allow for remote unlocking via ssh you'll need to use a systemd.target . Most of the information here is based a mail on Debian's mailinglist by Christian Seiler.","title":"Systemd Disk Decryption Target"},{"location":"security/systemd-decryption-target.html#crypttab-entries","text":"Add entries for your disks to /etc/crypttab . Use - to signal interactive passwords and add noauto to avoid hanging at early boot: # format : < name > < disk > < keyfile > < options > HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHK98JS - part1 - luks , noauto HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHLZA1S - part1 - luks , noauto WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683_LUKS / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683 - part1 - luks , noauto WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927_LUKS / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927 - part1 - luks , noauto","title":"crypttab entries"},{"location":"security/systemd-decryption-target.html#systemd-units","text":"Then you need to add a few unit files to create proper dependencies.","title":"Systemd Units"},{"location":"security/systemd-decryption-target.html#unlockmetarget","text":"Mostly just copy and edit /usr/lib/systemd/system/multi-user.target : [Unit] Description = System waiting for decryption of disks Requires = basic.target Conflicts = rescue.service rescue.target After = basic.target rescue.service rescue.target AllowIsolate = yes Copy wanted symlinks from /usr/lib : mkcd / etc / systemd / system / unlockme . target . wants / for w in / usr / lib / systemd / system / multi - user . target . wants /*; do ln -s $(readlink -f $w) done And add any services that you might require to be able to login via ssh : Warning Do not forget required networking services! ln - s / usr / lib / systemd / system / sshd . service ln - s / usr / lib / systemd / system / NetworkManager . service","title":"unlockme.target"},{"location":"security/systemd-decryption-target.html#unlockedtarget","text":"This target depends on all disks to be decrypted. The service's instance name is the <name> from your crypttab. [Unit] Description = Decrypted all disks Conflicts = systemd-ask-password-console.path systemd-ask-password-console.service Conflicts = systemd-ask-password-plymouth.path systemd-ask-password-plymouth.service Requires = unlockme.target \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS.service \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0284683_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0496927_LUKS.service Hint Remember to escape any names which might contain special characters in systemd's sense, i.e. run names though systemd-escape first. Note I needed a little override in /etc/systemd/system/systemd-cryptsetup@.service.d/dependencies.conf to make sure that this target properly waits for all systemd-cryptsetup@***.service units without specifying them all in After= manually: [Unit] DefaultDependencies = yes","title":"unlocked.target"},{"location":"security/systemd-decryption-target.html#continueservice","text":"Add the service which depends on those targets and then kicks off the rest of the startup procedures: [Unit] Description = Continue system startup after disk decryption Requires = unlocked.target After = unlocked.target [Service] Type = oneshot ExecStart = /usr/bin/systemctl --no-block start multi-user.target","title":"continue.service"},{"location":"security/systemd-decryption-target.html#change-default-target","text":"Finally, change your default target to unlockme.target to use this procedure: systemctl set - default unlockme . target Now reboot, login with ssh and then start the continuation service to unlock your disks and carry on: systemctl start continue You should be asked to enter the passwords on the commandline directly. Hint If you have trouble booting after any changes, apped init=/sysroot/bin/sh to your kernel commandline!","title":"Change Default Target"},{"location":"security/zfs-in-place-encryption.html","text":"ZFS in-place Encryption There are basically two possibilities to encrypt an existing array's disks: perform in-place encryption of cold disks with luksipc requires that you have enough free space at the end will not work if your array is assembled from /dev/disk/by-id/* paths, since those will change iterate over hot disks by overwriting and resilvering each one will leave your array in a degraded but usable state you should overwrite the entire disk to make sure plaintext traces are removed depending on how full your array is you might need to write up to twice your raw accumulated disk size worth of data .. this takes a lot of time! Hot Encryption I chose to use the latter method because there was not enough space at the end of the drives and I was not sure how ZFS could handle the changed disk paths. A couple things to note: make sure you align all partitions / containers! check your real physical block size (the drive might lie) check the ashift property of your pool use partitions, do not make the LUKS partition span the entire drive! begin the first partition at a multiple of your pbs , e.g. 4096 sectors / 2 MiB is a safe bet account for the LUKS header (usually should be 2 MiB) do not enlarge your partitions by too much, so you can replace them later on however make sure that the mapped device cannot be smaller than your original partition! Example For example, I originally had four 7809835008 sector partitions ( fdisk uses 512 byte sectors here). Partition sectors (512 bytes) approximate size Original ZFS 7809835008 3813396 MiB Full disk 7814037168 ~ 3815447 MiB Nominal 4 TiB disk 7812500000 ~ 3814697 MiB New ZFS 7811072000 3814000 MiB New LUKS 7811076096 3814002 MiB Rinse & Repeat Assume the original pool looked like this: kourier mirror - 0 ONLINE / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927 - part2 ONLINE / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHLZA1S - part2 ONLINE mirror - 1 ONLINE / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683 - part2 ONLINE / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHK98JS - part2 ONLINE Take the first drive offline: export DISK=\"WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927\" zpool offline kourier ata- ${ DISK } -part2 Overwrite with zeroes or random data: dd if =/ dev / zero of =/ dev / disk / by - id / ata - ${ DISK } status = progress bs = 1 M Create a new partition table and one partition with your desired size (the UUID sets the partition type to FreeBSD ZFS ): printf '%s\\n' \\ \"label: gpt\" \\ \"4096,+7811076096,516E7CBA-6ECF-11D6-8FF8-00022D09712B\" \\ | ./sfdisk-2.33 /dev/disk/by-id/ata- ${ DISK } Create and open the LUKS container with your desired cipher / hash / keysize settings: cryptsetup luksFormat -h sha384 /dev/disk/by-id/ata- ${ DISK } -part1 cryptsetup open /dev/disk/by-id/ata- ${ DISK } -part1 ${ DISK } _LUKS Replace the drive in the pool and wait for it to resilver: zpool replace kourier ata- ${ DISK } -part2 /dev/mapper/ ${ DISK } _LUKS watch -d zpool status -P Rinse and repeat with all four disks. This is what my pool looks like now: kourier mirror - 0 ONLINE / dev / mapper / WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927_LUKS ONLINE / dev / mapper / HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS ONLINE mirror - 1 ONLINE / dev / mapper / WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683_LUKS ONLINE / dev / mapper / HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS ONLINE systemd Target My next task will be to create proper dependencies for all my systemd services. I do not want my system to block boot, so I can later login and manually decrypt the disks. However I also don't want to have services randomly fail or attempt to create nonexistent paths because the zpool is not imported yet. They should just queue and wait for me to decrypt the drives and then automatically continue once I've done that. Useful pointers: systemd-cryptsetup@.service bundling all encrypted disks in a *.target After= , RequiredBy= / WantedBy= and BindsTo= properties of services systemd.unit(5) Note See Systemd Decryption Target for the finished result.","title":"ZFS in-place Encryption"},{"location":"security/zfs-in-place-encryption.html#zfs-in-place-encryption","text":"There are basically two possibilities to encrypt an existing array's disks: perform in-place encryption of cold disks with luksipc requires that you have enough free space at the end will not work if your array is assembled from /dev/disk/by-id/* paths, since those will change iterate over hot disks by overwriting and resilvering each one will leave your array in a degraded but usable state you should overwrite the entire disk to make sure plaintext traces are removed depending on how full your array is you might need to write up to twice your raw accumulated disk size worth of data .. this takes a lot of time!","title":"ZFS in-place Encryption"},{"location":"security/zfs-in-place-encryption.html#hot-encryption","text":"I chose to use the latter method because there was not enough space at the end of the drives and I was not sure how ZFS could handle the changed disk paths. A couple things to note: make sure you align all partitions / containers! check your real physical block size (the drive might lie) check the ashift property of your pool use partitions, do not make the LUKS partition span the entire drive! begin the first partition at a multiple of your pbs , e.g. 4096 sectors / 2 MiB is a safe bet account for the LUKS header (usually should be 2 MiB) do not enlarge your partitions by too much, so you can replace them later on however make sure that the mapped device cannot be smaller than your original partition!","title":"Hot Encryption"},{"location":"security/zfs-in-place-encryption.html#example","text":"For example, I originally had four 7809835008 sector partitions ( fdisk uses 512 byte sectors here). Partition sectors (512 bytes) approximate size Original ZFS 7809835008 3813396 MiB Full disk 7814037168 ~ 3815447 MiB Nominal 4 TiB disk 7812500000 ~ 3814697 MiB New ZFS 7811072000 3814000 MiB New LUKS 7811076096 3814002 MiB","title":"Example"},{"location":"security/zfs-in-place-encryption.html#rinse-repeat","text":"Assume the original pool looked like this: kourier mirror - 0 ONLINE / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927 - part2 ONLINE / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHLZA1S - part2 ONLINE mirror - 1 ONLINE / dev / disk / by - id / ata - WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683 - part2 ONLINE / dev / disk / by - id / ata - HGST_HDN724040ALE640_PK1334PEHK98JS - part2 ONLINE Take the first drive offline: export DISK=\"WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927\" zpool offline kourier ata- ${ DISK } -part2 Overwrite with zeroes or random data: dd if =/ dev / zero of =/ dev / disk / by - id / ata - ${ DISK } status = progress bs = 1 M Create a new partition table and one partition with your desired size (the UUID sets the partition type to FreeBSD ZFS ): printf '%s\\n' \\ \"label: gpt\" \\ \"4096,+7811076096,516E7CBA-6ECF-11D6-8FF8-00022D09712B\" \\ | ./sfdisk-2.33 /dev/disk/by-id/ata- ${ DISK } Create and open the LUKS container with your desired cipher / hash / keysize settings: cryptsetup luksFormat -h sha384 /dev/disk/by-id/ata- ${ DISK } -part1 cryptsetup open /dev/disk/by-id/ata- ${ DISK } -part1 ${ DISK } _LUKS Replace the drive in the pool and wait for it to resilver: zpool replace kourier ata- ${ DISK } -part2 /dev/mapper/ ${ DISK } _LUKS watch -d zpool status -P Rinse and repeat with all four disks. This is what my pool looks like now: kourier mirror - 0 ONLINE / dev / mapper / WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0496927_LUKS ONLINE / dev / mapper / HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS ONLINE mirror - 1 ONLINE / dev / mapper / WDC_WD40EZRX - 00 SPEB0_WD - WCC4E0284683_LUKS ONLINE / dev / mapper / HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS ONLINE","title":"Rinse &amp; Repeat"},{"location":"security/zfs-in-place-encryption.html#systemd-target","text":"My next task will be to create proper dependencies for all my systemd services. I do not want my system to block boot, so I can later login and manually decrypt the disks. However I also don't want to have services randomly fail or attempt to create nonexistent paths because the zpool is not imported yet. They should just queue and wait for me to decrypt the drives and then automatically continue once I've done that. Useful pointers: systemd-cryptsetup@.service bundling all encrypted disks in a *.target After= , RequiredBy= / WantedBy= and BindsTo= properties of services systemd.unit(5) Note See Systemd Decryption Target for the finished result.","title":"systemd Target"},{"location":"tips/ansible.html","text":"Ansible Inline Vault usage The Ansible vault can encrypt your secrets so you can add them to your inventory files and track those in your preferred version control system. Since version 2.3, Ansible allows using encrypted values inline in an otherwise unencrypted file. Create key In a simple setup with a single user you my want to use a password file with a high-entropy secret inside. Just don't add that to any VCS. $ high-entropy-password-gen > ~/.ansible/vaultkey # e.g. my diceware words alias: $ words 10 - > ~/.ansible/vaultkey Edit your ansible.cfg to use that key without prompting: # If set, configures the path to the Vault password file as an alternative to # specifying --vault-password-file on the command line. vault_password_file = ~/.ansible/vaultkey Encrypt secret values Then use ansible-vault encrypt_string to encrypt your secrets: $ echo mysecret | ansible-vault encrypt_string Reading plaintext input from stdin. ( ctrl-d to end input ) !vault | $ANSIBLE_VAULT ; 1 .1 ; AES256 34326362313132393835323362663331323238393837613134646465333339623034653666626633 6439616237613939393666363530626663373132616232300a346164363933613934333830613932 36356235323665346530626438313935653537333836373935313336343265343061656262396337 3832666631623739330a316363336463613530343132633765366166363532303135333736653931 62386637636532363064346134333735313737356666613233623166653239333832 Encryption successful If your secret is in the clipboard and my aliases are installed, a clipboard pipe works great: $ clipboard | ansible-vault encrypt_string | clipboard Finally paste the encrypted secret in your inventory or variable file: [ ... ] runner.rz.semjonov.de : ansemjo_gitlab_runner_registration_token : !vault | $ANSIBLE_VAULT;1.1;AES256 35376637383563383661366562613932306437653533623461303032346566633032626435356538 3564376461343131613165386135303534666166393138650a356233333030323730666562613637 36653561396430346539373966366338633861346130623135633732383030666130393765323431 6333393837336665650a343738646135323235323331306630333465303535363530653435383532 35633834666138373661336436363963363766393236336536306134653136343064 ansemjo_gitlab_runner_registration_url : https://git.rz.semjonov.de/ [ ... ]","title":"Ansible"},{"location":"tips/ansible.html#ansible","text":"","title":"Ansible"},{"location":"tips/ansible.html#inline-vault-usage","text":"The Ansible vault can encrypt your secrets so you can add them to your inventory files and track those in your preferred version control system. Since version 2.3, Ansible allows using encrypted values inline in an otherwise unencrypted file.","title":"Inline Vault usage"},{"location":"tips/ansible.html#create-key","text":"In a simple setup with a single user you my want to use a password file with a high-entropy secret inside. Just don't add that to any VCS. $ high-entropy-password-gen > ~/.ansible/vaultkey # e.g. my diceware words alias: $ words 10 - > ~/.ansible/vaultkey Edit your ansible.cfg to use that key without prompting: # If set, configures the path to the Vault password file as an alternative to # specifying --vault-password-file on the command line. vault_password_file = ~/.ansible/vaultkey","title":"Create key"},{"location":"tips/ansible.html#encrypt-secret-values","text":"Then use ansible-vault encrypt_string to encrypt your secrets: $ echo mysecret | ansible-vault encrypt_string Reading plaintext input from stdin. ( ctrl-d to end input ) !vault | $ANSIBLE_VAULT ; 1 .1 ; AES256 34326362313132393835323362663331323238393837613134646465333339623034653666626633 6439616237613939393666363530626663373132616232300a346164363933613934333830613932 36356235323665346530626438313935653537333836373935313336343265343061656262396337 3832666631623739330a316363336463613530343132633765366166363532303135333736653931 62386637636532363064346134333735313737356666613233623166653239333832 Encryption successful If your secret is in the clipboard and my aliases are installed, a clipboard pipe works great: $ clipboard | ansible-vault encrypt_string | clipboard Finally paste the encrypted secret in your inventory or variable file: [ ... ] runner.rz.semjonov.de : ansemjo_gitlab_runner_registration_token : !vault | $ANSIBLE_VAULT;1.1;AES256 35376637383563383661366562613932306437653533623461303032346566633032626435356538 3564376461343131613165386135303534666166393138650a356233333030323730666562613637 36653561396430346539373966366338633861346130623135633732383030666130393765323431 6333393837336665650a343738646135323235323331306630333465303535363530653435383532 35633834666138373661336436363963363766393236336536306134653136343064 ansemjo_gitlab_runner_registration_url : https://git.rz.semjonov.de/ [ ... ]","title":"Encrypt secret values"},{"location":"tips/backblaze.html","text":"Backblaze Manually Sync For example to additionally store my GitLab backups on Backblaze: move all relevant files to a folder (optionally) encrypt all files run b2 sync . b2://bucketname/ GitLab Backups My GitLab backups are currently stored in a Minio S3 object storage with WORM activated but files can still be deleted by accident from the server itself. A naiive workflow would look like this: Mirror all files from Minio locally: tmp mc mirror rz / gitlab / . / Encrypt all files with GPG and remove plaintexts: for f in $ ( ls ! ( * . gpg )) ; do \\ gpg -- recipient ansemjo -- encrypt $f ; \\ rm - fv $f ; \\ done ; Mirror encrypted files to Backblaze: # check file list b2 sync --dryRun --compareVersions none ./ b2://gitbacks/ # upload to backblaze b2 sync --compareVersions none ./ b2://gitbacks/ The --compareVersions none is needed since GPG does not create stable filesizes and the naiive approach obviously creates newer modification times. Unless there are problems during upload, the filename should be a stable enough distinction though.","title":"Backblaze"},{"location":"tips/backblaze.html#backblaze","text":"","title":"Backblaze"},{"location":"tips/backblaze.html#manually-sync","text":"For example to additionally store my GitLab backups on Backblaze: move all relevant files to a folder (optionally) encrypt all files run b2 sync . b2://bucketname/","title":"Manually Sync"},{"location":"tips/backblaze.html#gitlab-backups","text":"My GitLab backups are currently stored in a Minio S3 object storage with WORM activated but files can still be deleted by accident from the server itself. A naiive workflow would look like this: Mirror all files from Minio locally: tmp mc mirror rz / gitlab / . / Encrypt all files with GPG and remove plaintexts: for f in $ ( ls ! ( * . gpg )) ; do \\ gpg -- recipient ansemjo -- encrypt $f ; \\ rm - fv $f ; \\ done ; Mirror encrypted files to Backblaze: # check file list b2 sync --dryRun --compareVersions none ./ b2://gitbacks/ # upload to backblaze b2 sync --compareVersions none ./ b2://gitbacks/ The --compareVersions none is needed since GPG does not create stable filesizes and the naiive approach obviously creates newer modification times. Unless there are problems during upload, the filename should be a stable enough distinction though.","title":"GitLab Backups"},{"location":"tips/containers.html","text":"Containers Docker Firewalling By default, docker seems to start with --iptables=true everywhere. That means that the docker daemon will insert its own iptables rules to enable inter-container communication and publish ports. However that means that published ports will be published publicly by default. That means that a container started with -p 8000:8000 will be open to the world on that port. Even if your firewalld configuration does not permit this port. This is because Docker completely circumvents any firewall managers. Disable iptables tampering To disable this behaviour add --iptables=false to the start arguments of docker. Either do that by editing the systemd service, or set an DOCKER_OPTS=\"...\" in /etc/default/docker if applicable. $ systemctl edit docker . service [ Service ] ExecStart = ExecStart =/ usr / bin / dockerd - H fd : // --iptables=false This also disables the forwarding rules however. Your containers will not be able to reach the outside world anymore. To reenable the forwarding with firewalld use: $ firewall-cmd --add-masquerade --permanent Or using raw iptables rules: - A FORWARD - i docker0 - o eth0 - j ACCEPT - A FORWARD - i eth0 - o docker0 - j ACCEPT Full systemd in container Podman introduced some fixes that enable you running a full systemd init process inside of a rootless container. That way you can start a normal CentOS image with podman run ... centos init and login like you would in a virtual machine, enable systemd services etc. To properly login you need two small fixes however. First you need a known password. Since moust images have passwords disabled or empty for all accounts you'll need to mount an edited /etc/shadow . The following line for example sets the root password to literally password : root : $ 6 $ 1 xZg0v5W$XgEfFIUlHB3EIGsxJvABkytPaUITLEfTb7WocHoeFaAwBFfui2tIKZq1l / MoKtZHMQ7Q / 23 Dnr . qLhGfzz4VH / : 18061 : 0 : 99999 : 7 ::: Another fix is required for PAM, since the console accepts your password but PAM fails to create a session for you . The fix is simple: sed - i '/^session.*pam_loginuid.so/s/^/#/' / etc / pam . d / login Mount these two files inside the container and finally start it with: podman run --rm -it -v ... centos:latest init","title":"Containers"},{"location":"tips/containers.html#containers","text":"","title":"Containers"},{"location":"tips/containers.html#docker-firewalling","text":"By default, docker seems to start with --iptables=true everywhere. That means that the docker daemon will insert its own iptables rules to enable inter-container communication and publish ports. However that means that published ports will be published publicly by default. That means that a container started with -p 8000:8000 will be open to the world on that port. Even if your firewalld configuration does not permit this port. This is because Docker completely circumvents any firewall managers.","title":"Docker Firewalling"},{"location":"tips/containers.html#disable-iptables-tampering","text":"To disable this behaviour add --iptables=false to the start arguments of docker. Either do that by editing the systemd service, or set an DOCKER_OPTS=\"...\" in /etc/default/docker if applicable. $ systemctl edit docker . service [ Service ] ExecStart = ExecStart =/ usr / bin / dockerd - H fd : // --iptables=false This also disables the forwarding rules however. Your containers will not be able to reach the outside world anymore. To reenable the forwarding with firewalld use: $ firewall-cmd --add-masquerade --permanent Or using raw iptables rules: - A FORWARD - i docker0 - o eth0 - j ACCEPT - A FORWARD - i eth0 - o docker0 - j ACCEPT","title":"Disable iptables tampering"},{"location":"tips/containers.html#full-systemd-in-container","text":"Podman introduced some fixes that enable you running a full systemd init process inside of a rootless container. That way you can start a normal CentOS image with podman run ... centos init and login like you would in a virtual machine, enable systemd services etc. To properly login you need two small fixes however. First you need a known password. Since moust images have passwords disabled or empty for all accounts you'll need to mount an edited /etc/shadow . The following line for example sets the root password to literally password : root : $ 6 $ 1 xZg0v5W$XgEfFIUlHB3EIGsxJvABkytPaUITLEfTb7WocHoeFaAwBFfui2tIKZq1l / MoKtZHMQ7Q / 23 Dnr . qLhGfzz4VH / : 18061 : 0 : 99999 : 7 ::: Another fix is required for PAM, since the console accepts your password but PAM fails to create a session for you . The fix is simple: sed - i '/^session.*pam_loginuid.so/s/^/#/' / etc / pam . d / login Mount these two files inside the container and finally start it with: podman run --rm -it -v ... centos:latest init","title":"Full systemd in container"},{"location":"tips/coreos.html","text":"CoreOS Various tricks for the super slim container OS. QEMU Guest Agent The guest agent qemu-ga is required for the host to discover the virtual machine's network setup, specifically it's IP. You can start the guest agent in an Alpine container: docker run -d \\ -v /dev:/dev \\ --privileged \\ --net host \\ alpine ash -c 'apk add qemu-guest-agent && exec qemu-ga -v'","title":"CoreOS"},{"location":"tips/coreos.html#coreos","text":"Various tricks for the super slim container OS.","title":"CoreOS"},{"location":"tips/coreos.html#qemu-guest-agent","text":"The guest agent qemu-ga is required for the host to discover the virtual machine's network setup, specifically it's IP. You can start the guest agent in an Alpine container: docker run -d \\ -v /dev:/dev \\ --privileged \\ --net host \\ alpine ash -c 'apk add qemu-guest-agent && exec qemu-ga -v'","title":"QEMU Guest Agent"},{"location":"tips/documentscan.html","text":"Document Scanning For my document management workflow I have settled on an Android scanner app and optical character recognition on the commandline for now. Scanbot The scanner app is Scanbot . It is touted as the preferred document scanner app in various articles and has a couple of advantages compared to its competitors. Among them are a nice and clean interface which is important for a quick workflow and automatic uploading to a cloud storage of your choice, including local network SFTP servers. The Pro version is required for this but it is not too expensive. OCRmyPDF OCR is performed on a Linux computer with ocrmypdf . This has the advantage of using a beefier CPU to do the OCR and save my smartphone battery. It also produces consistently nice results because the tesseract engine it uses is pretty awesome. On many distributions it is available as a package in the repositories. On CentOS 7 you can install it and all its dependencies with (Python 3.6 + pip required): pip install ocrmypdf yum install - y ghostscript qpdf tesseract tesseract - langpack - deu unpaper pngquant Additionally I use the following bash alias to easily perform OCR on documents in-place: ocr () { file = $ 1 ; shift 1 ; [[ - z $ file ]] & amp ; & amp ; { printf ' perform ocr on pdfs with ocrmypdf \\ nusage : % s & lt ; path / to / pdf & gt ; [ & lt ; extra args & gt ;] \\ n ' \"$0\" 1 & gt ; & amp ; 2 ; exit 1 }; ocrmypdf - cd \"$@\" \"$file\" \"$file\" } Indexing After some hiccups, the GNOME tracker works pretty nicely for full-text indexing of my scanned documents. If everything was indexed correctly, you can search for your documents in the GNOME Documents program or enable full-text search in Nautilus by pressing on the magnifying glass icon. Signing I would like to add cryptographic signatures to my PDFs but there appear to be no Linux programs capable of adding such signatures from an X.509 certificate. Regardless, my default viewer evince would not display such signatures. If I have important documents I should thereforre resort to using detached GPG signatures or regularly signing a sha256sum file.","title":"Document Scanning"},{"location":"tips/documentscan.html#document-scanning","text":"For my document management workflow I have settled on an Android scanner app and optical character recognition on the commandline for now.","title":"Document Scanning"},{"location":"tips/documentscan.html#scanbot","text":"The scanner app is Scanbot . It is touted as the preferred document scanner app in various articles and has a couple of advantages compared to its competitors. Among them are a nice and clean interface which is important for a quick workflow and automatic uploading to a cloud storage of your choice, including local network SFTP servers. The Pro version is required for this but it is not too expensive.","title":"Scanbot"},{"location":"tips/documentscan.html#ocrmypdf","text":"OCR is performed on a Linux computer with ocrmypdf . This has the advantage of using a beefier CPU to do the OCR and save my smartphone battery. It also produces consistently nice results because the tesseract engine it uses is pretty awesome. On many distributions it is available as a package in the repositories. On CentOS 7 you can install it and all its dependencies with (Python 3.6 + pip required): pip install ocrmypdf yum install - y ghostscript qpdf tesseract tesseract - langpack - deu unpaper pngquant Additionally I use the following bash alias to easily perform OCR on documents in-place: ocr () { file = $ 1 ; shift 1 ; [[ - z $ file ]] & amp ; & amp ; { printf ' perform ocr on pdfs with ocrmypdf \\ nusage : % s & lt ; path / to / pdf & gt ; [ & lt ; extra args & gt ;] \\ n ' \"$0\" 1 & gt ; & amp ; 2 ; exit 1 }; ocrmypdf - cd \"$@\" \"$file\" \"$file\" }","title":"OCRmyPDF"},{"location":"tips/documentscan.html#indexing","text":"After some hiccups, the GNOME tracker works pretty nicely for full-text indexing of my scanned documents. If everything was indexed correctly, you can search for your documents in the GNOME Documents program or enable full-text search in Nautilus by pressing on the magnifying glass icon.","title":"Indexing"},{"location":"tips/documentscan.html#signing","text":"I would like to add cryptographic signatures to my PDFs but there appear to be no Linux programs capable of adding such signatures from an X.509 certificate. Regardless, my default viewer evince would not display such signatures. If I have important documents I should thereforre resort to using detached GPG signatures or regularly signing a sha256sum file.","title":"Signing"},{"location":"tips/freeipa.html","text":"FreeIPA Request Certificates Manually You can request TLS certificates manually for hosts that are not fully enrolled in the domain or don't have any FreeIPA tools installed at all (CoreOS hosts, for example). This requires however, that you either are an admin in the domain or at least have the rights to create new hosts and service principals. First of all, create a signing request on the host: openssl req - nodes - new - newkey rsa : 2048 - sha256 \\ - out test . csr - keyout test . key \\ - subj '/CN=test.example.com/' Now switch to a machine with the FreeIPA tools installed and add a host entry. You'll want to do this anyway to properly be able to set DNS records for your host. kinit admin ipa host - add test . example . com --ip-address 192.168.1.100 Now transfer the CSR to this machine and sign the request while simulateneously adding the HTTP/ service principal: ipa cert - request test . csr \\ --principal HTTP/test.example.com --add \\ --certificate-out test.crt This command will display the serial number, which can later be used to fetch information about the certificate or revoke it. Finally, just copy the test.crt back to your host and configure whatever service you want to secure with TLS.","title":"FreeIPA"},{"location":"tips/freeipa.html#freeipa","text":"","title":"FreeIPA"},{"location":"tips/freeipa.html#request-certificates-manually","text":"You can request TLS certificates manually for hosts that are not fully enrolled in the domain or don't have any FreeIPA tools installed at all (CoreOS hosts, for example). This requires however, that you either are an admin in the domain or at least have the rights to create new hosts and service principals. First of all, create a signing request on the host: openssl req - nodes - new - newkey rsa : 2048 - sha256 \\ - out test . csr - keyout test . key \\ - subj '/CN=test.example.com/' Now switch to a machine with the FreeIPA tools installed and add a host entry. You'll want to do this anyway to properly be able to set DNS records for your host. kinit admin ipa host - add test . example . com --ip-address 192.168.1.100 Now transfer the CSR to this machine and sign the request while simulateneously adding the HTTP/ service principal: ipa cert - request test . csr \\ --principal HTTP/test.example.com --add \\ --certificate-out test.crt This command will display the serial number, which can later be used to fetch information about the certificate or revoke it. Finally, just copy the test.crt back to your host and configure whatever service you want to secure with TLS.","title":"Request Certificates Manually"},{"location":"tips/git.html","text":"Git Prevent commits on a branch You can use a pre-commit hook to check the branch name to which you are commiting your changes. If you want to prevent direct changes to master create the following .git/hooks/pre-commit : 1 2 3 4 5 #!/bin/sh # prevent commits on master [ \" $( git rev-parse --abbrev-ref --symbolic-full-name HEAD ) \" == \"master\" ] \\ && { echo \"you shall not commit on master\" ; exit 1 ; } Make sure the hook script is executable. Merge Repositories I've had a few situations where I started front- and backend as two seperate projects but soon wished to track both in a single repository - as subdirectories. Last time I was in this situation, a stackoverflow answer proved most helpful. Here's the gist: Assume you have two repositories frontend and backend . Prepare repositories First, you should move all files in each repository into a subdirectory to avoid merge conflicts and properly preserve commit history later. It would make sense to put all files in the frontend repository into a frontend subdirectory .. etc. \u2981 project / front : master = $ mkdir frontend \u2981 project / front : master = $ mv ! ( frontend ) frontend / \u2981 project / front : master *%= $ git add . \u2981 project / front : master += $ git commit [ master c777b42 ] prepare frontend for merge 65 files changed , 0 insertions ( + ), 0 deletions ( - ) [...] Hint Don't forget about dotfiles, as those are not moved with mv !(frontend) frontend/ alone. Do the same analogously for the backend or any other repository you want to merge. Also, create a new repository to hold the merged projects. It helps to create an initial commit -- even if completely empty -- to indicate that unrelated histories were merged into each other. \u2981 project $ mkdir project && cd project \u2981 project / project $ git init \u2981 project / project : master # $ git commit --allow-empty Add and fetch remotes Add all prepared repositories as remotes and fetch them. \u2981 project / project : master $ git remote add - f frontend .. / frontend \u2981 project / project : master $ git remote add - f backend .. / backend ... Merge the repositories Finally, merge all those remotes into the combined project. Simply repeat this step for every remote you want to merge. At this point it pays off to prepare the repositories in order to avoid any merge conflicts right away. \u2981 project / project : master $ git merge - s ours --no-commit --allow-unrelated-histories frontend/master Automatic merge went well ; stopped before committing as requested \u2981 project / project : master | MERGING $ git read - tree --prefix= -u frontend/master \u2981 project / project : master +| MERGING $ git commit - a [ master 96012 d4 ] Merge remote - tracking branch 'frontend/master' You will end up with a repository where history looks somewhat like this: \u2981 project / project : master $ git log -- graph * 7 a53ff5 2019 - 02 - 19 11 : 28 : 15 + 0100 N Merge remote - tracking branch ' backend / master ' ( HEAD -> master ) [ Anton Semjonov ] | \\ | * f89bfa3 2019 - 02 - 18 16 : 57 : 07 + 0100 N prepare backend for merge ( backend / master ) [ Anton Semjonov ] | * ea8a4f7 2018 - 09 - 18 15 : 19 : 16 + 0200 N update scripts [ Anton Semjonov ] | [...] * 96012 d4 2019 - 02 - 19 11 : 27 : 49 + 0100 N Merge remote - tracking branch ' frontend / master ' [ Anton Semjonov ] | \\ | * c777b42 2019 - 02 - 18 16 : 57 : 56 + 0100 N prepare frontend for merge ( frontend / master ) [ Anton Semjonov ] | * e3812e2 2019 - 01 - 18 22 : 35 : 30 + 0100 N korrigiere Leonhard [ Anton Semjonov ] | [...] * b0605af 2019 - 02 - 19 11 : 25 : 36 + 0100 N prepare combined repository for project [ Anton Semjonov ]","title":"Git"},{"location":"tips/git.html#git","text":"","title":"Git"},{"location":"tips/git.html#prevent-commits-on-a-branch","text":"You can use a pre-commit hook to check the branch name to which you are commiting your changes. If you want to prevent direct changes to master create the following .git/hooks/pre-commit : 1 2 3 4 5 #!/bin/sh # prevent commits on master [ \" $( git rev-parse --abbrev-ref --symbolic-full-name HEAD ) \" == \"master\" ] \\ && { echo \"you shall not commit on master\" ; exit 1 ; } Make sure the hook script is executable.","title":"Prevent commits on a branch"},{"location":"tips/git.html#merge-repositories","text":"I've had a few situations where I started front- and backend as two seperate projects but soon wished to track both in a single repository - as subdirectories. Last time I was in this situation, a stackoverflow answer proved most helpful. Here's the gist: Assume you have two repositories frontend and backend .","title":"Merge Repositories"},{"location":"tips/git.html#prepare-repositories","text":"First, you should move all files in each repository into a subdirectory to avoid merge conflicts and properly preserve commit history later. It would make sense to put all files in the frontend repository into a frontend subdirectory .. etc. \u2981 project / front : master = $ mkdir frontend \u2981 project / front : master = $ mv ! ( frontend ) frontend / \u2981 project / front : master *%= $ git add . \u2981 project / front : master += $ git commit [ master c777b42 ] prepare frontend for merge 65 files changed , 0 insertions ( + ), 0 deletions ( - ) [...] Hint Don't forget about dotfiles, as those are not moved with mv !(frontend) frontend/ alone. Do the same analogously for the backend or any other repository you want to merge. Also, create a new repository to hold the merged projects. It helps to create an initial commit -- even if completely empty -- to indicate that unrelated histories were merged into each other. \u2981 project $ mkdir project && cd project \u2981 project / project $ git init \u2981 project / project : master # $ git commit --allow-empty","title":"Prepare repositories"},{"location":"tips/git.html#add-and-fetch-remotes","text":"Add all prepared repositories as remotes and fetch them. \u2981 project / project : master $ git remote add - f frontend .. / frontend \u2981 project / project : master $ git remote add - f backend .. / backend ...","title":"Add and fetch remotes"},{"location":"tips/git.html#merge-the-repositories","text":"Finally, merge all those remotes into the combined project. Simply repeat this step for every remote you want to merge. At this point it pays off to prepare the repositories in order to avoid any merge conflicts right away. \u2981 project / project : master $ git merge - s ours --no-commit --allow-unrelated-histories frontend/master Automatic merge went well ; stopped before committing as requested \u2981 project / project : master | MERGING $ git read - tree --prefix= -u frontend/master \u2981 project / project : master +| MERGING $ git commit - a [ master 96012 d4 ] Merge remote - tracking branch 'frontend/master' You will end up with a repository where history looks somewhat like this: \u2981 project / project : master $ git log -- graph * 7 a53ff5 2019 - 02 - 19 11 : 28 : 15 + 0100 N Merge remote - tracking branch ' backend / master ' ( HEAD -> master ) [ Anton Semjonov ] | \\ | * f89bfa3 2019 - 02 - 18 16 : 57 : 07 + 0100 N prepare backend for merge ( backend / master ) [ Anton Semjonov ] | * ea8a4f7 2018 - 09 - 18 15 : 19 : 16 + 0200 N update scripts [ Anton Semjonov ] | [...] * 96012 d4 2019 - 02 - 19 11 : 27 : 49 + 0100 N Merge remote - tracking branch ' frontend / master ' [ Anton Semjonov ] | \\ | * c777b42 2019 - 02 - 18 16 : 57 : 56 + 0100 N prepare frontend for merge ( frontend / master ) [ Anton Semjonov ] | * e3812e2 2019 - 01 - 18 22 : 35 : 30 + 0100 N korrigiere Leonhard [ Anton Semjonov ] | [...] * b0605af 2019 - 02 - 19 11 : 25 : 36 + 0100 N prepare combined repository for project [ Anton Semjonov ]","title":"Merge the repositories"},{"location":"tips/gitlab.html","text":"Gitlab Gitlab Runner in QEMU/KVM First deploy CoreOS in a virtual machine . Then deploy the Gitlab Runner as a Docker container itself. Following the documentation : docker run - d -- name runner -- restart always \\ - v / etc / gitlab - runner : / etc / gitlab - runner \\ - v / var / run / docker . sock : / var / run / docker . sock \\ gitlab / gitlab - runner : alpine docker exec - it runner register Hint You may need to install your CA certificate both on the CoreOS VM as well as in the runner configuration first: scp / etc / ipa / ca . crt runner : ssh runner sudo mv ca . crt / etc / ssl / certs / my - ca . pem sudo update - ca - certificates sudo mkdir - p / etc / gitlab - runner / certs sudo cp / etc / ssl / certs / my - ca . pem / etc / gitlab - runner / certs / ca . crt Reboot the VM and/or restart the Docker service afterwards. Gitlab API Official Documentation is available with all the v4 API routes. Bash Alias A useful bash alias for httpie to interact with the GitLab API: gitlab () { meth = ${ 1 :?http method } ; api = ${ 2 :?api path } ; shift 2 ; http --check-status \\ \" $meth \" \"https://git.rz.semjonov.de/api/v4/ $api \" \\ private-token: \" $TOKEN \" \\ \" $@ \" ; } Then export your personal access token to env: read TOKEN && export TOKEN Usage Chained to jq , the usage becomes: $ gitlab GET projects | jq 'map(.name)' [ \"deploy\" , \"bookstack\" , \"preseedinjector\" , \"frontend\" , \"sbupdate\" , \"...\" ] $ gitlab PUT projects/11 wiki_enabled = false HTTP/1.1 200 OK Cache-Control: max-age = 0 , private, must-revalidate Connection: keep-alive Content-Length: 2022 Content-Type: application/json Date: Fri, 27 Jul 2018 13 :41:41 GMT ... Examples Get the Wiki Status A stupid loop to get the wiki_enabled status of projects: for i in { 1 ..116 } ; do project = $( gitlab GET projects/ $i 2 >/dev/null ) \\ && wiki = $( jq .wiki_enabled <<< \" $project \" ) \\ && path = $( jq .path_with_namespace <<< \" $project \" ) \\ && echo \" $i $path : $wiki \" ; done","title":"Gitlab"},{"location":"tips/gitlab.html#gitlab","text":"","title":"Gitlab"},{"location":"tips/gitlab.html#gitlab-runner-in-qemukvm","text":"First deploy CoreOS in a virtual machine . Then deploy the Gitlab Runner as a Docker container itself. Following the documentation : docker run - d -- name runner -- restart always \\ - v / etc / gitlab - runner : / etc / gitlab - runner \\ - v / var / run / docker . sock : / var / run / docker . sock \\ gitlab / gitlab - runner : alpine docker exec - it runner register Hint You may need to install your CA certificate both on the CoreOS VM as well as in the runner configuration first: scp / etc / ipa / ca . crt runner : ssh runner sudo mv ca . crt / etc / ssl / certs / my - ca . pem sudo update - ca - certificates sudo mkdir - p / etc / gitlab - runner / certs sudo cp / etc / ssl / certs / my - ca . pem / etc / gitlab - runner / certs / ca . crt Reboot the VM and/or restart the Docker service afterwards.","title":"Gitlab Runner in QEMU/KVM"},{"location":"tips/gitlab.html#gitlab-api","text":"Official Documentation is available with all the v4 API routes.","title":"Gitlab API"},{"location":"tips/gitlab.html#bash-alias","text":"A useful bash alias for httpie to interact with the GitLab API: gitlab () { meth = ${ 1 :?http method } ; api = ${ 2 :?api path } ; shift 2 ; http --check-status \\ \" $meth \" \"https://git.rz.semjonov.de/api/v4/ $api \" \\ private-token: \" $TOKEN \" \\ \" $@ \" ; } Then export your personal access token to env: read TOKEN && export TOKEN","title":"Bash Alias"},{"location":"tips/gitlab.html#usage","text":"Chained to jq , the usage becomes: $ gitlab GET projects | jq 'map(.name)' [ \"deploy\" , \"bookstack\" , \"preseedinjector\" , \"frontend\" , \"sbupdate\" , \"...\" ] $ gitlab PUT projects/11 wiki_enabled = false HTTP/1.1 200 OK Cache-Control: max-age = 0 , private, must-revalidate Connection: keep-alive Content-Length: 2022 Content-Type: application/json Date: Fri, 27 Jul 2018 13 :41:41 GMT ...","title":"Usage"},{"location":"tips/gitlab.html#examples","text":"","title":"Examples"},{"location":"tips/gitlab.html#get-the-wiki-status","text":"A stupid loop to get the wiki_enabled status of projects: for i in { 1 ..116 } ; do project = $( gitlab GET projects/ $i 2 >/dev/null ) \\ && wiki = $( jq .wiki_enabled <<< \" $project \" ) \\ && path = $( jq .path_with_namespace <<< \" $project \" ) \\ && echo \" $i $path : $wiki \" ; done","title":"Get the Wiki Status"},{"location":"tips/golang.html","text":"Go Create a super minimal FROM scratch contianer image Go applications can be statically compiled to run completely standalone with no supporting Linux filesystem present whatsoever. Strip and compress the binary and you'll have an image barely larger than busybox. Compile your application statically and with stripped symbols. The necessary command wil vary depending on your project's complexity. But for small projects something like this will do: CGO_ENABLED = 0 go build - ldflags = '-s -w' - o main Optionally compress the binary with upx . Check that it still runs! Sometimes this will break binaries. upx main If your application makes HTTP requests to TLS endpoints you'll want a copy of the SystemCertPool . See root_linux.go for a list of files which are searched for by default on a Linux system. cp / etc / ssl / certs / ca - certificates . crt . Create a simple Dockerfile to create an image \"from scratch\": FROM scratch COPY main / main COPY ca - certificates . crt / etc / ssl / certs / ca - certificates . crt ENTRYPOINT [ \"/main\" ] Build the image as usual with podman build -t test . . My test image clocked in at a mere 2.87 MiB and was only 6 KiB larger than the binary and certificate store combined.","title":"Go"},{"location":"tips/golang.html#go","text":"","title":"Go"},{"location":"tips/golang.html#create-a-super-minimal-from-scratch-contianer-image","text":"Go applications can be statically compiled to run completely standalone with no supporting Linux filesystem present whatsoever. Strip and compress the binary and you'll have an image barely larger than busybox. Compile your application statically and with stripped symbols. The necessary command wil vary depending on your project's complexity. But for small projects something like this will do: CGO_ENABLED = 0 go build - ldflags = '-s -w' - o main Optionally compress the binary with upx . Check that it still runs! Sometimes this will break binaries. upx main If your application makes HTTP requests to TLS endpoints you'll want a copy of the SystemCertPool . See root_linux.go for a list of files which are searched for by default on a Linux system. cp / etc / ssl / certs / ca - certificates . crt . Create a simple Dockerfile to create an image \"from scratch\": FROM scratch COPY main / main COPY ca - certificates . crt / etc / ssl / certs / ca - certificates . crt ENTRYPOINT [ \"/main\" ] Build the image as usual with podman build -t test . . My test image clocked in at a mere 2.87 MiB and was only 6 KiB larger than the binary and certificate store combined.","title":"Create a super minimal FROM scratch contianer image"},{"location":"tips/lenovo.html","text":"Lenovo Updating BIOS on a Lenovo from Linux Some modern Lenovo machines do not have an optical disc drive. The only option for machines without Windows is a bootable .iso image though. What now? Turns out inside that image there is another bootable format: an El Torito image. You can extract that with a script called geteltorito.pl and flash it to a USB stick. # . / geteltorito . pl - o n10ur17w - usb . img n10ur17w . iso Booting catalog starts at sector : 20 Manufacturer of CD : NERO BURNING ROM Image architecture : x86 Boot media type is : harddisk El Torito image starts at sector 27 and has 47104 sector ( s ) of 512 Bytes Image has been written to file \" n10ur17w-usb.img \" . # dd if = n10ur17w - usb . img of =/ dev / sdb bs = 1 M 23 + 0 records in 23 + 0 records out 24117248 bytes ( 24 MB , 23 MiB ) copied , 0 . 354471 s , 68 . 0 MB / s Note This information and script is taken from thinkwiki.de","title":"Lenovo"},{"location":"tips/lenovo.html#lenovo","text":"","title":"Lenovo"},{"location":"tips/lenovo.html#updating-bios-on-a-lenovo-from-linux","text":"Some modern Lenovo machines do not have an optical disc drive. The only option for machines without Windows is a bootable .iso image though. What now? Turns out inside that image there is another bootable format: an El Torito image. You can extract that with a script called geteltorito.pl and flash it to a USB stick. # . / geteltorito . pl - o n10ur17w - usb . img n10ur17w . iso Booting catalog starts at sector : 20 Manufacturer of CD : NERO BURNING ROM Image architecture : x86 Boot media type is : harddisk El Torito image starts at sector 27 and has 47104 sector ( s ) of 512 Bytes Image has been written to file \" n10ur17w-usb.img \" . # dd if = n10ur17w - usb . img of =/ dev / sdb bs = 1 M 23 + 0 records in 23 + 0 records out 24117248 bytes ( 24 MB , 23 MiB ) copied , 0 . 354471 s , 68 . 0 MB / s Note This information and script is taken from thinkwiki.de","title":"Updating BIOS on a Lenovo from Linux"},{"location":"tips/openwrt.html","text":"OpenWRT Image Builder The Image Builder (previously called the Image Generator) is a pre-compiled environment suitable for creating custom images without the need for compiling them from source. It downloads pre-compiled packages and integrates them in a single flashable image. Look for the openwrt-imagebuilder-<target>-<type>.Linux-x86_64.tar.xz in the firmware image folder for your device. Download and extract it somewhere. Get a list of available profiles with make info . For my TP-Link Archer C7 v2: imagebuilder: openwrt-imagebuilder-ar71xx-generic.Linux-x86_64.tar.xz profile: archer-c7-v2 You can include extra packages by configuring PACKAGES= . make image PROFILE = \"archer-c7-v2\" PACKAGES = \"-ppp -ppp-mod-pppoe luci-ssl wireguard\" The result will be stored in ./bin/targets/<target>/<type>/ . Link I wrote a small script to automate these steps for my Archer C7 v2, so I can quickly build a new snapshot firmware.","title":"OpenWRT"},{"location":"tips/openwrt.html#openwrt","text":"","title":"OpenWRT"},{"location":"tips/openwrt.html#image-builder","text":"The Image Builder (previously called the Image Generator) is a pre-compiled environment suitable for creating custom images without the need for compiling them from source. It downloads pre-compiled packages and integrates them in a single flashable image. Look for the openwrt-imagebuilder-<target>-<type>.Linux-x86_64.tar.xz in the firmware image folder for your device. Download and extract it somewhere. Get a list of available profiles with make info . For my TP-Link Archer C7 v2: imagebuilder: openwrt-imagebuilder-ar71xx-generic.Linux-x86_64.tar.xz profile: archer-c7-v2 You can include extra packages by configuring PACKAGES= . make image PROFILE = \"archer-c7-v2\" PACKAGES = \"-ppp -ppp-mod-pppoe luci-ssl wireguard\" The result will be stored in ./bin/targets/<target>/<type>/ . Link I wrote a small script to automate these steps for my Archer C7 v2, so I can quickly build a new snapshot firmware.","title":"Image Builder"},{"location":"tips/python.html","text":"Python Embedding version information in packages with setuptools TODO, see https://github.com/ansemjo/tinyssh-keyconvert/compare/0.3.1...0.3.2 Basically: use version.sh script modify release seperator to '-dev' use only 'version' output, not 'describe' to conform to pep 440 read version when packaging with a subprocess cmd, use directly in hash write that version into a simple file that exports __version__ in your package in your script try to import said __version__ in a try-except-clause and fallback now you can use the same version in the script, yay","title":"Python"},{"location":"tips/python.html#python","text":"","title":"Python"},{"location":"tips/python.html#embedding-version-information-in-packages-with-setuptools","text":"TODO, see https://github.com/ansemjo/tinyssh-keyconvert/compare/0.3.1...0.3.2 Basically: use version.sh script modify release seperator to '-dev' use only 'version' output, not 'describe' to conform to pep 440 read version when packaging with a subprocess cmd, use directly in hash write that version into a simple file that exports __version__ in your package in your script try to import said __version__ in a try-except-clause and fallback now you can use the same version in the script, yay","title":"Embedding version information in packages with setuptools"},{"location":"tips/restic.html","text":"Restic Most of these tips are assuming that you have your RESTIC_REPOSITORY and necessary API keys, e.g. B2_ACCOUNT_{ID|KEY} , set in your environment. This enables you to simply use restic [command] instead of specifying the repository with -r <repo> and entering the password interactively. Restore a single file List your snapshots: restic snapshots Fetch a list of files within a snapshot or search for one: restic ls - l { latest | snapshot - id } restic find Restore a single file matching a pattern: restic restore \\ -- target / restore / path \\ -- include filename_or_pattern \\ { latest | snapshot - id } Mount and browse a snapshot Or mount a snapshot and browse inside interactively: restic mount / mount / path ls - la / mount / path","title":"Restic"},{"location":"tips/restic.html#restic","text":"Most of these tips are assuming that you have your RESTIC_REPOSITORY and necessary API keys, e.g. B2_ACCOUNT_{ID|KEY} , set in your environment. This enables you to simply use restic [command] instead of specifying the repository with -r <repo> and entering the password interactively.","title":"Restic"},{"location":"tips/restic.html#restore-a-single-file","text":"List your snapshots: restic snapshots Fetch a list of files within a snapshot or search for one: restic ls - l { latest | snapshot - id } restic find Restore a single file matching a pattern: restic restore \\ -- target / restore / path \\ -- include filename_or_pattern \\ { latest | snapshot - id }","title":"Restore a single file"},{"location":"tips/restic.html#mount-and-browse-a-snapshot","text":"Or mount a snapshot and browse inside interactively: restic mount / mount / path ls - la / mount / path","title":"Mount and browse a snapshot"},{"location":"tips/tor.html","text":"Tor Local SOCKS Proxy Running a local Tor client / proxy is currently the default if none of ORPort , DirPort or ControlPort are defined. Since most distributions should ship a default config with those commented out you only need to download / install tor and start it. A possible obstacle is the configured DataDirectory if you want to run it as a user. In this case use this simple configuration: SocksPort 9050 Log notice stderr DataDirectory ~/ . local / share / tor Start tor with tor -f ~/.config/torrc or whereever you saved that config.","title":"Tor"},{"location":"tips/tor.html#tor","text":"","title":"Tor"},{"location":"tips/tor.html#local-socks-proxy","text":"Running a local Tor client / proxy is currently the default if none of ORPort , DirPort or ControlPort are defined. Since most distributions should ship a default config with those commented out you only need to download / install tor and start it. A possible obstacle is the configured DataDirectory if you want to run it as a user. In this case use this simple configuration: SocksPort 9050 Log notice stderr DataDirectory ~/ . local / share / tor Start tor with tor -f ~/.config/torrc or whereever you saved that config.","title":"Local SOCKS Proxy"}]}