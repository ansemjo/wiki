{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"This is ansemjo\u2019s small wiki. It contains all sorts of more or less useful information. Unless otherwise noted, the content on this site is licensed under the Creative Commons Attribution-ShareAlike 4.0 , which also applies to my website .","title":"Home"},{"location":"software.html","text":"Software \u00b6 Collaboration \u00b6 A collection of nice software that can be used for collaborative tasks, preferrably self-hosted. Name Description self-hosted Airtable Cloud-hosted Database with a beautiful UI and API integration no CodiMD Free fork of HackMD, realtime collaborative text editor similar to etherpad yes Archival \u00b6 An overview of archival software for different purposes. E-Mail \u00b6 Name Type Notes ansemjo/imapfetch Python fetches from IMAP, config via ini , archived in maildir for mutt raymii/NoPriv Python open formats, no search, combination with imapbox and calaca possible, exports to HTML tree mailpiler PHP many dependencies, only accepts via SMTP receiver Mailstore Windows App requires Windows, no scheduling in the free Home version Mailarchiva Java werid licensing model, cumbersome installation Documents \u00b6 Name Server Client OCR Notes mayanEDMS Docker Web yes not sure why I initially dismissed this .. beautiful, has OCR, easily installed with docker-compose .. ecodms Debian / Docker Web / Windows / Linux yes not very pretty; simple installation logicalDoc Docker Web not in CE nice feature set and relatively easy to set up; lacks OCR in community edition ambar huge docker-compose Web yes ONLY for searching; good for temporary projects seeddms LAMP stack Web ? rather simple, fewer professional features nuxeo Debian / Docker Web no very beautiful; resource intensive and not very intuitive / usable for smaller deployments agorum ? Web yes not successfully demo-ed yet; looks promising though","title":"Software"},{"location":"software.html#software","text":"","title":"Software"},{"location":"software.html#collaboration","text":"A collection of nice software that can be used for collaborative tasks, preferrably self-hosted. Name Description self-hosted Airtable Cloud-hosted Database with a beautiful UI and API integration no CodiMD Free fork of HackMD, realtime collaborative text editor similar to etherpad yes","title":"Collaboration"},{"location":"software.html#archival","text":"An overview of archival software for different purposes.","title":"Archival"},{"location":"software.html#e-mail","text":"Name Type Notes ansemjo/imapfetch Python fetches from IMAP, config via ini , archived in maildir for mutt raymii/NoPriv Python open formats, no search, combination with imapbox and calaca possible, exports to HTML tree mailpiler PHP many dependencies, only accepts via SMTP receiver Mailstore Windows App requires Windows, no scheduling in the free Home version Mailarchiva Java werid licensing model, cumbersome installation","title":"E-Mail"},{"location":"software.html#documents","text":"Name Server Client OCR Notes mayanEDMS Docker Web yes not sure why I initially dismissed this .. beautiful, has OCR, easily installed with docker-compose .. ecodms Debian / Docker Web / Windows / Linux yes not very pretty; simple installation logicalDoc Docker Web not in CE nice feature set and relatively easy to set up; lacks OCR in community edition ambar huge docker-compose Web yes ONLY for searching; good for temporary projects seeddms LAMP stack Web ? rather simple, fewer professional features nuxeo Debian / Docker Web no very beautiful; resource intensive and not very intuitive / usable for smaller deployments agorum ? Web yes not successfully demo-ed yet; looks promising though","title":"Documents"},{"location":"personal/l\u00f6schungsantrag.html","text":"DSGVO L\u00f6schungsantrag \u00b6 Den folgenden Text habe ich am 2018-08-10 an change.org geschickt, um mein Konto zu Schlie\u00dfen und eine vollst\u00e4ndige L\u00f6schung meiner Daten zu beantragen. Ich glaube, dass er sich gut als Vorlage f\u00fcr zuk\u00fcnftige L\u00f6schungsersuchen eignet: Sehr geehrte Damen und Herren, ich m\u00f6chte Sie hiermit auffordern alle personenbezogenen Daten zu meiner Person, die Sie gespeichert haben vollst\u00e4ndig zu l\u00f6schen (\u00a717 DSGVO) und mein Konto zu schlie\u00dfen. Sollte eine vollst\u00e4ndige L\u00f6schung aufgrund von Mindestaufbewahrungsfristen nicht m\u00f6glich sein, so widerspreche ich hiermit mindestens aber einer weiteren Verwendung meiner Daten zu Werbe- und Marktforschungszwecken und der \u00dcbermittlung an Dritte. In diesem Fall bitte ich um Auskunft \u00fcber die verbleibenden gespeicherten Daten. Bitte best\u00e4tigen Sie mir die L\u00f6schung schriftlich per E-Mail. Mit freundlichen Gr\u00fc\u00dfen, Anton Semjonov","title":"DSGVO L\u00f6schungsantrag"},{"location":"personal/l\u00f6schungsantrag.html#dsgvo-loschungsantrag","text":"Den folgenden Text habe ich am 2018-08-10 an change.org geschickt, um mein Konto zu Schlie\u00dfen und eine vollst\u00e4ndige L\u00f6schung meiner Daten zu beantragen. Ich glaube, dass er sich gut als Vorlage f\u00fcr zuk\u00fcnftige L\u00f6schungsersuchen eignet: Sehr geehrte Damen und Herren, ich m\u00f6chte Sie hiermit auffordern alle personenbezogenen Daten zu meiner Person, die Sie gespeichert haben vollst\u00e4ndig zu l\u00f6schen (\u00a717 DSGVO) und mein Konto zu schlie\u00dfen. Sollte eine vollst\u00e4ndige L\u00f6schung aufgrund von Mindestaufbewahrungsfristen nicht m\u00f6glich sein, so widerspreche ich hiermit mindestens aber einer weiteren Verwendung meiner Daten zu Werbe- und Marktforschungszwecken und der \u00dcbermittlung an Dritte. In diesem Fall bitte ich um Auskunft \u00fcber die verbleibenden gespeicherten Daten. Bitte best\u00e4tigen Sie mir die L\u00f6schung schriftlich per E-Mail. Mit freundlichen Gr\u00fc\u00dfen, Anton Semjonov","title":"DSGVO L\u00f6schungsantrag"},{"location":"personal/versicherungen.html","text":"Notizen zur privaten Haftpflichtversicherung \u00b6 G\u00fcnstige Tarife gibt es bspw. bei der HUK-Coburg oder deren Direktversicherungs-Tochter HUK24 . Je nach Selbstbeteiligung und Zusatzleistungen landet man dort bei 40 bis 55 \u20ac. Ein anderer, sehr gut bewerteter Versicherer ist die VHV Gruppe .","title":"Versicherungen"},{"location":"personal/versicherungen.html#notizen-zur-privaten-haftpflichtversicherung","text":"G\u00fcnstige Tarife gibt es bspw. bei der HUK-Coburg oder deren Direktversicherungs-Tochter HUK24 . Je nach Selbstbeteiligung und Zusatzleistungen landet man dort bei 40 bis 55 \u20ac. Ein anderer, sehr gut bewerteter Versicherer ist die VHV Gruppe .","title":"Notizen zur privaten Haftpflichtversicherung"},{"location":"rechenzentrum/index.html","text":"Overview \u00b6 \u201cRechenzentrum\u201d is the name I gave my little deskside homelab. This section contains information on: booting systems over the network performing automated installations managing a small number of machines various general sysadmin tasks","title":"Overview"},{"location":"rechenzentrum/index.html#overview","text":"\u201cRechenzentrum\u201d is the name I gave my little deskside homelab. This section contains information on: booting systems over the network performing automated installations managing a small number of machines various general sysadmin tasks","title":"Overview"},{"location":"rechenzentrum/bootstrap.html","text":"Note This page needs tidying up. Network Booting \u00b6 Combined DHCP responses \u00b6 Last time I changed my PXE procedure to use custom iPXE scripts and compiled iPXE binaries, I ran into the problem that the bootloop needs to be broken somehow, if you specify the iPXE binary as the boot target via DHCP and do not want to recompile your binary with new scripts embedded every time. This assumed dumb DHCP servers, which cannot react to different client classes. And while OpenWRT and the underlying dnsmasq are not exactly dumb , the necessary settings are not comfortably exposed in Luci. So I used an unused DHCP option to specify the real PXE boot target and an embedded iPXE script which reads this option and then chainloads. Whew! Today I learned that clients can assemble responses from multiple DHCP servers and dnsmasq can act as a proxy just fine, meaning it will only serve the settings relevant to PXE booting and leave all the IP assignments, DNS settings, etc. to the main DHCP server in the network. Yay! This means that the same server that shall act as the target for PXE booting (possibly even containing a gRPC-enabled matchbox ) can also serve the relevant DHCP settings so clients will use it. CoreOS containers \u00b6 The CoreOS team provides containers for both matchbox and dnsmasq : quay.io/coreos/matchbox quay.io/coreos/dnsmasq Together with the sample systemd service files provided in matchbox releases in the contrib/systemd/ subdirectory, these can easily be used to bootstrap a fully functional network boot target on top of CoreOS. [Unit] Description=CoreOS matchbox Server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/matchbox\" Environment=\"VERSION=v0.7.1\" Environment=\"MATCHBOX_ADDRESS=0.0.0.0:8080\" Environment=\"MATCHBOX_RPC_ADDRESS=0.0.0.0:8081\" Environment=\"MATCHBOX_LOG_LEVEL=debug\" ExecStartPre=/usr/bin/mkdir -p /etc/matchbox ExecStartPre=/usr/bin/mkdir -p /var/lib/matchbox/assets ExecStart=/usr/bin/rkt run \\ --net=host \\ --inherit-env \\ --trust-keys-from-https \\ --mount volume=data,target=/var/lib/matchbox \\ --mount volume=config,target=/etc/matchbox \\ --volume data,kind=host,source=/var/lib/matchbox \\ --volume config,kind=host,source=/etc/matchbox \\ ${IMAGE}:${VERSION} [Install] WantedBy=multi-user.target [Unit] Description=CoreOS dnsmasq DHCP proxy and TFTP server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/dnsmasq\" Environment=\"VERSION=v0.5.0\" Environment=\"NETWORK=172.26.63.1\" Environment=\"MATCHBOX=172.26.63.242:8080\" # replace with %H ? ExecStart=/usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy=multi-user.target Bootstrap Process \u00b6 This is an overview of the necessary procedures to bootstrap the entire Rechenzentrum\ufffd: bootstrap the network boot target \u2018matchbox\u2019 boot and install CoreOS to disk generate tls keys for matchbox gRPC enable systemd services for matchbox and dnsmasq optionally add custom iPXE scripts in matchbox \u2018 assets and tweak boot option optionally download and store kernel and initramfs locally in assets configure infrastructure with terraform use providers for matchbox and vsphere / libvirt /\u2026 configure profiles for machines that bootstrap until you can ssh in terraform apply to bring them up and have them boot from pxe use ansible to do proper deployments and configuration management bonus points if you use terraform state as inventory for ansible Matchbox \u00b6 Boot CoreOS into RAM through your preferred method, e.g. using the CoreOS ISO or netboot.xyz . Then install to disk with coreos-install : curl -Lo install.json https://ks.surge.sh/matchbox.json sudo coreos-install -d /dev/sda -i install.json sudo reboot Append -o vmware_raw to the installation command if you\u2019re installing on VMware and replace /dev/sda with /dev/vda if you\u2019re installing on KVM (or use scsi-virtio disks). Reboot twice for the autologin to take effect. Then add this matchbox server in your DNS. Install CentOS over Serial Cable \u00b6 Simply using netboot.xyz.kpxe is not sufficient when installing from network with only a serial connection (e.g. on my X10SBA) because it does not allow you to edit the kernel commandline - which does not include console=ttyS0 by default. Thus boot into an iPXE shell and use the following script to start an interactive CentOS install over serial: imgfree set repo http://mirror.23media.de/centos/7/os/x86_64 kernel ${repo}/images/pxeboot/vmlinuz initrd ${repo}/images/pxeboot/initrd.img imgargs vmlinuz ramdisk_size=8192 console=ttyS0,115200 text method=${repo}/ boot Careful when copy-pasting though. I have had incomplete pastes which lead to a missing _64 in the repo URL, etc. Squid proxy \u00b6 Instead of mirroring multiple repositories locally, you could run a Squid proxy in your network and point all yum clients at it. With some URL rewriting you can cache the same packages from many different mirrors in a deduplicated fashion. I used the sameersbn/squid image on a CoreOS host, started with the following arguments: docker run -d --net host --name squid \\ -v /var/spool/squid:/var/spool/squid \\ -v /etc/squid:/etc/squid sameersbn/squid The configuration files cache all .rpm \u2018s from any mirrors. This is probably too open and broad to be used as a general proxy, so be careful. /etc/squid/storeid.db : # /etc/squid/squid.conf acl safe_ports port 80 21 443 acl tls_ports port 443 acl CONNECT method CONNECT http_access deny !safe_ports http_access deny CONNECT !tls_ports http_access allow localhost http_access allow all http_port 3128 # cache yum/rpm downloads: # http://ma.ttwagner.com/lazy-distro-mirrors-with-squid/ # https://serverfault.com/questions/837291/squid-and-caching-of-dnf-yum-downloads cache_replacement_policy heap LFUDA # least-frequently-used cache_dir aufs /var/spool/squid 20000 16 256 # 20GB disk cache maximum_object_size 4096 MB # up to 4GB files store_id_program /usr/lib/squid/storeid_file_rewrite /etc/squid/storeid.db # rewrite all centos mirror urls coredump_dir /var/spool/squid refresh_pattern -i \\.(deb|rpm|tar|tar.gz|tgz)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private refresh_pattern . 0 20% 4320 /etc/squid/storeid.db : \\/([0-9\\.\\-]+)\\/([a-z]+)\\/(x86_64|i386)\\/(.*\\.d?rpm) http://rpmcache.squid.internal/$1/$2/$3/$4 Note: the storeid.db rules need tabs as whitespace, not spaces! And during initial creation I found out that squid sends a quoted string to the helper, so using any regular expression with (.*\\.rpm)$ at the end did not match. Use debug_options ALL,5 and grep for storeId if you\u2019re having problems. Finally, simply add proxy=http://url.to.your.proxy:3128 in your clients\u2019 /etc/yum.conf . reverse proxy \u00b6 With a small addition to your squid.conf you can also make Squid act as a reverse proxy (or \u201caccelerator\u201d in Squid terms) for a mirror of your choice, using the same cache . Although the wiki still says that cache_peer requests do not pass through the storeid helper, it in fact seems to do just that. At the time of this writing, the latest container uses Squid Cache: Version 3.5.27 from Ubuntu\u2019s repositories. ... http_port 3128 http_port 80 accel defaultsite=ftp.halifax.rwth-aachen.de no-vhost cache_peer ftp.halifax.rwth-aachen.de parent 80 0 no-query originserver name=mirror acl mirror dstdomain ftp.halifax.rwth-aachen.de http_access allow mirror cache_peer_access mirror allow mirror cache_peer_access mirror deny all ... With this configuration, you could also use Squid as a mirror to install your machines via kickstart, because also the .../os/x86_64/images/pxeboot/* files will be cached. You might want to amend your refresh_pattern rules to account for that.","title":"Bootstrap"},{"location":"rechenzentrum/bootstrap.html#network-booting","text":"","title":"Network Booting"},{"location":"rechenzentrum/bootstrap.html#combined-dhcp-responses","text":"Last time I changed my PXE procedure to use custom iPXE scripts and compiled iPXE binaries, I ran into the problem that the bootloop needs to be broken somehow, if you specify the iPXE binary as the boot target via DHCP and do not want to recompile your binary with new scripts embedded every time. This assumed dumb DHCP servers, which cannot react to different client classes. And while OpenWRT and the underlying dnsmasq are not exactly dumb , the necessary settings are not comfortably exposed in Luci. So I used an unused DHCP option to specify the real PXE boot target and an embedded iPXE script which reads this option and then chainloads. Whew! Today I learned that clients can assemble responses from multiple DHCP servers and dnsmasq can act as a proxy just fine, meaning it will only serve the settings relevant to PXE booting and leave all the IP assignments, DNS settings, etc. to the main DHCP server in the network. Yay! This means that the same server that shall act as the target for PXE booting (possibly even containing a gRPC-enabled matchbox ) can also serve the relevant DHCP settings so clients will use it.","title":"Combined DHCP responses"},{"location":"rechenzentrum/bootstrap.html#coreos-containers","text":"The CoreOS team provides containers for both matchbox and dnsmasq : quay.io/coreos/matchbox quay.io/coreos/dnsmasq Together with the sample systemd service files provided in matchbox releases in the contrib/systemd/ subdirectory, these can easily be used to bootstrap a fully functional network boot target on top of CoreOS. [Unit] Description=CoreOS matchbox Server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/matchbox\" Environment=\"VERSION=v0.7.1\" Environment=\"MATCHBOX_ADDRESS=0.0.0.0:8080\" Environment=\"MATCHBOX_RPC_ADDRESS=0.0.0.0:8081\" Environment=\"MATCHBOX_LOG_LEVEL=debug\" ExecStartPre=/usr/bin/mkdir -p /etc/matchbox ExecStartPre=/usr/bin/mkdir -p /var/lib/matchbox/assets ExecStart=/usr/bin/rkt run \\ --net=host \\ --inherit-env \\ --trust-keys-from-https \\ --mount volume=data,target=/var/lib/matchbox \\ --mount volume=config,target=/etc/matchbox \\ --volume data,kind=host,source=/var/lib/matchbox \\ --volume config,kind=host,source=/etc/matchbox \\ ${IMAGE}:${VERSION} [Install] WantedBy=multi-user.target [Unit] Description=CoreOS dnsmasq DHCP proxy and TFTP server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/dnsmasq\" Environment=\"VERSION=v0.5.0\" Environment=\"NETWORK=172.26.63.1\" Environment=\"MATCHBOX=172.26.63.242:8080\" # replace with %H ? ExecStart=/usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy=multi-user.target","title":"CoreOS containers"},{"location":"rechenzentrum/bootstrap.html#bootstrap-process","text":"This is an overview of the necessary procedures to bootstrap the entire Rechenzentrum\ufffd: bootstrap the network boot target \u2018matchbox\u2019 boot and install CoreOS to disk generate tls keys for matchbox gRPC enable systemd services for matchbox and dnsmasq optionally add custom iPXE scripts in matchbox \u2018 assets and tweak boot option optionally download and store kernel and initramfs locally in assets configure infrastructure with terraform use providers for matchbox and vsphere / libvirt /\u2026 configure profiles for machines that bootstrap until you can ssh in terraform apply to bring them up and have them boot from pxe use ansible to do proper deployments and configuration management bonus points if you use terraform state as inventory for ansible","title":"Bootstrap Process"},{"location":"rechenzentrum/bootstrap.html#matchbox","text":"Boot CoreOS into RAM through your preferred method, e.g. using the CoreOS ISO or netboot.xyz . Then install to disk with coreos-install : curl -Lo install.json https://ks.surge.sh/matchbox.json sudo coreos-install -d /dev/sda -i install.json sudo reboot Append -o vmware_raw to the installation command if you\u2019re installing on VMware and replace /dev/sda with /dev/vda if you\u2019re installing on KVM (or use scsi-virtio disks). Reboot twice for the autologin to take effect. Then add this matchbox server in your DNS.","title":"Matchbox"},{"location":"rechenzentrum/bootstrap.html#install-centos-over-serial-cable","text":"Simply using netboot.xyz.kpxe is not sufficient when installing from network with only a serial connection (e.g. on my X10SBA) because it does not allow you to edit the kernel commandline - which does not include console=ttyS0 by default. Thus boot into an iPXE shell and use the following script to start an interactive CentOS install over serial: imgfree set repo http://mirror.23media.de/centos/7/os/x86_64 kernel ${repo}/images/pxeboot/vmlinuz initrd ${repo}/images/pxeboot/initrd.img imgargs vmlinuz ramdisk_size=8192 console=ttyS0,115200 text method=${repo}/ boot Careful when copy-pasting though. I have had incomplete pastes which lead to a missing _64 in the repo URL, etc.","title":"Install CentOS over Serial Cable"},{"location":"rechenzentrum/bootstrap.html#squid-proxy","text":"Instead of mirroring multiple repositories locally, you could run a Squid proxy in your network and point all yum clients at it. With some URL rewriting you can cache the same packages from many different mirrors in a deduplicated fashion. I used the sameersbn/squid image on a CoreOS host, started with the following arguments: docker run -d --net host --name squid \\ -v /var/spool/squid:/var/spool/squid \\ -v /etc/squid:/etc/squid sameersbn/squid The configuration files cache all .rpm \u2018s from any mirrors. This is probably too open and broad to be used as a general proxy, so be careful. /etc/squid/storeid.db : # /etc/squid/squid.conf acl safe_ports port 80 21 443 acl tls_ports port 443 acl CONNECT method CONNECT http_access deny !safe_ports http_access deny CONNECT !tls_ports http_access allow localhost http_access allow all http_port 3128 # cache yum/rpm downloads: # http://ma.ttwagner.com/lazy-distro-mirrors-with-squid/ # https://serverfault.com/questions/837291/squid-and-caching-of-dnf-yum-downloads cache_replacement_policy heap LFUDA # least-frequently-used cache_dir aufs /var/spool/squid 20000 16 256 # 20GB disk cache maximum_object_size 4096 MB # up to 4GB files store_id_program /usr/lib/squid/storeid_file_rewrite /etc/squid/storeid.db # rewrite all centos mirror urls coredump_dir /var/spool/squid refresh_pattern -i \\.(deb|rpm|tar|tar.gz|tgz)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private refresh_pattern . 0 20% 4320 /etc/squid/storeid.db : \\/([0-9\\.\\-]+)\\/([a-z]+)\\/(x86_64|i386)\\/(.*\\.d?rpm) http://rpmcache.squid.internal/$1/$2/$3/$4 Note: the storeid.db rules need tabs as whitespace, not spaces! And during initial creation I found out that squid sends a quoted string to the helper, so using any regular expression with (.*\\.rpm)$ at the end did not match. Use debug_options ALL,5 and grep for storeId if you\u2019re having problems. Finally, simply add proxy=http://url.to.your.proxy:3128 in your clients\u2019 /etc/yum.conf .","title":"Squid proxy"},{"location":"rechenzentrum/bootstrap.html#reverse-proxy","text":"With a small addition to your squid.conf you can also make Squid act as a reverse proxy (or \u201caccelerator\u201d in Squid terms) for a mirror of your choice, using the same cache . Although the wiki still says that cache_peer requests do not pass through the storeid helper, it in fact seems to do just that. At the time of this writing, the latest container uses Squid Cache: Version 3.5.27 from Ubuntu\u2019s repositories. ... http_port 3128 http_port 80 accel defaultsite=ftp.halifax.rwth-aachen.de no-vhost cache_peer ftp.halifax.rwth-aachen.de parent 80 0 no-query originserver name=mirror acl mirror dstdomain ftp.halifax.rwth-aachen.de http_access allow mirror cache_peer_access mirror allow mirror cache_peer_access mirror deny all ... With this configuration, you could also use Squid as a mirror to install your machines via kickstart, because also the .../os/x86_64/images/pxeboot/* files will be cached. You might want to amend your refresh_pattern rules to account for that.","title":"reverse proxy"},{"location":"rechenzentrum/docker-in-kvm.html","text":"Docker in QEMU/KVM \u00b6 Some applications may require a properly isolated Docker engine where users of the API have every freedom but when they must not be able to compromise the host security. Since access to the Docker socket is equivalent to being root ( or worse ) we must preferably run the engine on a seperate machine. Long story short: virtualization with QEMU/KVM provides all the required isolation and CoreOS is easy to deploy and bundles Docker by default. The following steps are designed for a CentOS 7 hypervisor. Prerequisites \u00b6 First of all, we need to prepare our hypervisor, so install QEMU and libvirt. yum install qemu-kvm libvirt virt-install modprobe kvm systemctl enable --now libvirtd Make sure you have hardware virtualization available. If you\u2019re running in a virtual machine already you may need to enable passthrough explicitly. On an Intel machine you should have a module kvm_intel loaded as well. Info We are going to use virt-install as well, however the version in EPEL is not recent enough to use the kernel= and initrd= arguments with --location . Thus prefer a local manager and append --connect qemu+ssh://root@hypervisor/system to virsh or virt-install commands. Boot a CoreOS Virtual Machine \u00b6 There is a guide on how to boot CoreOS with libvirt but I prefer to perform a clean installation to disk. Therefore we need to boot CoreOS to RAM and deploy using an Ignition configuration. Preferably, this is done with the provided PXE images. Depending on your version of virt-install there are different installation methods available: in the terminal via text console or remotely over VNC. Via Text Console (modern virt-install ) \u00b6 If you don\u2019t want to bother with VNC connections and would prefer to install via a text console on the hypervisor itself, you can download and run the CoreOS vmlinuz and cpio.gz directly by specifying them in the --location argument: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart --graphics none \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --location \"https://stable.release.core-os.net/amd64-usr/current/,kernel=coreos_production_pxe.vmlinuz,initrd=coreos_production_pxe_image.cpio.gz\" \\ --extra-args \"coreos.autologin console=ttyS0\" If you prefer to download and verify an ISO locally instead, you can substitute the --location argument: --location ../path/to/coreos.iso,kernel=/coreos/vmlinuz,initrd=/coreos/cpio.gz \\ This is useful when you\u2019re doing many installs to avoid the repeated downloads. Hint You can find files inside an ISO with isoinfo -Jf -i /path/to/disc.iso . Via Text Console (older virt-install ) \u00b6 Older versions of virt-install \u2013 among them version 1.5.0 that is shipped with CentOS 7 \u2013 do not support the --location ...,kernel=...,initrd=... syntax and complain about unreachable URLs. In this case you can download the files and fake a Debain installation directory that is autodetected simply by passing the directory path to virt-install . Download and verify the PXE image as per the CoreOS docs: cd /var/lib/libvirt/images mkdir -p coreos && cd coreos stable=https://stable.release.core-os.net/amd64-usr/current/ wget $stable/coreos_production_pxe.vmlinuz wget $stable/coreos_production_pxe.vmlinuz.sig wget $stable/coreos_production_pxe_image.cpio.gz wget $stable/coreos_production_pxe_image.cpio.gz.sig gpg --verify coreos_production_pxe.vmlinuz.sig gpg --verify coreos_production_pxe_image.cpio.gz.sig Create a fake MANIFEST and a directory structure that mimics a Debian netboot installer: mkdir -p amd64/current/images/netboot/debian-installer/amd64/ echo debian-installer > amd64/current/images/MANIFEST ln -sr coreos_production_pxe.vmlinuz amd64/current/images/netboot/debian-installer/amd64/linux ln -sr coreos_production_pxe_image.cpio.gz amd64/current/images/netboot/debian-installer/amd64/initrd.gz Pass the amd64 subdirectory as the installer location: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart --graphics none \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --location /var/lib/libvirt/images/coreos/amd64 \\ --extra-args \"coreos.autologin console=ttyS0\" This method is probably useful for other distributions that don\u2019t get detected automatically either as well. Via VNC Viewer \u00b6 Sometimes an installer may just refuse to start on the serial console or you\u2019re more confident in a graphical installer. This method also applies when you want to use an ISO image without specifying additional kernel parameters. As an example, this section uses an image of netboot.xyz , which can be used to interactively boot many different distributions. First, download the netboot.xyz image: cd /var/lib/libvirt/boot curl -LO https://boot.netboot.xyz/ipxe/netboot.xyz.iso Now create the virtual machine with virt-install , specifying the ISO with the --cdrom argument: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --cdrom /var/lib/libvirt/boot/netboot.xyz.iso This should start the installation process and enable a VNC console. You can check the port with virsh vncdisplay runner and verify with ss -tln if in doubt. In my case a default of :0 corresponds to port 5900 on the host, so temporarily open that port in the firewall: firewall-cmd --add-port 5900/tcp Connect with your favourite VNC client and complete the installation. Hint You can\u2019t currently change the keyboard map on the console. Set a password with sudo passwd core and connect with ssh instead if you run into problems. Install CoreOS to Disk \u00b6 Prepare Ignition \u00b6 By now you should have prepared an Ignition configuration. There is of course a lot of variation possible here but most importantly you should enable rngd.service and docker.service and make sure that you can connect with SSH public keys. Mine looks somewhat like this: --- # enable docker service systemd: units: - name: rngd.service enabled: yes - name: docker.service enabled: yes # ssh public keys passwd: users: - name: core ssh_authorized_keys: - # add your keys here # automatic updates during maintenance window locksmith: reboot_strategy: reboot window_start: 04:00 window_length: 3h # enable console autologin storage: filesystems: - name: OEM mount: device: /dev/disk/by-label/OEM format: ext4 files: - filesystem: OEM path: /grub.cfg mode: 0644 append: true contents: inline: | set linux_append=\"$linux_append coreos.autologin\" Installation \u00b6 After transpiling, I am using surge.sh to host small static files quickly. Download the configuration and finally install CoreOS to disk: curl -LO \"https://ks.surge.sh/coreos/docker.json\" sudo coreos-install -d /dev/vda -i docker.json sudo udevadm settle sudo reboot Miscellaneous \u00b6 Fixed DHCP Address \u00b6 You can add a fixed address for this virtual machine by creating an IP assignment for its MAC address with virsh : virsh net-dhcp-leases default # see current leases virsh net-update default add-last ip-dhcp-host \\ --xml \"<host mac='52:54:00:e7:b6:4d' ip='192.168.122.2' />\" \\ --live --config SSH Client Configuration \u00b6 Add an appropriate SSH config on the hypervisor: Host runner User core HostName 192.168.122.2 StrictHostKeyChecking no UserKnownHostsFile /dev/null","title":"Docker in QEMU/KVM"},{"location":"rechenzentrum/docker-in-kvm.html#docker-in-qemukvm","text":"Some applications may require a properly isolated Docker engine where users of the API have every freedom but when they must not be able to compromise the host security. Since access to the Docker socket is equivalent to being root ( or worse ) we must preferably run the engine on a seperate machine. Long story short: virtualization with QEMU/KVM provides all the required isolation and CoreOS is easy to deploy and bundles Docker by default. The following steps are designed for a CentOS 7 hypervisor.","title":"Docker in QEMU/KVM"},{"location":"rechenzentrum/docker-in-kvm.html#prerequisites","text":"First of all, we need to prepare our hypervisor, so install QEMU and libvirt. yum install qemu-kvm libvirt virt-install modprobe kvm systemctl enable --now libvirtd Make sure you have hardware virtualization available. If you\u2019re running in a virtual machine already you may need to enable passthrough explicitly. On an Intel machine you should have a module kvm_intel loaded as well. Info We are going to use virt-install as well, however the version in EPEL is not recent enough to use the kernel= and initrd= arguments with --location . Thus prefer a local manager and append --connect qemu+ssh://root@hypervisor/system to virsh or virt-install commands.","title":"Prerequisites"},{"location":"rechenzentrum/docker-in-kvm.html#boot-a-coreos-virtual-machine","text":"There is a guide on how to boot CoreOS with libvirt but I prefer to perform a clean installation to disk. Therefore we need to boot CoreOS to RAM and deploy using an Ignition configuration. Preferably, this is done with the provided PXE images. Depending on your version of virt-install there are different installation methods available: in the terminal via text console or remotely over VNC.","title":"Boot a CoreOS Virtual Machine"},{"location":"rechenzentrum/docker-in-kvm.html#via-text-console-modern-virt-install","text":"If you don\u2019t want to bother with VNC connections and would prefer to install via a text console on the hypervisor itself, you can download and run the CoreOS vmlinuz and cpio.gz directly by specifying them in the --location argument: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart --graphics none \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --location \"https://stable.release.core-os.net/amd64-usr/current/,kernel=coreos_production_pxe.vmlinuz,initrd=coreos_production_pxe_image.cpio.gz\" \\ --extra-args \"coreos.autologin console=ttyS0\" If you prefer to download and verify an ISO locally instead, you can substitute the --location argument: --location ../path/to/coreos.iso,kernel=/coreos/vmlinuz,initrd=/coreos/cpio.gz \\ This is useful when you\u2019re doing many installs to avoid the repeated downloads. Hint You can find files inside an ISO with isoinfo -Jf -i /path/to/disc.iso .","title":"Via Text Console (modern virt-install)"},{"location":"rechenzentrum/docker-in-kvm.html#via-text-console-older-virt-install","text":"Older versions of virt-install \u2013 among them version 1.5.0 that is shipped with CentOS 7 \u2013 do not support the --location ...,kernel=...,initrd=... syntax and complain about unreachable URLs. In this case you can download the files and fake a Debain installation directory that is autodetected simply by passing the directory path to virt-install . Download and verify the PXE image as per the CoreOS docs: cd /var/lib/libvirt/images mkdir -p coreos && cd coreos stable=https://stable.release.core-os.net/amd64-usr/current/ wget $stable/coreos_production_pxe.vmlinuz wget $stable/coreos_production_pxe.vmlinuz.sig wget $stable/coreos_production_pxe_image.cpio.gz wget $stable/coreos_production_pxe_image.cpio.gz.sig gpg --verify coreos_production_pxe.vmlinuz.sig gpg --verify coreos_production_pxe_image.cpio.gz.sig Create a fake MANIFEST and a directory structure that mimics a Debian netboot installer: mkdir -p amd64/current/images/netboot/debian-installer/amd64/ echo debian-installer > amd64/current/images/MANIFEST ln -sr coreos_production_pxe.vmlinuz amd64/current/images/netboot/debian-installer/amd64/linux ln -sr coreos_production_pxe_image.cpio.gz amd64/current/images/netboot/debian-installer/amd64/initrd.gz Pass the amd64 subdirectory as the installer location: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart --graphics none \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --location /var/lib/libvirt/images/coreos/amd64 \\ --extra-args \"coreos.autologin console=ttyS0\" This method is probably useful for other distributions that don\u2019t get detected automatically either as well.","title":"Via Text Console (older virt-install)"},{"location":"rechenzentrum/docker-in-kvm.html#via-vnc-viewer","text":"Sometimes an installer may just refuse to start on the serial console or you\u2019re more confident in a graphical installer. This method also applies when you want to use an ISO image without specifying additional kernel parameters. As an example, this section uses an image of netboot.xyz , which can be used to interactively boot many different distributions. First, download the netboot.xyz image: cd /var/lib/libvirt/boot curl -LO https://boot.netboot.xyz/ipxe/netboot.xyz.iso Now create the virtual machine with virt-install , specifying the ISO with the --cdrom argument: virt-install --name core --memory 2048 --vcpus 2 \\ --accelerate --rng /dev/urandom --autostart \\ --disk size=20,bus=virtio --os-variant virtio26 \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --cdrom /var/lib/libvirt/boot/netboot.xyz.iso This should start the installation process and enable a VNC console. You can check the port with virsh vncdisplay runner and verify with ss -tln if in doubt. In my case a default of :0 corresponds to port 5900 on the host, so temporarily open that port in the firewall: firewall-cmd --add-port 5900/tcp Connect with your favourite VNC client and complete the installation. Hint You can\u2019t currently change the keyboard map on the console. Set a password with sudo passwd core and connect with ssh instead if you run into problems.","title":"Via VNC Viewer"},{"location":"rechenzentrum/docker-in-kvm.html#install-coreos-to-disk","text":"","title":"Install CoreOS to Disk"},{"location":"rechenzentrum/docker-in-kvm.html#prepare-ignition","text":"By now you should have prepared an Ignition configuration. There is of course a lot of variation possible here but most importantly you should enable rngd.service and docker.service and make sure that you can connect with SSH public keys. Mine looks somewhat like this: --- # enable docker service systemd: units: - name: rngd.service enabled: yes - name: docker.service enabled: yes # ssh public keys passwd: users: - name: core ssh_authorized_keys: - # add your keys here # automatic updates during maintenance window locksmith: reboot_strategy: reboot window_start: 04:00 window_length: 3h # enable console autologin storage: filesystems: - name: OEM mount: device: /dev/disk/by-label/OEM format: ext4 files: - filesystem: OEM path: /grub.cfg mode: 0644 append: true contents: inline: | set linux_append=\"$linux_append coreos.autologin\"","title":"Prepare Ignition"},{"location":"rechenzentrum/docker-in-kvm.html#installation","text":"After transpiling, I am using surge.sh to host small static files quickly. Download the configuration and finally install CoreOS to disk: curl -LO \"https://ks.surge.sh/coreos/docker.json\" sudo coreos-install -d /dev/vda -i docker.json sudo udevadm settle sudo reboot","title":"Installation"},{"location":"rechenzentrum/docker-in-kvm.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"rechenzentrum/docker-in-kvm.html#fixed-dhcp-address","text":"You can add a fixed address for this virtual machine by creating an IP assignment for its MAC address with virsh : virsh net-dhcp-leases default # see current leases virsh net-update default add-last ip-dhcp-host \\ --xml \"<host mac='52:54:00:e7:b6:4d' ip='192.168.122.2' />\" \\ --live --config","title":"Fixed DHCP Address"},{"location":"rechenzentrum/docker-in-kvm.html#ssh-client-configuration","text":"Add an appropriate SSH config on the hypervisor: Host runner User core HostName 192.168.122.2 StrictHostKeyChecking no UserKnownHostsFile /dev/null","title":"SSH Client Configuration"},{"location":"rechenzentrum/flynn.html","text":"Installing Flynn on Debian \u00b6 Flynn is mostly compatible with Debian. Some packages require the contrib repository though. Installing a single-node Flynn \u201ccluster\u201d is as easy as: Perform a clean Debian 9 Stretch installation. Download the official script from dl.flynn.io/install-flynn Patch the function is_ubuntu_xenial() to also check for Debian GNU/Linux 9 . Enable or add the contrib repository in /etc/apt/sources.list . Run the install-flynn script. Start and enable flynn-host.service . Export CLUSTER_DOMAIN to the appropriate FQDN. Apps will be $app.CLUSTER_DOMAIN . Bootstrap with flynn-host bootstrap .","title":"Flynn"},{"location":"rechenzentrum/flynn.html#installing-flynn-on-debian","text":"Flynn is mostly compatible with Debian. Some packages require the contrib repository though. Installing a single-node Flynn \u201ccluster\u201d is as easy as: Perform a clean Debian 9 Stretch installation. Download the official script from dl.flynn.io/install-flynn Patch the function is_ubuntu_xenial() to also check for Debian GNU/Linux 9 . Enable or add the contrib repository in /etc/apt/sources.list . Run the install-flynn script. Start and enable flynn-host.service . Export CLUSTER_DOMAIN to the appropriate FQDN. Apps will be $app.CLUSTER_DOMAIN . Bootstrap with flynn-host bootstrap .","title":"Installing Flynn on Debian"},{"location":"rechenzentrum/kvm-on-alpine.html","text":"KVM on Alpine \u00b6 Installing a KVM hypervisor with absolutely minimal footprint: Install Alpine \u00b6 Install Alpine Linux by booting from ISO or via netboot and running setup-alpine , choosing sys as the disktype. Packages \u00b6 Install KVM packages: apk add qemu-system-x86_64 libvirt libvirt-daemon dbus polkit qemu-img Load Modules \u00b6 Reboot or just load necessary kernel modules: modprobe kvm-intel br_netfilter Hint br_netfilter is required for the network bridge below. Bridge Interface \u00b6 Add a bridge configuration in /etc/network/interfaces : auto lo iface lo inet loopback auto br0 iface br0 inet dhcp pre-up modprobe br_netfilter pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-arptables pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-ip6tables bridge_ports eth0","title":"KVM on Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#kvm-on-alpine","text":"Installing a KVM hypervisor with absolutely minimal footprint:","title":"KVM on Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#install-alpine","text":"Install Alpine Linux by booting from ISO or via netboot and running setup-alpine , choosing sys as the disktype.","title":"Install Alpine"},{"location":"rechenzentrum/kvm-on-alpine.html#packages","text":"Install KVM packages: apk add qemu-system-x86_64 libvirt libvirt-daemon dbus polkit qemu-img","title":"Packages"},{"location":"rechenzentrum/kvm-on-alpine.html#load-modules","text":"Reboot or just load necessary kernel modules: modprobe kvm-intel br_netfilter Hint br_netfilter is required for the network bridge below.","title":"Load Modules"},{"location":"rechenzentrum/kvm-on-alpine.html#bridge-interface","text":"Add a bridge configuration in /etc/network/interfaces : auto lo iface lo inet loopback auto br0 iface br0 inet dhcp pre-up modprobe br_netfilter pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-arptables pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables pre-up echo 0 > /proc/sys/net/bridge/bridge-nf-call-ip6tables bridge_ports eth0","title":"Bridge Interface"},{"location":"rechenzentrum/netboot.html","text":"Network Boot \u00b6 Todo This page is a work in progress. See the coreos dnsmasq image for details on how to create a simple dnsmasq container. The service that is started with rkt on my matchbox host is: # from: coreos/matchbox --> contrib/systemd/matchbox-for-tectonic.service # from: coreos/matchbox --> contrib/dnsmasq/README.md [Unit] Description=CoreOS dnsmasq DHCP proxy and TFTP server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/dnsmasq\" Environment=\"VERSION=v0.5.0\" Environment=\"NETWORK=172.26.63.1\" Environment=\"MATCHBOX=%H:8080\" ExecStart=/usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy=multi-user.target Important bits are probably --net host and the userclass , pxe-service and dhcp-range stuff in dnsmasq\u2019s options. I also need to build iPXE binaries: undionly.kpxe (bios -> ipxe) ipxe.efi (uefi -> ipxe) With the two files above present in /var/tftp the following seems to work: dnsmasq -d -q --port 0 \\ --dhcp-range=172.26.63.0,proxy --enable-tftp --tftp-root=/var/tftp \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"chainload bios --> ipxe\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,BC_EFI,\"chainload bc_efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,BC_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,x86-64_EFI,\"chainload efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,x86-64_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe Hint Add --log-dhcp to get more verbose information about served DHCP requests.","title":"Network Boot"},{"location":"rechenzentrum/netboot.html#network-boot","text":"Todo This page is a work in progress. See the coreos dnsmasq image for details on how to create a simple dnsmasq container. The service that is started with rkt on my matchbox host is: # from: coreos/matchbox --> contrib/systemd/matchbox-for-tectonic.service # from: coreos/matchbox --> contrib/dnsmasq/README.md [Unit] Description=CoreOS dnsmasq DHCP proxy and TFTP server Documentation=https://github.com/coreos/matchbox [Service] Environment=\"IMAGE=quay.io/coreos/dnsmasq\" Environment=\"VERSION=v0.5.0\" Environment=\"NETWORK=172.26.63.1\" Environment=\"MATCHBOX=%H:8080\" ExecStart=/usr/bin/rkt run \\ --net=host \\ --trust-keys-from-https \\ ${IMAGE}:${VERSION} \\ --caps-retain=CAP_NET_ADMIN,CAP_NET_BIND_SERVICE,CAP_SETGID,CAP_SETUID,CAP_NET_RAW \\ -- -d -q \\ --dhcp-range=${NETWORK},proxy,255.255.255.0 \\ --enable-tftp --tftp-root=/var/lib/tftpboot \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"PXE chainload to iPXE\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"iPXE\",http://${MATCHBOX}/boot.ipxe \\ --log-queries \\ --log-dhcp [Install] WantedBy=multi-user.target Important bits are probably --net host and the userclass , pxe-service and dhcp-range stuff in dnsmasq\u2019s options. I also need to build iPXE binaries: undionly.kpxe (bios -> ipxe) ipxe.efi (uefi -> ipxe) With the two files above present in /var/tftp the following seems to work: dnsmasq -d -q --port 0 \\ --dhcp-range=172.26.63.0,proxy --enable-tftp --tftp-root=/var/tftp \\ --dhcp-userclass=set:ipxe,iPXE \\ --pxe-service=tag:#ipxe,x86PC,\"chainload bios --> ipxe\",undionly.kpxe \\ --pxe-service=tag:ipxe,x86PC,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,BC_EFI,\"chainload bc_efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,BC_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe \\ --pxe-service=tag:#ipxe,x86-64_EFI,\"chainload efi --> ipxe\",ipxe.efi \\ --pxe-service=tag:ipxe,x86-64_EFI,\"load menu\",http://boot.rz.semjonov.de/ks/menu.ipxe Hint Add --log-dhcp to get more verbose information about served DHCP requests.","title":"Network Boot"},{"location":"rechenzentrum/tls-host-aliases.html","text":"TLS for Host Aliases \u00b6 You can request certificates for host aliases without creating a seperate host entry with no enrolled machine. Minimum version According to this discussion at least FreeIPA version 4.5 is required. Add Principal Alias \u00b6 In FreeIPA on the Identity > Services page add a HTTP service for an actual, enrolled host. Edit that service and add a principal alias: Service Settings Principal alias HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [Delete] HTTP/s3.rz.semjonov.de@RZ.SEMJONOV.DE [Delete] [Add] Request Certificate \u00b6 Request the certificate for the alias with ipa-getcert : $ export S3=s3.rz.semjonov.de $ export tls=/etc/pki/tls $ ipa-getcert request \\ -k $tls/private/$S3.key \\ -f $tls/certs/$S3.crt \\ -D $S3 \\ -N CN=ifrit.rz.semjonov.de \\ -K HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE \\ -I $S3 $ ipa-getcert list Number of certificates and requests being tracked: 1. Request ID 's3.rz.semjonov.de': [...] subject: CN=ifrit.rz.semjonov.de,OU=Rechenzentrum,O=rz.semjonov.de,C=DE expires: 2020-07-26 22:04:55 UTC dns: s3.rz.semjonov.de,ifrit.rz.semjonov.de principal name: HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [...]","title":"TLS for Host Aliases"},{"location":"rechenzentrum/tls-host-aliases.html#tls-for-host-aliases","text":"You can request certificates for host aliases without creating a seperate host entry with no enrolled machine. Minimum version According to this discussion at least FreeIPA version 4.5 is required.","title":"TLS for Host Aliases"},{"location":"rechenzentrum/tls-host-aliases.html#add-principal-alias","text":"In FreeIPA on the Identity > Services page add a HTTP service for an actual, enrolled host. Edit that service and add a principal alias: Service Settings Principal alias HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [Delete] HTTP/s3.rz.semjonov.de@RZ.SEMJONOV.DE [Delete] [Add]","title":"Add Principal Alias"},{"location":"rechenzentrum/tls-host-aliases.html#request-certificate","text":"Request the certificate for the alias with ipa-getcert : $ export S3=s3.rz.semjonov.de $ export tls=/etc/pki/tls $ ipa-getcert request \\ -k $tls/private/$S3.key \\ -f $tls/certs/$S3.crt \\ -D $S3 \\ -N CN=ifrit.rz.semjonov.de \\ -K HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE \\ -I $S3 $ ipa-getcert list Number of certificates and requests being tracked: 1. Request ID 's3.rz.semjonov.de': [...] subject: CN=ifrit.rz.semjonov.de,OU=Rechenzentrum,O=rz.semjonov.de,C=DE expires: 2020-07-26 22:04:55 UTC dns: s3.rz.semjonov.de,ifrit.rz.semjonov.de principal name: HTTP/ifrit.rz.semjonov.de@RZ.SEMJONOV.DE [...]","title":"Request Certificate"},{"location":"security/net-disk-decryption.html","text":"Network Disk Decryption \u00b6 Recently RedHat 7.4 introduced the possibility to bind your encrypted disks to a network presence. It is called Network-Bound Disk Encryption and uses the projects tang and clevis . In essence, an encrypted payload is transformed with some ECDH key exchange magic with the tang server and the disk is decrypted automatically. If however the tang server is unavailable, this method fails and you must fall back to manually entering a passphrase. I decided this is a nice addition to my systemd decryption target , so here\u2019s how I implemented it: tang server \u00b6 First of all, install a release of the tang server. There\u2019s a package for CentOS and a package in the AUR for Arch Linux. There are probably others, too. But it shouldn\u2019t be too hard to build it yourself either. yum install tang I didn\u2019t like tang running on port 80 by default, so I changed it with systemctl edit tangd.socket : [Socket] ListenStream= ListenStream=51653 Start and enable the service. Make sure to open the firewall on that port. systemctl enable --now tangd.socket You can now make sure that a key is present and print the public key: tang-show-keys 51653 4yWvhO4ZthpAGHDmdMn78Pe2Bg0 clevis client \u00b6 Now for the client part on my fileserver. There is a nice post on the RedHat blog describing the entire procedure. I\u2019m assuming you already have some encrypted disks that you want to set up for network-bound decryption. First, install clevis: yum install clevis clevis-luks And make sure that you can reach your tang server: curl -f tang.yourdomain:51653/adv | jq . Now we bind a secret to this tang server and add it as a new key on our LUKS disks. This will use the luksmeta storage, so you might want to take header backups on all disks to avoid data loss: cryptsetup luksHeaderBackup /dev/disk/by-id/ata-... --header-backup-file ... Then bind the disks to the tang server: clevis luks bind -d /dev/disk/by-id/ata-... \\ tang '{\"url\":\"http://tang.yourdomain:51653\"}' The advertisement contains the following signing keys: 4yWvhO4ZthpAGHDmdMn78Pe2Bg0 Do you wish to trust these keys? [ynYN] y ... Make sure the key matches the tang-show-keys output above! You\u2019ll be asked to initialize the LUKS metadata storage and must then enter an existing passphrase to add the newly bound secret as a new encryption key to your disk. I didn\u2019t bother to setup automatic decryption on boot since I already have a semi-automatic decryption environment in place with my systemd decryption target. I\u2019m fine with decrypting disks with clevis manually: clevis luks unlock -d /dev/disk/by-id/ata-... -n mappername However, I automated this for all four disks in my array with a small script, which reads the disks and names from /etc/crypttab and then starts continue.service : #!/bin/sh # unlock disks with tang and clevis echo \"+ unlock disks\" while read name disk opts; do echo \" $disk\" clevis luks unlock -d \"$disk\" -n \"$name\" done < /etc/crypttab # continue system startup sleep 2 echo \"+ continue startup\" systemctl start continue.service","title":"Network Disk Decryption"},{"location":"security/net-disk-decryption.html#network-disk-decryption","text":"Recently RedHat 7.4 introduced the possibility to bind your encrypted disks to a network presence. It is called Network-Bound Disk Encryption and uses the projects tang and clevis . In essence, an encrypted payload is transformed with some ECDH key exchange magic with the tang server and the disk is decrypted automatically. If however the tang server is unavailable, this method fails and you must fall back to manually entering a passphrase. I decided this is a nice addition to my systemd decryption target , so here\u2019s how I implemented it:","title":"Network Disk Decryption"},{"location":"security/net-disk-decryption.html#tang-server","text":"First of all, install a release of the tang server. There\u2019s a package for CentOS and a package in the AUR for Arch Linux. There are probably others, too. But it shouldn\u2019t be too hard to build it yourself either. yum install tang I didn\u2019t like tang running on port 80 by default, so I changed it with systemctl edit tangd.socket : [Socket] ListenStream= ListenStream=51653 Start and enable the service. Make sure to open the firewall on that port. systemctl enable --now tangd.socket You can now make sure that a key is present and print the public key: tang-show-keys 51653 4yWvhO4ZthpAGHDmdMn78Pe2Bg0","title":"tang server"},{"location":"security/net-disk-decryption.html#clevis-client","text":"Now for the client part on my fileserver. There is a nice post on the RedHat blog describing the entire procedure. I\u2019m assuming you already have some encrypted disks that you want to set up for network-bound decryption. First, install clevis: yum install clevis clevis-luks And make sure that you can reach your tang server: curl -f tang.yourdomain:51653/adv | jq . Now we bind a secret to this tang server and add it as a new key on our LUKS disks. This will use the luksmeta storage, so you might want to take header backups on all disks to avoid data loss: cryptsetup luksHeaderBackup /dev/disk/by-id/ata-... --header-backup-file ... Then bind the disks to the tang server: clevis luks bind -d /dev/disk/by-id/ata-... \\ tang '{\"url\":\"http://tang.yourdomain:51653\"}' The advertisement contains the following signing keys: 4yWvhO4ZthpAGHDmdMn78Pe2Bg0 Do you wish to trust these keys? [ynYN] y ... Make sure the key matches the tang-show-keys output above! You\u2019ll be asked to initialize the LUKS metadata storage and must then enter an existing passphrase to add the newly bound secret as a new encryption key to your disk. I didn\u2019t bother to setup automatic decryption on boot since I already have a semi-automatic decryption environment in place with my systemd decryption target. I\u2019m fine with decrypting disks with clevis manually: clevis luks unlock -d /dev/disk/by-id/ata-... -n mappername However, I automated this for all four disks in my array with a small script, which reads the disks and names from /etc/crypttab and then starts continue.service : #!/bin/sh # unlock disks with tang and clevis echo \"+ unlock disks\" while read name disk opts; do echo \" $disk\" clevis luks unlock -d \"$disk\" -n \"$name\" done < /etc/crypttab # continue system startup sleep 2 echo \"+ continue startup\" systemctl start continue.service","title":"clevis client"},{"location":"security/secureboot.html","text":"Secureboot \u00b6 These are guides to install your system with your own secureboot keys and enforce signed Linux kernels. Tools \u00b6 Some useful tools for this job: name description ansemjo/mkefikeys generate signing keys ansemjo/mksignkernels bundle and sign kernel images Guides \u00b6 Arch Linux \u00b6 TODO Fedora \u00b6 Installation \u00b6 Do a somewhat standard installation on an UEFI system. I used the Fedora 28 Server netinst image. During partitioning, make sure to select at least \u201ccustom\u201d partitioning and add a seperate EFI system partition in /boot/efi . Tick the boxes to encrypt your / and any swap partitions you might create. Technically, a seperate /boot partition is not required with the bundled kernels we are going to use but Anaconda complains otherwise and you will not be able to boot the system after installation. You could probably do all these steps from within a live rescue system but I haven\u2019t tried that route yet. Required Packages \u00b6 You will need to additionally (after a minimal setup) install: * git * make * binutils * sbsigntools Then clone and install the above two tools: mkefikeys and mksignkernels . cd /tmp/... git clone https://github.com/ansemjo/mkefikeys git clone https://github.com/ansemjo/mksignkernels (cd mkefikeys && make -f install.mk install) (cd mksignkernels && make -f install.mk install) Signing keys \u00b6 Create a set of signing keys in /etc/efikeys : mkdir /etc/efikeys && cd /etc/efikeys mkefikeys certs der The der target is required to output DER-encoded certificates in case you need to install those in your firmware. This is the case for OVMF, i.e. KVM machines. My Thinkpad needs authenticated \u201cefi signature lists\u201d .. generate them with mkefikeys auth . Copy files required for installation to the unencrypted ESP: cp /etc/efikeys/*.cer /boot/efi Installing them in your firmware is out of the scope of this entry. Warning Do not enable Secureboot yet. We haven\u2019t signed anything yet and your system will fail to boot. Sign your kernels \u00b6 Now we need to sign the kernels. Simply running mksignkernels will probably fail with a not-so-useful error message because something will be missing. On virtual machines the Intel microcode is usually not useful and thus not present. Add an empty MICROCODE = line in /etc/mksignkernels.mk . Additionally, you\u2019ll want to use the same kernel commandline as is used for your default installation. You can get the commandline of currently running kernel from cat /proc/cmdline . # blablabla # ------- custom targets -------- MICROCODE = CMDLINE = whatever_your_default_kernel_uses Next, we need to create the output directory: mkdir /boot/efi/EFI/Linux Running mksignkernels should succeed now. Otherwise check all the prerequisites: EFI stub in /usr/lib/systemd/boot/efi/linuxx64.efi.stub Signing keys in /etc/efikeys/DatabaseKey.{key,crt} Kernel in /boot/vmlinuz-* Initramfs in corresponding /boot/initramfs-*.img Use systemd-boot \u00b6 Check that systemd-boot is installed and you are indeed running UEFI, yadda yadda .. bootctl status To install it as the default bootloader, simply issue: bootctl install To enable the selection prompt uncomment the timeout in /boot/efi/loader/loader.conf . Otherwise it directly boots the default kernel. Sign your bootloader \u00b6 Before you reboot and attempt to enable Secureboot now, you need to sign the bootloader itself: mksignkernels sign SIGN=/boot/efi/systemd/systemd-bootx64.efi mksignkernels sign SIGN=/boot/efi/BOOT/BOOTX64.EFI I actually do not know if both are necessary, I just signed both just in case. Reboot \u00b6 When you reboot you should see systemd-boot\u2019s selection prompt instead of GRUB. If that is the case, there should be an option to \u201cReboot into Firmware Setup\u201d. Do that and enable Secureboot now. If all went fine you should be able to normally boot your system now. Starting your machine via GRUB should fail though, as neither GRUB nor any of the kernels it tries to boot are signed.","title":"Secureboot"},{"location":"security/secureboot.html#secureboot","text":"These are guides to install your system with your own secureboot keys and enforce signed Linux kernels.","title":"Secureboot"},{"location":"security/secureboot.html#tools","text":"Some useful tools for this job: name description ansemjo/mkefikeys generate signing keys ansemjo/mksignkernels bundle and sign kernel images","title":"Tools"},{"location":"security/secureboot.html#guides","text":"","title":"Guides"},{"location":"security/secureboot.html#arch-linux","text":"TODO","title":"Arch Linux"},{"location":"security/secureboot.html#fedora","text":"","title":"Fedora"},{"location":"security/secureboot.html#installation","text":"Do a somewhat standard installation on an UEFI system. I used the Fedora 28 Server netinst image. During partitioning, make sure to select at least \u201ccustom\u201d partitioning and add a seperate EFI system partition in /boot/efi . Tick the boxes to encrypt your / and any swap partitions you might create. Technically, a seperate /boot partition is not required with the bundled kernels we are going to use but Anaconda complains otherwise and you will not be able to boot the system after installation. You could probably do all these steps from within a live rescue system but I haven\u2019t tried that route yet.","title":"Installation"},{"location":"security/secureboot.html#required-packages","text":"You will need to additionally (after a minimal setup) install: * git * make * binutils * sbsigntools Then clone and install the above two tools: mkefikeys and mksignkernels . cd /tmp/... git clone https://github.com/ansemjo/mkefikeys git clone https://github.com/ansemjo/mksignkernels (cd mkefikeys && make -f install.mk install) (cd mksignkernels && make -f install.mk install)","title":"Required Packages"},{"location":"security/secureboot.html#signing-keys","text":"Create a set of signing keys in /etc/efikeys : mkdir /etc/efikeys && cd /etc/efikeys mkefikeys certs der The der target is required to output DER-encoded certificates in case you need to install those in your firmware. This is the case for OVMF, i.e. KVM machines. My Thinkpad needs authenticated \u201cefi signature lists\u201d .. generate them with mkefikeys auth . Copy files required for installation to the unencrypted ESP: cp /etc/efikeys/*.cer /boot/efi Installing them in your firmware is out of the scope of this entry. Warning Do not enable Secureboot yet. We haven\u2019t signed anything yet and your system will fail to boot.","title":"Signing keys"},{"location":"security/secureboot.html#sign-your-kernels","text":"Now we need to sign the kernels. Simply running mksignkernels will probably fail with a not-so-useful error message because something will be missing. On virtual machines the Intel microcode is usually not useful and thus not present. Add an empty MICROCODE = line in /etc/mksignkernels.mk . Additionally, you\u2019ll want to use the same kernel commandline as is used for your default installation. You can get the commandline of currently running kernel from cat /proc/cmdline . # blablabla # ------- custom targets -------- MICROCODE = CMDLINE = whatever_your_default_kernel_uses Next, we need to create the output directory: mkdir /boot/efi/EFI/Linux Running mksignkernels should succeed now. Otherwise check all the prerequisites: EFI stub in /usr/lib/systemd/boot/efi/linuxx64.efi.stub Signing keys in /etc/efikeys/DatabaseKey.{key,crt} Kernel in /boot/vmlinuz-* Initramfs in corresponding /boot/initramfs-*.img","title":"Sign your kernels"},{"location":"security/secureboot.html#use-systemd-boot","text":"Check that systemd-boot is installed and you are indeed running UEFI, yadda yadda .. bootctl status To install it as the default bootloader, simply issue: bootctl install To enable the selection prompt uncomment the timeout in /boot/efi/loader/loader.conf . Otherwise it directly boots the default kernel.","title":"Use systemd-boot"},{"location":"security/secureboot.html#sign-your-bootloader","text":"Before you reboot and attempt to enable Secureboot now, you need to sign the bootloader itself: mksignkernels sign SIGN=/boot/efi/systemd/systemd-bootx64.efi mksignkernels sign SIGN=/boot/efi/BOOT/BOOTX64.EFI I actually do not know if both are necessary, I just signed both just in case.","title":"Sign your bootloader"},{"location":"security/secureboot.html#reboot","text":"When you reboot you should see systemd-boot\u2019s selection prompt instead of GRUB. If that is the case, there should be an option to \u201cReboot into Firmware Setup\u201d. Do that and enable Secureboot now. If all went fine you should be able to normally boot your system now. Starting your machine via GRUB should fail though, as neither GRUB nor any of the kernels it tries to boot are signed.","title":"Reboot"},{"location":"security/systemd-decryption-target.html","text":"Systemd Disk Decryption Target \u00b6 In order to delay most of your systems services during boot until a bunch of harddisks are decrypted but still bring up enough to allow for remote unlocking via ssh you\u2019ll need to use a systemd.target . Most of the information here is based a mail on Debian\u2019s mailinglist by Christian Seiler. crypttab entries \u00b6 Add entries for your disks to /etc/crypttab . Use - to signal interactive passwords and add noauto to avoid hanging at early boot: # format: <name> <disk> <keyfile> <options> HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHK98JS-part1 - luks,noauto HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHLZA1S-part1 - luks,noauto WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683_LUKS /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683-part1 - luks,noauto WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927_LUKS /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927-part1 - luks,noauto Systemd Units \u00b6 Then you need to add a few unit files to create proper dependencies. unlockme.target \u00b6 Mostly just copy and edit /usr/lib/systemd/system/multi-user.target : [Unit] Description=System waiting for decryption of disks Requires=basic.target Conflicts=rescue.service rescue.target After=basic.target rescue.service rescue.target AllowIsolate=yes Copy wanted symlinks from /usr/lib : mkcd /etc/systemd/system/unlockme.target.wants/ for w in /usr/lib/systemd/system/multi-user.target.wants/*; do ln -s $(readlink -f $w) done And add any services that you might require to be able to login via ssh : Warning Do not forget required networking services! ln -s /usr/lib/systemd/system/sshd.service ln -s /usr/lib/systemd/system/NetworkManager.service unlocked.target \u00b6 This target depends on all disks to be decrypted. The service\u2019s instance name is the <name> from your crypttab. [Unit] Description=Decrypted all disks Conflicts=systemd-ask-password-console.path systemd-ask-password-console.service Conflicts=systemd-ask-password-plymouth.path systemd-ask-password-plymouth.service Requires=unlockme.target \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS.service \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0284683_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0496927_LUKS.service Hint Remember to escape any names which might contain special characters in systemd\u2019s sense, i.e. run names though systemd-escape first. Note I needed a little override in /etc/systemd/system/systemd-cryptsetup@.service.d/dependencies.conf to make sure that this target properly waits for all systemd-cryptsetup@***.service units without specifying them all in After= manually: [Unit] DefaultDependencies=yes continue.service \u00b6 Add the service which depends on those targets and then kicks off the rest of the startup procedures: [Unit] Description=Continue system startup after disk decryption Requires=unlocked.target After=unlocked.target [Service] Type=oneshot ExecStart=/usr/bin/systemctl --no-block start multi-user.target Change Default Target \u00b6 Finally, change your default target to unlockme.target to use this procedure: systemctl set-default unlockme.target Now reboot, login with ssh and then start the continuation service to unlock your disks and carry on: systemctl start continue You should be asked to enter the passwords on the commandline directly. Hint If you have trouble booting after any changes, apped init=/sysroot/bin/sh to your kernel commandline!","title":"Systemd Disk Decryption Target"},{"location":"security/systemd-decryption-target.html#systemd-disk-decryption-target","text":"In order to delay most of your systems services during boot until a bunch of harddisks are decrypted but still bring up enough to allow for remote unlocking via ssh you\u2019ll need to use a systemd.target . Most of the information here is based a mail on Debian\u2019s mailinglist by Christian Seiler.","title":"Systemd Disk Decryption Target"},{"location":"security/systemd-decryption-target.html#crypttab-entries","text":"Add entries for your disks to /etc/crypttab . Use - to signal interactive passwords and add noauto to avoid hanging at early boot: # format: <name> <disk> <keyfile> <options> HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHK98JS-part1 - luks,noauto HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHLZA1S-part1 - luks,noauto WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683_LUKS /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683-part1 - luks,noauto WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927_LUKS /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927-part1 - luks,noauto","title":"crypttab entries"},{"location":"security/systemd-decryption-target.html#systemd-units","text":"Then you need to add a few unit files to create proper dependencies.","title":"Systemd Units"},{"location":"security/systemd-decryption-target.html#unlockmetarget","text":"Mostly just copy and edit /usr/lib/systemd/system/multi-user.target : [Unit] Description=System waiting for decryption of disks Requires=basic.target Conflicts=rescue.service rescue.target After=basic.target rescue.service rescue.target AllowIsolate=yes Copy wanted symlinks from /usr/lib : mkcd /etc/systemd/system/unlockme.target.wants/ for w in /usr/lib/systemd/system/multi-user.target.wants/*; do ln -s $(readlink -f $w) done And add any services that you might require to be able to login via ssh : Warning Do not forget required networking services! ln -s /usr/lib/systemd/system/sshd.service ln -s /usr/lib/systemd/system/NetworkManager.service","title":"unlockme.target"},{"location":"security/systemd-decryption-target.html#unlockedtarget","text":"This target depends on all disks to be decrypted. The service\u2019s instance name is the <name> from your crypttab. [Unit] Description=Decrypted all disks Conflicts=systemd-ask-password-console.path systemd-ask-password-console.service Conflicts=systemd-ask-password-plymouth.path systemd-ask-password-plymouth.service Requires=unlockme.target \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS.service \\ systemd-cryptsetup@HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0284683_LUKS.service \\ systemd-cryptsetup@WDC_WD40EZRX\\x2d00SPEB0_WD\\x2dWCC4E0496927_LUKS.service Hint Remember to escape any names which might contain special characters in systemd\u2019s sense, i.e. run names though systemd-escape first. Note I needed a little override in /etc/systemd/system/systemd-cryptsetup@.service.d/dependencies.conf to make sure that this target properly waits for all systemd-cryptsetup@***.service units without specifying them all in After= manually: [Unit] DefaultDependencies=yes","title":"unlocked.target"},{"location":"security/systemd-decryption-target.html#continueservice","text":"Add the service which depends on those targets and then kicks off the rest of the startup procedures: [Unit] Description=Continue system startup after disk decryption Requires=unlocked.target After=unlocked.target [Service] Type=oneshot ExecStart=/usr/bin/systemctl --no-block start multi-user.target","title":"continue.service"},{"location":"security/systemd-decryption-target.html#change-default-target","text":"Finally, change your default target to unlockme.target to use this procedure: systemctl set-default unlockme.target Now reboot, login with ssh and then start the continuation service to unlock your disks and carry on: systemctl start continue You should be asked to enter the passwords on the commandline directly. Hint If you have trouble booting after any changes, apped init=/sysroot/bin/sh to your kernel commandline!","title":"Change Default Target"},{"location":"security/zfs-in-place-encryption.html","text":"ZFS in-place Encryption \u00b6 There are basically two possibilities to encrypt an existing array\u2019s disks: perform in-place encryption of cold disks with luksipc requires that you have enough free space at the end will not work if your array is assembled from /dev/disk/by-id/* paths, since those will change iterate over hot disks by overwriting and resilvering each one will leave your array in a degraded but usable state you should overwrite the entire disk to make sure plaintext traces are removed depending on how full your array is you might need to write up to twice your raw accumulated disk size worth of data .. this takes a lot of time! Hot Encryption \u00b6 I chose to use the latter method because there was not enough space at the end of the drives and I was not sure how ZFS could handle the changed disk paths. A couple things to note: make sure you align all partitions / containers! check your real physical block size (the drive might lie) check the ashift property of your pool use partitions, do not make the LUKS partition span the entire drive! begin the first partition at a multiple of your pbs , e.g. 4096 sectors / 2 MiB is a safe bet account for the LUKS header (usually should be 2 MiB) do not enlarge your partitions by too much, so you can replace them later on however make sure that the mapped device cannot be smaller than your original partition! Example \u00b6 For example, I originally had four 7809835008 sector partitions ( fdisk uses 512 byte sectors here). Partition sectors (512 bytes) approximate size Original ZFS 7809835008 3813396 MiB Full disk 7814037168 ~ 3815447 MiB Nominal 4 TiB disk 7812500000 ~ 3814697 MiB New ZFS 7811072000 3814000 MiB New LUKS 7811076096 3814002 MiB Rinse & Repeat \u00b6 Assume the original pool looked like this: kourier mirror-0 ONLINE /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927-part2 ONLINE /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHLZA1S-part2 ONLINE mirror-1 ONLINE /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683-part2 ONLINE /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHK98JS-part2 ONLINE Take the first drive offline: export DISK=\"WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927\" zpool offline kourier ata-${DISK}-part2 Overwrite with zeroes or random data: dd if=/dev/zero of=/dev/disk/by-id/ata-${DISK} status=progress bs=1M Create a new partition table and one partition with your desired size (the UUID sets the partition type to FreeBSD ZFS ): sfdisk /dev/disk/by-id/ata-${DISK} <<EOF label: gpt start=2M size=7811076096 type=516E7CBA-6ECF-11D6-8FF8-00022D09712B EOF Create and open the LUKS container with your desired cipher / hash / keysize settings: cryptsetup luksFormat /dev/disk/by-id/ata-${DISK}-part1 cryptsetup open /dev/disk/by-id/ata-${DISK}-part1 ${DISK}_LUKS Replace the drive in the pool and wait for it to resilver: zpool replace kourier ata-${DISK}-part2 /dev/mapper/${DISK}_LUKS watch -d zpool status -P Rinse and repeat with all four disks. This is what my pool looks like now: kourier mirror-0 ONLINE /dev/mapper/WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927_LUKS ONLINE /dev/mapper/HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS ONLINE mirror-1 ONLINE /dev/mapper/WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683_LUKS ONLINE /dev/mapper/HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS ONLINE systemd Target \u00b6 My next task will be to create proper dependencies for all my systemd services. I do not want my system to block boot, so I can later login and manually decrypt the disks. However I also don\u2019t want to have services randomly fail or attempt to create nonexistent paths because the zpool is not imported yet. They should just queue and wait for me to decrypt the drives and then automatically continue once I\u2019ve done that. Useful pointers: systemd-cryptsetup@.service bundling all encrypted disks in a *.target After= , RequiredBy= / WantedBy= and BindsTo= properties of services systemd.unit(5) Note See Systemd Decryption Target for the finished result.","title":"ZFS in-place Encryption"},{"location":"security/zfs-in-place-encryption.html#zfs-in-place-encryption","text":"There are basically two possibilities to encrypt an existing array\u2019s disks: perform in-place encryption of cold disks with luksipc requires that you have enough free space at the end will not work if your array is assembled from /dev/disk/by-id/* paths, since those will change iterate over hot disks by overwriting and resilvering each one will leave your array in a degraded but usable state you should overwrite the entire disk to make sure plaintext traces are removed depending on how full your array is you might need to write up to twice your raw accumulated disk size worth of data .. this takes a lot of time!","title":"ZFS in-place Encryption"},{"location":"security/zfs-in-place-encryption.html#hot-encryption","text":"I chose to use the latter method because there was not enough space at the end of the drives and I was not sure how ZFS could handle the changed disk paths. A couple things to note: make sure you align all partitions / containers! check your real physical block size (the drive might lie) check the ashift property of your pool use partitions, do not make the LUKS partition span the entire drive! begin the first partition at a multiple of your pbs , e.g. 4096 sectors / 2 MiB is a safe bet account for the LUKS header (usually should be 2 MiB) do not enlarge your partitions by too much, so you can replace them later on however make sure that the mapped device cannot be smaller than your original partition!","title":"Hot Encryption"},{"location":"security/zfs-in-place-encryption.html#example","text":"For example, I originally had four 7809835008 sector partitions ( fdisk uses 512 byte sectors here). Partition sectors (512 bytes) approximate size Original ZFS 7809835008 3813396 MiB Full disk 7814037168 ~ 3815447 MiB Nominal 4 TiB disk 7812500000 ~ 3814697 MiB New ZFS 7811072000 3814000 MiB New LUKS 7811076096 3814002 MiB","title":"Example"},{"location":"security/zfs-in-place-encryption.html#rinse-repeat","text":"Assume the original pool looked like this: kourier mirror-0 ONLINE /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927-part2 ONLINE /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHLZA1S-part2 ONLINE mirror-1 ONLINE /dev/disk/by-id/ata-WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683-part2 ONLINE /dev/disk/by-id/ata-HGST_HDN724040ALE640_PK1334PEHK98JS-part2 ONLINE Take the first drive offline: export DISK=\"WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927\" zpool offline kourier ata-${DISK}-part2 Overwrite with zeroes or random data: dd if=/dev/zero of=/dev/disk/by-id/ata-${DISK} status=progress bs=1M Create a new partition table and one partition with your desired size (the UUID sets the partition type to FreeBSD ZFS ): sfdisk /dev/disk/by-id/ata-${DISK} <<EOF label: gpt start=2M size=7811076096 type=516E7CBA-6ECF-11D6-8FF8-00022D09712B EOF Create and open the LUKS container with your desired cipher / hash / keysize settings: cryptsetup luksFormat /dev/disk/by-id/ata-${DISK}-part1 cryptsetup open /dev/disk/by-id/ata-${DISK}-part1 ${DISK}_LUKS Replace the drive in the pool and wait for it to resilver: zpool replace kourier ata-${DISK}-part2 /dev/mapper/${DISK}_LUKS watch -d zpool status -P Rinse and repeat with all four disks. This is what my pool looks like now: kourier mirror-0 ONLINE /dev/mapper/WDC_WD40EZRX-00SPEB0_WD-WCC4E0496927_LUKS ONLINE /dev/mapper/HGST_HDN724040ALE640_PK1334PEHLZA1S_LUKS ONLINE mirror-1 ONLINE /dev/mapper/WDC_WD40EZRX-00SPEB0_WD-WCC4E0284683_LUKS ONLINE /dev/mapper/HGST_HDN724040ALE640_PK1334PEHK98JS_LUKS ONLINE","title":"Rinse &amp; Repeat"},{"location":"security/zfs-in-place-encryption.html#systemd-target","text":"My next task will be to create proper dependencies for all my systemd services. I do not want my system to block boot, so I can later login and manually decrypt the disks. However I also don\u2019t want to have services randomly fail or attempt to create nonexistent paths because the zpool is not imported yet. They should just queue and wait for me to decrypt the drives and then automatically continue once I\u2019ve done that. Useful pointers: systemd-cryptsetup@.service bundling all encrypted disks in a *.target After= , RequiredBy= / WantedBy= and BindsTo= properties of services systemd.unit(5) Note See Systemd Decryption Target for the finished result.","title":"systemd Target"},{"location":"tips/ansible.html","text":"Ansible \u00b6 Inline Vault usage \u00b6 The Ansible vault can encrypt your secrets so you can add them to your inventory files and track those in your preferred version control system. Since version 2.3, Ansible allows using encrypted values inline in an otherwise unencrypted file. Create key \u00b6 In a simple setup with a single user you my want to use a password file with a high-entropy secret inside. Just don\u2019t add that to any VCS. $ high-entropy-password-gen > ~/.ansible/vaultkey # e.g. my diceware words alias: $ words 10 - > ~/.ansible/vaultkey Edit your ansible.cfg to use that key without prompting: # If set, configures the path to the Vault password file as an alternative to # specifying --vault-password-file on the command line. vault_password_file = ~/.ansible/vaultkey Encrypt secret values \u00b6 Then use ansible-vault encrypt_string to encrypt your secrets: $ echo mysecret | ansible-vault encrypt_string Reading plaintext input from stdin. (ctrl-d to end input) !vault | $ANSIBLE_VAULT;1.1;AES256 34326362313132393835323362663331323238393837613134646465333339623034653666626633 6439616237613939393666363530626663373132616232300a346164363933613934333830613932 36356235323665346530626438313935653537333836373935313336343265343061656262396337 3832666631623739330a316363336463613530343132633765366166363532303135333736653931 62386637636532363064346134333735313737356666613233623166653239333832 Encryption successful If your secret is in the clipboard and my aliases are installed, a clipboard pipe works great: $ clipboard | ansible-vault encrypt_string | clipboard Finally paste the encrypted secret in your inventory or variable file: [...] runner.rz.semjonov.de: ansemjo_gitlab_runner_registration_token: !vault | $ANSIBLE_VAULT;1.1;AES256 35376637383563383661366562613932306437653533623461303032346566633032626435356538 3564376461343131613165386135303534666166393138650a356233333030323730666562613637 36653561396430346539373966366338633861346130623135633732383030666130393765323431 6333393837336665650a343738646135323235323331306630333465303535363530653435383532 35633834666138373661336436363963363766393236336536306134653136343064 ansemjo_gitlab_runner_registration_url: https://git.rz.semjonov.de/ [...]","title":"Ansible"},{"location":"tips/ansible.html#ansible","text":"","title":"Ansible"},{"location":"tips/ansible.html#inline-vault-usage","text":"The Ansible vault can encrypt your secrets so you can add them to your inventory files and track those in your preferred version control system. Since version 2.3, Ansible allows using encrypted values inline in an otherwise unencrypted file.","title":"Inline Vault usage"},{"location":"tips/ansible.html#create-key","text":"In a simple setup with a single user you my want to use a password file with a high-entropy secret inside. Just don\u2019t add that to any VCS. $ high-entropy-password-gen > ~/.ansible/vaultkey # e.g. my diceware words alias: $ words 10 - > ~/.ansible/vaultkey Edit your ansible.cfg to use that key without prompting: # If set, configures the path to the Vault password file as an alternative to # specifying --vault-password-file on the command line. vault_password_file = ~/.ansible/vaultkey","title":"Create key"},{"location":"tips/ansible.html#encrypt-secret-values","text":"Then use ansible-vault encrypt_string to encrypt your secrets: $ echo mysecret | ansible-vault encrypt_string Reading plaintext input from stdin. (ctrl-d to end input) !vault | $ANSIBLE_VAULT;1.1;AES256 34326362313132393835323362663331323238393837613134646465333339623034653666626633 6439616237613939393666363530626663373132616232300a346164363933613934333830613932 36356235323665346530626438313935653537333836373935313336343265343061656262396337 3832666631623739330a316363336463613530343132633765366166363532303135333736653931 62386637636532363064346134333735313737356666613233623166653239333832 Encryption successful If your secret is in the clipboard and my aliases are installed, a clipboard pipe works great: $ clipboard | ansible-vault encrypt_string | clipboard Finally paste the encrypted secret in your inventory or variable file: [...] runner.rz.semjonov.de: ansemjo_gitlab_runner_registration_token: !vault | $ANSIBLE_VAULT;1.1;AES256 35376637383563383661366562613932306437653533623461303032346566633032626435356538 3564376461343131613165386135303534666166393138650a356233333030323730666562613637 36653561396430346539373966366338633861346130623135633732383030666130393765323431 6333393837336665650a343738646135323235323331306630333465303535363530653435383532 35633834666138373661336436363963363766393236336536306134653136343064 ansemjo_gitlab_runner_registration_url: https://git.rz.semjonov.de/ [...]","title":"Encrypt secret values"},{"location":"tips/arduino.html","text":"Flashing over ISP header \u00b6 Recently I had the need to program an Arduino that was not responding over USB, i.e. the bootloader was probably broken somehow. I wrote a blog entry about that. The usual ISP header on an Arduino is mapped like this: \u250f\u2500\u2500\u2500\u2500\u2500\u256e MISO \u2502 1 2 \u2502 VCC SCK \u2502 3 4 \u2502 MOSI RST \u2502 5 6 \u2502 GND \u2570\u2500\u2500\u2500\u2500\u2500\u256f Sparkfun FTDI FT232R Breakout Board \u00b6 The FT232R provides a straightforward \u201cbit-bang\u201d mode to drive these pins. Some breakout boards come with an ISP header directly soldered on but you can also just use the breadboard pins on a full breakout. I\u2019m using a Sparkfun FT232R Breakout to do this. These are the pins of a FT232R which correspond to the above ISP header: \u250f\u2500\u2500\u2500\u2500\u2500\u256e CTS \u2502 1 2 \u2502 VCC DSR \u2502 3 4 \u2502 DCD RI \u2502 5 6 \u2502 GND \u2570\u2500\u2500\u2500\u2500\u2500\u256f On the bottom of the Sparkfun breakout the legs are mapped like this: USB \u250f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u25a0 DCD PWREN \u25a1 \u2502 \u2502 \u25a0 DSR TXDEN \u25a1 \u2502 \u2502 \u25a0 GND SLEEP \u25a1 \u2502 \u2502 \u25a0 RI CTS \u25a0 \u2502 \u2502 \u25a1 RXD V3.3 \u25a1 \u2502 \u2502 \u25a1 VCCIO VCC \u25a0 \u2502 \u2502 \u25a1 RTS RXLED \u25a1 \u2502 \u2502 \u25a1 DTR TXLED \u25a1 \u2502 \u2502 \u25a1 TXD GND \u25a1 \u2502 \u2502 \u25a1 \u25a1 \u25a1 \u25a1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f This configuration should come shipped with a decently modern avrdude version already. If it\u2019s not, here is a copy: # see http://www.geocities.jp/arduino_diecimila/bootloader/index_en.html # Note: pins are numbered from 1! programmer id = \"arduino-ft232r\"; desc = \"Arduino: FT232R connected to ISP\"; type = \"ftdi_syncbb\"; connection_type = usb; miso = 3; # CTS X3(1) sck = 5; # DSR X3(2) mosi = 6; # DCD X3(3) reset = 7; # RI X3(4) ; Using avrdude like this is said to be slower than other methods but in my testing it turned out to be decently quick \u2013 not \u201cminutes\u201d like some comments suggest anyway. avrdude -c arduino-ft232r -p m328p -v Adafruit FTDI FT232H Breakout Board \u00b6 Even better, the FT232H provides a proper MPSSE SPI interface. I mentioned above that bit-banging is said to be slower but I didn\u2019t perceive it as too bad. Oh it does make a difference! Performing a simple benchmark with successive readout and writebacks of the eeprom and flash areas on an Arduino Nano clone took 16 seconds using the FT232H while it took over two minutes on the FT232R. Looking from the top, the pins on the Adafruit board are used like this: USB \u250f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u25a0 5V C9 \u25a1 \u2502 \u2502 \u25a0 GND C8 \u25a1 \u2502 \u2502 \u25a0 D0 SCK C7 \u25a1 \u2502 \u2502 \u25a0 D1 MOSI C6 \u25a1 \u2502 \u2502 \u25a0 D2 MISO C5 \u25a1 \u2502 \u2502 \u25a0 D3 RST C4 \u25a1 \u2502 \u2502 \u25a1 D4 C3 \u25a1 \u2502 \u2502 \u25a1 D5 C2 \u25a1 \u2502 \u2502 \u25a1 D6 C1 \u25a1 \u2502 \u2502 \u25a1 D7 C0 \u25a1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f The pins are all nicely in one row, so you can easily craft a custom cable, too. The avrdude config was first described on helix.air.net.au and is now integrated in the systemwide config as programmer UM232H : # UM232H module from FTDI and Glyn.com.au. # See helix.air.net.au for detailed usage information. # /* ... */ # Use the -b flag to set the SPI clock rate eg -b 3750000 is the fastest I could get # a 16MHz Atmega1280 to program reliably. The 232H is conveniently 5V tolerant. programmer id = \"UM232H\"; desc = \"FT232H based module from FTDI and Glyn.com.au\"; type = \"avrftdi\"; usbvid = 0x0403; usbpid = 0x6014; usbdev = \"A\"; usbvendor = \"\"; usbproduct = \"\"; usbsn = \"\"; #ISP-signals sck = 0; mosi = 1; miso = 2; reset = 3; ; I\u2019ve created two straightforward programmer aliases in my ~/.avrduderc config and can use these two breakout boards with avrdude -c ft232r and avrdude -c ft232h respectively: # alias for adafruit ft232h programmer parent \"UM232H\" id = \"ft232h\"; desc = \"Adafruit FT232H based SPI programmer\"; ; # alias for sparkfun ft232r breakout programmer parent \"arduino-ft232r\" id = \"ft232r\"; desc = \"Sparkfun FT232R breakout bit-banging\"; ; Raspberry Pi \u00b6 At the time, however, I used the GPIO pins on a Raspberry Pi Zero W and amended the avrdude configuration to use bit-banging as well. Here is a possible mapping of the GPIO pins on the 40-pin header: 15 \u2506 \u00b7 \u00b7 \u2506 16 3.3V 17 \u2502 x \u00b7 \u2502 18 (GPIO 10) MOSI 19 \u2502 x x \u2502 20 GND (GPIO 09) MISO 21 \u2502 x x \u2502 22 Reset (GPIO 25) (GPIO 11) SCLK 23 \u2502 x \u00b7 \u2502 24 25 \u2506 \u00b7 \u00b7 \u2506 25 This wiring can be used with the following avrdude programmer configuration: # avr programmer via linux gpio pins programmer id = \"gpio\"; desc = \"Use the Linux sysfs to bitbang GPIO lines\"; type = \"linuxgpio\"; reset = 25; sck = 11; mosi = 10; miso = 9; ; Put that in ~/.avrduderc or a seperate file, which can be included with avrdude -C +gpio.conf ... . Now use this programmer config like this: sudo avrdude -c gpio -p m1284p -v","title":"Arduino"},{"location":"tips/arduino.html#flashing-over-isp-header","text":"Recently I had the need to program an Arduino that was not responding over USB, i.e. the bootloader was probably broken somehow. I wrote a blog entry about that. The usual ISP header on an Arduino is mapped like this: \u250f\u2500\u2500\u2500\u2500\u2500\u256e MISO \u2502 1 2 \u2502 VCC SCK \u2502 3 4 \u2502 MOSI RST \u2502 5 6 \u2502 GND \u2570\u2500\u2500\u2500\u2500\u2500\u256f","title":"Flashing over ISP header"},{"location":"tips/arduino.html#sparkfun-ftdi-ft232r-breakout-board","text":"The FT232R provides a straightforward \u201cbit-bang\u201d mode to drive these pins. Some breakout boards come with an ISP header directly soldered on but you can also just use the breadboard pins on a full breakout. I\u2019m using a Sparkfun FT232R Breakout to do this. These are the pins of a FT232R which correspond to the above ISP header: \u250f\u2500\u2500\u2500\u2500\u2500\u256e CTS \u2502 1 2 \u2502 VCC DSR \u2502 3 4 \u2502 DCD RI \u2502 5 6 \u2502 GND \u2570\u2500\u2500\u2500\u2500\u2500\u256f On the bottom of the Sparkfun breakout the legs are mapped like this: USB \u250f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u25a0 DCD PWREN \u25a1 \u2502 \u2502 \u25a0 DSR TXDEN \u25a1 \u2502 \u2502 \u25a0 GND SLEEP \u25a1 \u2502 \u2502 \u25a0 RI CTS \u25a0 \u2502 \u2502 \u25a1 RXD V3.3 \u25a1 \u2502 \u2502 \u25a1 VCCIO VCC \u25a0 \u2502 \u2502 \u25a1 RTS RXLED \u25a1 \u2502 \u2502 \u25a1 DTR TXLED \u25a1 \u2502 \u2502 \u25a1 TXD GND \u25a1 \u2502 \u2502 \u25a1 \u25a1 \u25a1 \u25a1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f This configuration should come shipped with a decently modern avrdude version already. If it\u2019s not, here is a copy: # see http://www.geocities.jp/arduino_diecimila/bootloader/index_en.html # Note: pins are numbered from 1! programmer id = \"arduino-ft232r\"; desc = \"Arduino: FT232R connected to ISP\"; type = \"ftdi_syncbb\"; connection_type = usb; miso = 3; # CTS X3(1) sck = 5; # DSR X3(2) mosi = 6; # DCD X3(3) reset = 7; # RI X3(4) ; Using avrdude like this is said to be slower than other methods but in my testing it turned out to be decently quick \u2013 not \u201cminutes\u201d like some comments suggest anyway. avrdude -c arduino-ft232r -p m328p -v","title":"Sparkfun FTDI FT232R Breakout Board"},{"location":"tips/arduino.html#adafruit-ftdi-ft232h-breakout-board","text":"Even better, the FT232H provides a proper MPSSE SPI interface. I mentioned above that bit-banging is said to be slower but I didn\u2019t perceive it as too bad. Oh it does make a difference! Performing a simple benchmark with successive readout and writebacks of the eeprom and flash areas on an Arduino Nano clone took 16 seconds using the FT232H while it took over two minutes on the FT232R. Looking from the top, the pins on the Adafruit board are used like this: USB \u250f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u25a0 5V C9 \u25a1 \u2502 \u2502 \u25a0 GND C8 \u25a1 \u2502 \u2502 \u25a0 D0 SCK C7 \u25a1 \u2502 \u2502 \u25a0 D1 MOSI C6 \u25a1 \u2502 \u2502 \u25a0 D2 MISO C5 \u25a1 \u2502 \u2502 \u25a0 D3 RST C4 \u25a1 \u2502 \u2502 \u25a1 D4 C3 \u25a1 \u2502 \u2502 \u25a1 D5 C2 \u25a1 \u2502 \u2502 \u25a1 D6 C1 \u25a1 \u2502 \u2502 \u25a1 D7 C0 \u25a1 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f The pins are all nicely in one row, so you can easily craft a custom cable, too. The avrdude config was first described on helix.air.net.au and is now integrated in the systemwide config as programmer UM232H : # UM232H module from FTDI and Glyn.com.au. # See helix.air.net.au for detailed usage information. # /* ... */ # Use the -b flag to set the SPI clock rate eg -b 3750000 is the fastest I could get # a 16MHz Atmega1280 to program reliably. The 232H is conveniently 5V tolerant. programmer id = \"UM232H\"; desc = \"FT232H based module from FTDI and Glyn.com.au\"; type = \"avrftdi\"; usbvid = 0x0403; usbpid = 0x6014; usbdev = \"A\"; usbvendor = \"\"; usbproduct = \"\"; usbsn = \"\"; #ISP-signals sck = 0; mosi = 1; miso = 2; reset = 3; ; I\u2019ve created two straightforward programmer aliases in my ~/.avrduderc config and can use these two breakout boards with avrdude -c ft232r and avrdude -c ft232h respectively: # alias for adafruit ft232h programmer parent \"UM232H\" id = \"ft232h\"; desc = \"Adafruit FT232H based SPI programmer\"; ; # alias for sparkfun ft232r breakout programmer parent \"arduino-ft232r\" id = \"ft232r\"; desc = \"Sparkfun FT232R breakout bit-banging\"; ;","title":"Adafruit FTDI FT232H Breakout Board"},{"location":"tips/arduino.html#raspberry-pi","text":"At the time, however, I used the GPIO pins on a Raspberry Pi Zero W and amended the avrdude configuration to use bit-banging as well. Here is a possible mapping of the GPIO pins on the 40-pin header: 15 \u2506 \u00b7 \u00b7 \u2506 16 3.3V 17 \u2502 x \u00b7 \u2502 18 (GPIO 10) MOSI 19 \u2502 x x \u2502 20 GND (GPIO 09) MISO 21 \u2502 x x \u2502 22 Reset (GPIO 25) (GPIO 11) SCLK 23 \u2502 x \u00b7 \u2502 24 25 \u2506 \u00b7 \u00b7 \u2506 25 This wiring can be used with the following avrdude programmer configuration: # avr programmer via linux gpio pins programmer id = \"gpio\"; desc = \"Use the Linux sysfs to bitbang GPIO lines\"; type = \"linuxgpio\"; reset = 25; sck = 11; mosi = 10; miso = 9; ; Put that in ~/.avrduderc or a seperate file, which can be included with avrdude -C +gpio.conf ... . Now use this programmer config like this: sudo avrdude -c gpio -p m1284p -v","title":"Raspberry Pi"},{"location":"tips/backblaze.html","text":"Backblaze \u00b6 Manually Sync \u00b6 For example to additionally store my GitLab backups on Backblaze: move all relevant files to a folder (optionally) encrypt all files run b2 sync . b2://bucketname/ GitLab Backups \u00b6 My GitLab backups are currently stored in a Minio S3 object storage with WORM activated but files can still be deleted by accident from the server itself. A naiive workflow would look like this: Mirror all files from Minio locally: tmp mc mirror rz/gitlab/ ./ Encrypt all files with GPG and remove plaintexts: for f in $(ls !(*.gpg)); do \\ gpg --recipient ansemjo --encrypt $f; \\ rm -fv $f; \\ done; Mirror encrypted files to Backblaze: # check file list b2 sync --dryRun --compareVersions none ./ b2://gitbacks/ # upload to backblaze b2 sync --compareVersions none ./ b2://gitbacks/ The --compareVersions none is needed since GPG does not create stable filesizes and the naiive approach obviously creates newer modification times. Unless there are problems during upload, the filename should be a stable enough distinction though.","title":"Backblaze"},{"location":"tips/backblaze.html#backblaze","text":"","title":"Backblaze"},{"location":"tips/backblaze.html#manually-sync","text":"For example to additionally store my GitLab backups on Backblaze: move all relevant files to a folder (optionally) encrypt all files run b2 sync . b2://bucketname/","title":"Manually Sync"},{"location":"tips/backblaze.html#gitlab-backups","text":"My GitLab backups are currently stored in a Minio S3 object storage with WORM activated but files can still be deleted by accident from the server itself. A naiive workflow would look like this: Mirror all files from Minio locally: tmp mc mirror rz/gitlab/ ./ Encrypt all files with GPG and remove plaintexts: for f in $(ls !(*.gpg)); do \\ gpg --recipient ansemjo --encrypt $f; \\ rm -fv $f; \\ done; Mirror encrypted files to Backblaze: # check file list b2 sync --dryRun --compareVersions none ./ b2://gitbacks/ # upload to backblaze b2 sync --compareVersions none ./ b2://gitbacks/ The --compareVersions none is needed since GPG does not create stable filesizes and the naiive approach obviously creates newer modification times. Unless there are problems during upload, the filename should be a stable enough distinction though.","title":"GitLab Backups"},{"location":"tips/bash.html","text":"Bash \u00b6 Check if a variable is defined \u00b6 The simplest approach of using if [[ -n $var ]]; then .. cannot detect if the variable has been defined but is assigned an empty value. If this distinction is important, e.g. when writing user-facing commandline scripts you can use the ${var+isdefined} expansion. For example: Check if a variable is defined: if [[ -n ${var+defined} ]]; then do something with $var fi Error if argument was not given: if [[ -z ${1+undefined} ]]; then echo \"error, argument required\" >&2 fi","title":"Bash"},{"location":"tips/bash.html#bash","text":"","title":"Bash"},{"location":"tips/bash.html#check-if-a-variable-is-defined","text":"The simplest approach of using if [[ -n $var ]]; then .. cannot detect if the variable has been defined but is assigned an empty value. If this distinction is important, e.g. when writing user-facing commandline scripts you can use the ${var+isdefined} expansion. For example: Check if a variable is defined: if [[ -n ${var+defined} ]]; then do something with $var fi Error if argument was not given: if [[ -z ${1+undefined} ]]; then echo \"error, argument required\" >&2 fi","title":"Check if a variable is defined"},{"location":"tips/bios-updates.html","text":"BIOS Updates \u00b6 Lenovo Laptops from Linux \u00b6 Some modern Lenovo machines do not have an optical disc drive. The only option for machines without Windows is a bootable .iso image though. What now? Turns out inside that image there is another bootable format: an El Torito image. You can extract that with a script called geteltorito.pl and flash it to a USB stick. # ./geteltorito.pl -o n10ur17w-usb.img n10ur17w.iso Booting catalog starts at sector: 20 Manufacturer of CD: NERO BURNING ROM Image architecture: x86 Boot media type is: harddisk El Torito image starts at sector 27 and has 47104 sector(s) of 512 Bytes Image has been written to file \"n10ur17w-usb.img\". # dd if=n10ur17w-usb.img of=/dev/sdb bs=1M 23+0 records in 23+0 records out 24117248 bytes (24 MB, 23 MiB) copied, 0.354471 s, 68.0 MB/s Note This information and script is taken from thinkwiki.de Supermicro Boards via IPMI \u00b6 On boards that include the IPMI remote management feature you can just upload the firmware file in the web interface. Easy peasy. Supermicro Boards with UEFI \u00b6 On other boards you\u2019ll need to create a bootable USB stick with the BIOS updater. With UEFI there\u2019s an easier way though. This is info and the script below come from the Thomas Krenn Wiki . create a USB stick with a FAT32 partition; it doesn\u2019t need to be bootable but using a GPT partition table and marking the partition as \u201cEFI System\u201d seems to help extract the flash script ( flash.tar ) to this partition; these files are from an X10DRI BIOS update package copy the downloaded firmware file for you motherboard to the partition reboot the server; maybe use the \u201cReboot with ME disable\u201d mode to enable updates to the Management Engine use the boot menu to enter the \u201cBuilt-in EFI Shell\u201d navigate to your USB stick; that should be fs0: \u2026 start the update with flash.nsh {updatefile}","title":"BIOS Updates"},{"location":"tips/bios-updates.html#bios-updates","text":"","title":"BIOS Updates"},{"location":"tips/bios-updates.html#lenovo-laptops-from-linux","text":"Some modern Lenovo machines do not have an optical disc drive. The only option for machines without Windows is a bootable .iso image though. What now? Turns out inside that image there is another bootable format: an El Torito image. You can extract that with a script called geteltorito.pl and flash it to a USB stick. # ./geteltorito.pl -o n10ur17w-usb.img n10ur17w.iso Booting catalog starts at sector: 20 Manufacturer of CD: NERO BURNING ROM Image architecture: x86 Boot media type is: harddisk El Torito image starts at sector 27 and has 47104 sector(s) of 512 Bytes Image has been written to file \"n10ur17w-usb.img\". # dd if=n10ur17w-usb.img of=/dev/sdb bs=1M 23+0 records in 23+0 records out 24117248 bytes (24 MB, 23 MiB) copied, 0.354471 s, 68.0 MB/s Note This information and script is taken from thinkwiki.de","title":"Lenovo Laptops from Linux"},{"location":"tips/bios-updates.html#supermicro-boards-via-ipmi","text":"On boards that include the IPMI remote management feature you can just upload the firmware file in the web interface. Easy peasy.","title":"Supermicro Boards via IPMI"},{"location":"tips/bios-updates.html#supermicro-boards-with-uefi","text":"On other boards you\u2019ll need to create a bootable USB stick with the BIOS updater. With UEFI there\u2019s an easier way though. This is info and the script below come from the Thomas Krenn Wiki . create a USB stick with a FAT32 partition; it doesn\u2019t need to be bootable but using a GPT partition table and marking the partition as \u201cEFI System\u201d seems to help extract the flash script ( flash.tar ) to this partition; these files are from an X10DRI BIOS update package copy the downloaded firmware file for you motherboard to the partition reboot the server; maybe use the \u201cReboot with ME disable\u201d mode to enable updates to the Management Engine use the boot menu to enter the \u201cBuilt-in EFI Shell\u201d navigate to your USB stick; that should be fs0: \u2026 start the update with flash.nsh {updatefile}","title":"Supermicro Boards with UEFI"},{"location":"tips/centos.html","text":"CentOS \u00b6 Custom post-transaction hooks \u00b6 Yum allows executing custom scripts with a post-action plugin. For that you need to first install the plugin and then drop your actions in /etc/yum/post-actions/*.action . yum install yum-plugin-post-transaction-actions Check that it is enabled first. You can find more information on the action usage here . A silly example executing upon any vim updates could look like this: vim*:any:bash -c \"(date; id) > /tmp/post\"","title":"CentOS"},{"location":"tips/centos.html#centos","text":"","title":"CentOS"},{"location":"tips/centos.html#custom-post-transaction-hooks","text":"Yum allows executing custom scripts with a post-action plugin. For that you need to first install the plugin and then drop your actions in /etc/yum/post-actions/*.action . yum install yum-plugin-post-transaction-actions Check that it is enabled first. You can find more information on the action usage here . A silly example executing upon any vim updates could look like this: vim*:any:bash -c \"(date; id) > /tmp/post\"","title":"Custom post-transaction hooks"},{"location":"tips/containers.html","text":"Containers \u00b6 Docker Firewalling \u00b6 By default, docker seems to start with --iptables=true everywhere. That means that the docker daemon will insert its own iptables rules to enable inter-container communication and publish ports. However that means that published ports will be published publicly by default. That means that a container started with -p 8000:8000 will be open to the world on that port. Even if your firewalld configuration does not permit this port. This is because Docker completely circumvents any firewall managers. Disable iptables tampering \u00b6 To disable this behaviour add --iptables=false to the start arguments of docker. Either do that by editing the systemd service, or set an DOCKER_OPTS=\"...\" in /etc/default/docker if applicable. $ systemctl edit docker.service [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --iptables=false This also disables the forwarding rules however. Your containers will not be able to reach the outside world anymore. To reenable the forwarding with firewalld use: $ firewall-cmd --add-masquerade --permanent Or using raw iptables rules: -A FORWARD -i docker0 -o eth0 -j ACCEPT -A FORWARD -i eth0 -o docker0 -j ACCEPT Full systemd in container \u00b6 Podman introduced some fixes that enable you running a full systemd init process inside of a rootless container. That way you can start a normal CentOS image with podman run ... centos init and login like you would in a virtual machine, enable systemd services etc. To properly login you need two small fixes however. First you need a known password. Since moust images have passwords disabled or empty for all accounts you\u2019ll need to mount an edited /etc/shadow . The following line for example sets the root password to literally password : root:$6$1xZg0v5W$XgEfFIUlHB3EIGsxJvABkytPaUITLEfTb7WocHoeFaAwBFfui2tIKZq1l/MoKtZHMQ7Q/23Dnr.qLhGfzz4VH/:18061:0:99999:7::: Another fix is required for PAM, since the console accepts your password but PAM fails to create a session for you . The fix is simple: sed -i '/^session.*pam_loginuid.so/s/^/#/' /etc/pam.d/login Mount these two files inside the container and finally start it with: podman run --rm -it -v ... centos:latest init","title":"Containers"},{"location":"tips/containers.html#containers","text":"","title":"Containers"},{"location":"tips/containers.html#docker-firewalling","text":"By default, docker seems to start with --iptables=true everywhere. That means that the docker daemon will insert its own iptables rules to enable inter-container communication and publish ports. However that means that published ports will be published publicly by default. That means that a container started with -p 8000:8000 will be open to the world on that port. Even if your firewalld configuration does not permit this port. This is because Docker completely circumvents any firewall managers.","title":"Docker Firewalling"},{"location":"tips/containers.html#disable-iptables-tampering","text":"To disable this behaviour add --iptables=false to the start arguments of docker. Either do that by editing the systemd service, or set an DOCKER_OPTS=\"...\" in /etc/default/docker if applicable. $ systemctl edit docker.service [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// --iptables=false This also disables the forwarding rules however. Your containers will not be able to reach the outside world anymore. To reenable the forwarding with firewalld use: $ firewall-cmd --add-masquerade --permanent Or using raw iptables rules: -A FORWARD -i docker0 -o eth0 -j ACCEPT -A FORWARD -i eth0 -o docker0 -j ACCEPT","title":"Disable iptables tampering"},{"location":"tips/containers.html#full-systemd-in-container","text":"Podman introduced some fixes that enable you running a full systemd init process inside of a rootless container. That way you can start a normal CentOS image with podman run ... centos init and login like you would in a virtual machine, enable systemd services etc. To properly login you need two small fixes however. First you need a known password. Since moust images have passwords disabled or empty for all accounts you\u2019ll need to mount an edited /etc/shadow . The following line for example sets the root password to literally password : root:$6$1xZg0v5W$XgEfFIUlHB3EIGsxJvABkytPaUITLEfTb7WocHoeFaAwBFfui2tIKZq1l/MoKtZHMQ7Q/23Dnr.qLhGfzz4VH/:18061:0:99999:7::: Another fix is required for PAM, since the console accepts your password but PAM fails to create a session for you . The fix is simple: sed -i '/^session.*pam_loginuid.so/s/^/#/' /etc/pam.d/login Mount these two files inside the container and finally start it with: podman run --rm -it -v ... centos:latest init","title":"Full systemd in container"},{"location":"tips/coreos.html","text":"CoreOS \u00b6 Various tricks for the super slim container OS. QEMU Guest Agent \u00b6 The guest agent qemu-ga is required for the host to discover the virtual machine\u2019s network setup, specifically it\u2019s IP. You can start the guest agent in an Alpine container: docker run -d \\ -v /dev:/dev \\ --privileged \\ --net host \\ alpine ash -c 'apk add qemu-guest-agent && exec qemu-ga -v' An alternative for very minimal machines deployed with virt-install exists, where the necessary channel needs to be created in the domain XML first. Add the following device by editing the domain definiton with virsh edit $name : <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> </channel> You\u2019ll need to fully shut the machine down and start it again. A single reboot is not enough. Now download a qemu-guest-agent package in a TAR archive. Extract the contained qemu-ga binary to /opt/bin and use the following systemd service: [Unit] Description=QEMU Guest Agent ConditionPathExists=/dev/virtio-ports/org.qemu.guest_agent.0 [Service] ExecStart=/opt/bin/qemu-ga [Install] WantedBy=multi-user.target","title":"CoreOS"},{"location":"tips/coreos.html#coreos","text":"Various tricks for the super slim container OS.","title":"CoreOS"},{"location":"tips/coreos.html#qemu-guest-agent","text":"The guest agent qemu-ga is required for the host to discover the virtual machine\u2019s network setup, specifically it\u2019s IP. You can start the guest agent in an Alpine container: docker run -d \\ -v /dev:/dev \\ --privileged \\ --net host \\ alpine ash -c 'apk add qemu-guest-agent && exec qemu-ga -v' An alternative for very minimal machines deployed with virt-install exists, where the necessary channel needs to be created in the domain XML first. Add the following device by editing the domain definiton with virsh edit $name : <channel type='unix'> <target type='virtio' name='org.qemu.guest_agent.0'/> </channel> You\u2019ll need to fully shut the machine down and start it again. A single reboot is not enough. Now download a qemu-guest-agent package in a TAR archive. Extract the contained qemu-ga binary to /opt/bin and use the following systemd service: [Unit] Description=QEMU Guest Agent ConditionPathExists=/dev/virtio-ports/org.qemu.guest_agent.0 [Service] ExecStart=/opt/bin/qemu-ga [Install] WantedBy=multi-user.target","title":"QEMU Guest Agent"},{"location":"tips/documentscan.html","text":"Document Scanning \u00b6 For my document management workflow I have settled on an Android scanner app and optical character recognition on the commandline for now. Scanbot \u00b6 The scanner app is Scanbot . It is touted as the preferred document scanner app in various articles and has a couple of advantages compared to its competitors. Among them are a nice and clean interface which is important for a quick workflow and automatic uploading to a cloud storage of your choice, including local network SFTP servers. The Pro version is required for this but it is not too expensive. OCRmyPDF \u00b6 OCR is performed on a Linux computer with ocrmypdf . This has the advantage of using a beefier CPU to do the OCR and save my smartphone battery. It also produces consistently nice results because the tesseract engine it uses is pretty awesome. On many distributions it is available as a package in the repositories. On CentOS 7 you can install it and all its dependencies with (Python 3.6 + pip required): pip install ocrmypdf yum install -y ghostscript qpdf tesseract tesseract-langpack-deu unpaper pngquant Additionally I use the following bash alias to easily perform OCR on documents in-place: ocr () { file=$1; shift 1; [[ -z $file ]] && { printf 'perform ocr on pdfs with ocrmypdf\\nusage: %s <path/to/pdf> [<extra args>]\\n' \"$0\" 1>&2; exit 1 }; ocrmypdf -cd \"$@\" \"$file\" \"$file\" } Indexing \u00b6 After some hiccups, the GNOME tracker works pretty nicely for full-text indexing of my scanned documents. If everything was indexed correctly, you can search for your documents in the GNOME Documents program or enable full-text search in Nautilus by pressing on the magnifying glass icon. Signing \u00b6 I would like to add cryptographic signatures to my PDFs but there appear to be no Linux programs capable of adding such signatures from an X.509 certificate. Regardless, my default viewer evince would not display such signatures. If I have important documents I should thereforre resort to using detached GPG signatures or regularly signing a sha256sum file.","title":"Document Scanning"},{"location":"tips/documentscan.html#document-scanning","text":"For my document management workflow I have settled on an Android scanner app and optical character recognition on the commandline for now.","title":"Document Scanning"},{"location":"tips/documentscan.html#scanbot","text":"The scanner app is Scanbot . It is touted as the preferred document scanner app in various articles and has a couple of advantages compared to its competitors. Among them are a nice and clean interface which is important for a quick workflow and automatic uploading to a cloud storage of your choice, including local network SFTP servers. The Pro version is required for this but it is not too expensive.","title":"Scanbot"},{"location":"tips/documentscan.html#ocrmypdf","text":"OCR is performed on a Linux computer with ocrmypdf . This has the advantage of using a beefier CPU to do the OCR and save my smartphone battery. It also produces consistently nice results because the tesseract engine it uses is pretty awesome. On many distributions it is available as a package in the repositories. On CentOS 7 you can install it and all its dependencies with (Python 3.6 + pip required): pip install ocrmypdf yum install -y ghostscript qpdf tesseract tesseract-langpack-deu unpaper pngquant Additionally I use the following bash alias to easily perform OCR on documents in-place: ocr () { file=$1; shift 1; [[ -z $file ]] && { printf 'perform ocr on pdfs with ocrmypdf\\nusage: %s <path/to/pdf> [<extra args>]\\n' \"$0\" 1>&2; exit 1 }; ocrmypdf -cd \"$@\" \"$file\" \"$file\" }","title":"OCRmyPDF"},{"location":"tips/documentscan.html#indexing","text":"After some hiccups, the GNOME tracker works pretty nicely for full-text indexing of my scanned documents. If everything was indexed correctly, you can search for your documents in the GNOME Documents program or enable full-text search in Nautilus by pressing on the magnifying glass icon.","title":"Indexing"},{"location":"tips/documentscan.html#signing","text":"I would like to add cryptographic signatures to my PDFs but there appear to be no Linux programs capable of adding such signatures from an X.509 certificate. Regardless, my default viewer evince would not display such signatures. If I have important documents I should thereforre resort to using detached GPG signatures or regularly signing a sha256sum file.","title":"Signing"},{"location":"tips/freeipa.html","text":"FreeIPA \u00b6 Request Certificates Manually \u00b6 You can request TLS certificates manually for hosts that are not fully enrolled in the domain or don\u2019t have any FreeIPA tools installed at all (CoreOS hosts, for example). This requires however, that you either are an admin in the domain or at least have the rights to create new hosts and service principals. First of all, create a signing request on the host: openssl req -nodes -new -newkey rsa:2048 -sha256 \\ -out test.csr -keyout test.key \\ -subj '/CN=test.example.com/' Now switch to a machine with the FreeIPA tools installed and add a host entry. You\u2019ll want to do this anyway to properly be able to set DNS records for your host. kinit admin ipa host-add test.example.com --ip-address 192.168.1.100 Now transfer the CSR to this machine and sign the request while simulateneously adding the HTTP/ service principal: ipa cert-request test.csr \\ --principal HTTP/test.example.com --add \\ --certificate-out test.crt This command will display the serial number, which can later be used to fetch information about the certificate or revoke it. Finally, just copy the test.crt back to your host and configure whatever service you want to secure with TLS.","title":"FreeIPA"},{"location":"tips/freeipa.html#freeipa","text":"","title":"FreeIPA"},{"location":"tips/freeipa.html#request-certificates-manually","text":"You can request TLS certificates manually for hosts that are not fully enrolled in the domain or don\u2019t have any FreeIPA tools installed at all (CoreOS hosts, for example). This requires however, that you either are an admin in the domain or at least have the rights to create new hosts and service principals. First of all, create a signing request on the host: openssl req -nodes -new -newkey rsa:2048 -sha256 \\ -out test.csr -keyout test.key \\ -subj '/CN=test.example.com/' Now switch to a machine with the FreeIPA tools installed and add a host entry. You\u2019ll want to do this anyway to properly be able to set DNS records for your host. kinit admin ipa host-add test.example.com --ip-address 192.168.1.100 Now transfer the CSR to this machine and sign the request while simulateneously adding the HTTP/ service principal: ipa cert-request test.csr \\ --principal HTTP/test.example.com --add \\ --certificate-out test.crt This command will display the serial number, which can later be used to fetch information about the certificate or revoke it. Finally, just copy the test.crt back to your host and configure whatever service you want to secure with TLS.","title":"Request Certificates Manually"},{"location":"tips/git.html","text":"Git \u00b6 Prevent commits on a branch \u00b6 You can use a pre-commit hook to check the branch name to which you are commiting your changes. If you want to prevent direct changes to master create the following .git/hooks/pre-commit : #!/bin/sh # prevent commits on master [ \"$(git rev-parse --abbrev-ref --symbolic-full-name HEAD)\" == \"master\" ] \\ && { echo \"you shall not commit on master\"; exit 1; } Make sure the hook script is executable. Merge Repositories \u00b6 I\u2019ve had a few situations where I started front- and backend as two seperate projects but soon wished to track both in a single repository - as subdirectories. Last time I was in this situation, a stackoverflow answer proved most helpful. Here\u2019s the gist: Assume you have two repositories frontend and backend . Prepare repositories \u00b6 First, you should move all files in each repository into a subdirectory to avoid merge conflicts and properly preserve commit history later. It would make sense to put all files in the frontend repository into a frontend subdirectory .. etc. \u2981 project/front : master= $ mkdir frontend \u2981 project/front : master= $ mv !(frontend) frontend/ \u2981 project/front : master *%= $ git add . \u2981 project/front : master += $ git commit [master c777b42] prepare frontend for merge 65 files changed, 0 insertions(+), 0 deletions(-) [...] Hint Don\u2019t forget about dotfiles, as those are not moved with mv !(frontend) frontend/ alone. Do the same analogously for the backend or any other repository you want to merge. Also, create a new repository to hold the merged projects. It helps to create an initial commit \u2013 even if completely empty \u2013 to indicate that unrelated histories were merged into each other. \u2981 project $ mkdir project && cd project \u2981 project/project $ git init \u2981 project/project : master # $ git commit --allow-empty Add and fetch remotes \u00b6 Add all prepared repositories as remotes and fetch them. \u2981 project/project : master $ git remote add -f frontend ../frontend \u2981 project/project : master $ git remote add -f backend ../backend ... Merge the repositories \u00b6 Finally, merge all those remotes into the combined project. Simply repeat this step for every remote you want to merge. At this point it pays off to prepare the repositories in order to avoid any merge conflicts right away. \u2981 project/project : master $ git merge -s ours --no-commit --allow-unrelated-histories frontend/master Automatic merge went well; stopped before committing as requested \u2981 project/project : master|MERGING $ git read-tree --prefix= -u frontend/master \u2981 project/project : master +|MERGING $ git commit -a [master 96012d4] Merge remote-tracking branch 'frontend/master' You will end up with a repository where history looks somewhat like this: \u2981 project/project : master $ git log --graph * 7a53ff5 2019-02-19 11:28:15 +0100 N Merge remote-tracking branch 'backend/master' (HEAD -> master) [Anton Semjonov] |\\ | * f89bfa3 2019-02-18 16:57:07 +0100 N prepare backend for merge (backend/master) [Anton Semjonov] | * ea8a4f7 2018-09-18 15:19:16 +0200 N update scripts [Anton Semjonov] | [...] * 96012d4 2019-02-19 11:27:49 +0100 N Merge remote-tracking branch 'frontend/master' [Anton Semjonov] |\\ | * c777b42 2019-02-18 16:57:56 +0100 N prepare frontend for merge (frontend/master) [Anton Semjonov] | * e3812e2 2019-01-18 22:35:30 +0100 N korrigiere Leonhard [Anton Semjonov] | [...] * b0605af 2019-02-19 11:25:36 +0100 N prepare combined repository for project [Anton Semjonov]","title":"Git"},{"location":"tips/git.html#git","text":"","title":"Git"},{"location":"tips/git.html#prevent-commits-on-a-branch","text":"You can use a pre-commit hook to check the branch name to which you are commiting your changes. If you want to prevent direct changes to master create the following .git/hooks/pre-commit : #!/bin/sh # prevent commits on master [ \"$(git rev-parse --abbrev-ref --symbolic-full-name HEAD)\" == \"master\" ] \\ && { echo \"you shall not commit on master\"; exit 1; } Make sure the hook script is executable.","title":"Prevent commits on a branch"},{"location":"tips/git.html#merge-repositories","text":"I\u2019ve had a few situations where I started front- and backend as two seperate projects but soon wished to track both in a single repository - as subdirectories. Last time I was in this situation, a stackoverflow answer proved most helpful. Here\u2019s the gist: Assume you have two repositories frontend and backend .","title":"Merge Repositories"},{"location":"tips/git.html#prepare-repositories","text":"First, you should move all files in each repository into a subdirectory to avoid merge conflicts and properly preserve commit history later. It would make sense to put all files in the frontend repository into a frontend subdirectory .. etc. \u2981 project/front : master= $ mkdir frontend \u2981 project/front : master= $ mv !(frontend) frontend/ \u2981 project/front : master *%= $ git add . \u2981 project/front : master += $ git commit [master c777b42] prepare frontend for merge 65 files changed, 0 insertions(+), 0 deletions(-) [...] Hint Don\u2019t forget about dotfiles, as those are not moved with mv !(frontend) frontend/ alone. Do the same analogously for the backend or any other repository you want to merge. Also, create a new repository to hold the merged projects. It helps to create an initial commit \u2013 even if completely empty \u2013 to indicate that unrelated histories were merged into each other. \u2981 project $ mkdir project && cd project \u2981 project/project $ git init \u2981 project/project : master # $ git commit --allow-empty","title":"Prepare repositories"},{"location":"tips/git.html#add-and-fetch-remotes","text":"Add all prepared repositories as remotes and fetch them. \u2981 project/project : master $ git remote add -f frontend ../frontend \u2981 project/project : master $ git remote add -f backend ../backend ...","title":"Add and fetch remotes"},{"location":"tips/git.html#merge-the-repositories","text":"Finally, merge all those remotes into the combined project. Simply repeat this step for every remote you want to merge. At this point it pays off to prepare the repositories in order to avoid any merge conflicts right away. \u2981 project/project : master $ git merge -s ours --no-commit --allow-unrelated-histories frontend/master Automatic merge went well; stopped before committing as requested \u2981 project/project : master|MERGING $ git read-tree --prefix= -u frontend/master \u2981 project/project : master +|MERGING $ git commit -a [master 96012d4] Merge remote-tracking branch 'frontend/master' You will end up with a repository where history looks somewhat like this: \u2981 project/project : master $ git log --graph * 7a53ff5 2019-02-19 11:28:15 +0100 N Merge remote-tracking branch 'backend/master' (HEAD -> master) [Anton Semjonov] |\\ | * f89bfa3 2019-02-18 16:57:07 +0100 N prepare backend for merge (backend/master) [Anton Semjonov] | * ea8a4f7 2018-09-18 15:19:16 +0200 N update scripts [Anton Semjonov] | [...] * 96012d4 2019-02-19 11:27:49 +0100 N Merge remote-tracking branch 'frontend/master' [Anton Semjonov] |\\ | * c777b42 2019-02-18 16:57:56 +0100 N prepare frontend for merge (frontend/master) [Anton Semjonov] | * e3812e2 2019-01-18 22:35:30 +0100 N korrigiere Leonhard [Anton Semjonov] | [...] * b0605af 2019-02-19 11:25:36 +0100 N prepare combined repository for project [Anton Semjonov]","title":"Merge the repositories"},{"location":"tips/gitlab.html","text":"Gitlab \u00b6 Gitlab Runner in QEMU/KVM \u00b6 First deploy CoreOS in a virtual machine . Then deploy the Gitlab Runner as a Docker container itself. Following the documentation : docker run -d --name runner --restart always \\ -v /etc/gitlab-runner:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:alpine docker exec -it runner register Hint You may need to install your CA certificate both on the CoreOS VM as well as in the runner configuration first: scp /etc/ipa/ca.crt runner: ssh runner sudo mv ca.crt /etc/ssl/certs/my-ca.pem sudo update-ca-certificates sudo mkdir -p /etc/gitlab-runner/certs sudo cp /etc/ssl/certs/my-ca.pem /etc/gitlab-runner/certs/ca.crt Reboot the VM and/or restart the Docker service afterwards. Gitlab API \u00b6 Official Documentation is available with all the v4 API routes. Bash Alias \u00b6 A useful bash alias for httpie to interact with the GitLab API: gitlab() { meth=${1:?http method}; api=${2:?api path}; shift 2; http --check-status \\ \"$meth\" \"https://git.rz.semjonov.de/api/v4/$api\" \\ private-token:\"$TOKEN\" \\ \"$@\"; } Then export your personal access token to env: read TOKEN && export TOKEN Usage \u00b6 Chained to jq , the usage becomes: $ gitlab GET projects | jq 'map(.name)' [ \"deploy\", \"bookstack\", \"preseedinjector\", \"frontend\", \"sbupdate\", \"...\" ] $ gitlab PUT projects/11 wiki_enabled=false HTTP/1.1 200 OK Cache-Control: max-age=0, private, must-revalidate Connection: keep-alive Content-Length: 2022 Content-Type: application/json Date: Fri, 27 Jul 2018 13:41:41 GMT ... Examples \u00b6 Get the Wiki Status \u00b6 A stupid loop to get the wiki_enabled status of projects: for i in {1..116}; do project=$(gitlab GET projects/$i 2>/dev/null) \\ && wiki=$(jq .wiki_enabled <<<\"$project\") \\ && path=$(jq .path_with_namespace <<<\"$project\") \\ && echo \"$i $path: $wiki\"; done","title":"Gitlab"},{"location":"tips/gitlab.html#gitlab","text":"","title":"Gitlab"},{"location":"tips/gitlab.html#gitlab-runner-in-qemukvm","text":"First deploy CoreOS in a virtual machine . Then deploy the Gitlab Runner as a Docker container itself. Following the documentation : docker run -d --name runner --restart always \\ -v /etc/gitlab-runner:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:alpine docker exec -it runner register Hint You may need to install your CA certificate both on the CoreOS VM as well as in the runner configuration first: scp /etc/ipa/ca.crt runner: ssh runner sudo mv ca.crt /etc/ssl/certs/my-ca.pem sudo update-ca-certificates sudo mkdir -p /etc/gitlab-runner/certs sudo cp /etc/ssl/certs/my-ca.pem /etc/gitlab-runner/certs/ca.crt Reboot the VM and/or restart the Docker service afterwards.","title":"Gitlab Runner in QEMU/KVM"},{"location":"tips/gitlab.html#gitlab-api","text":"Official Documentation is available with all the v4 API routes.","title":"Gitlab API"},{"location":"tips/gitlab.html#bash-alias","text":"A useful bash alias for httpie to interact with the GitLab API: gitlab() { meth=${1:?http method}; api=${2:?api path}; shift 2; http --check-status \\ \"$meth\" \"https://git.rz.semjonov.de/api/v4/$api\" \\ private-token:\"$TOKEN\" \\ \"$@\"; } Then export your personal access token to env: read TOKEN && export TOKEN","title":"Bash Alias"},{"location":"tips/gitlab.html#usage","text":"Chained to jq , the usage becomes: $ gitlab GET projects | jq 'map(.name)' [ \"deploy\", \"bookstack\", \"preseedinjector\", \"frontend\", \"sbupdate\", \"...\" ] $ gitlab PUT projects/11 wiki_enabled=false HTTP/1.1 200 OK Cache-Control: max-age=0, private, must-revalidate Connection: keep-alive Content-Length: 2022 Content-Type: application/json Date: Fri, 27 Jul 2018 13:41:41 GMT ...","title":"Usage"},{"location":"tips/gitlab.html#examples","text":"","title":"Examples"},{"location":"tips/gitlab.html#get-the-wiki-status","text":"A stupid loop to get the wiki_enabled status of projects: for i in {1..116}; do project=$(gitlab GET projects/$i 2>/dev/null) \\ && wiki=$(jq .wiki_enabled <<<\"$project\") \\ && path=$(jq .path_with_namespace <<<\"$project\") \\ && echo \"$i $path: $wiki\"; done","title":"Get the Wiki Status"},{"location":"tips/golang.html","text":"Go \u00b6 Create a super minimal FROM scratch contianer image \u00b6 Go applications can be statically compiled to run completely standalone with no supporting Linux filesystem present whatsoever. Strip and compress the binary and you\u2019ll have an image barely larger than busybox. Compile your application statically and with stripped symbols. The necessary command wil vary depending on your project\u2019s complexity. But for small projects something like this will do: CGO_ENABLED=0 go build -ldflags='-s -w' -o main Optionally compress the binary with upx . Check that it still runs! Sometimes this will break binaries. upx main If your application makes HTTP requests to TLS endpoints you\u2019ll want a copy of the SystemCertPool . See root_linux.go for a list of files which are searched for by default on a Linux system. cp /etc/ssl/certs/ca-certificates.crt . Create a simple Dockerfile to create an image \u201cfrom scratch\u201d: FROM scratch COPY main /main COPY ca-certificates.crt /etc/ssl/certs/ca-certificates.crt ENTRYPOINT [\"/main\"] Build the image as usual with podman build -t test . . My test image clocked in at a mere 2.87 MiB and was only 6 KiB larger than the binary and certificate store combined.","title":"Go"},{"location":"tips/golang.html#go","text":"","title":"Go"},{"location":"tips/golang.html#create-a-super-minimal-from-scratch-contianer-image","text":"Go applications can be statically compiled to run completely standalone with no supporting Linux filesystem present whatsoever. Strip and compress the binary and you\u2019ll have an image barely larger than busybox. Compile your application statically and with stripped symbols. The necessary command wil vary depending on your project\u2019s complexity. But for small projects something like this will do: CGO_ENABLED=0 go build -ldflags='-s -w' -o main Optionally compress the binary with upx . Check that it still runs! Sometimes this will break binaries. upx main If your application makes HTTP requests to TLS endpoints you\u2019ll want a copy of the SystemCertPool . See root_linux.go for a list of files which are searched for by default on a Linux system. cp /etc/ssl/certs/ca-certificates.crt . Create a simple Dockerfile to create an image \u201cfrom scratch\u201d: FROM scratch COPY main /main COPY ca-certificates.crt /etc/ssl/certs/ca-certificates.crt ENTRYPOINT [\"/main\"] Build the image as usual with podman build -t test . . My test image clocked in at a mere 2.87 MiB and was only 6 KiB larger than the binary and certificate store combined.","title":"Create a super minimal FROM scratch contianer image"},{"location":"tips/openwrt.html","text":"OpenWRT \u00b6 Image Builder \u00b6 The Image Builder (previously called the Image Generator) is a pre-compiled environment suitable for creating custom images without the need for compiling them from source. It downloads pre-compiled packages and integrates them in a single flashable image. Look for the openwrt-imagebuilder-<target>-<type>.Linux-x86_64.tar.xz in the firmware image folder for your device. Download and extract it somewhere. Get a list of available profiles with make info . For my TP-Link Archer C7 v2: imagebuilder: openwrt-imagebuilder-ar71xx-generic.Linux-x86_64.tar.xz profile: archer-c7-v2 You can include extra packages by configuring PACKAGES= . make image PROFILE=\"archer-c7-v2\" PACKAGES=\"-ppp -ppp-mod-pppoe luci-ssl wireguard\" The result will be stored in ./bin/targets/<target>/<type>/ . Link I wrote a small script to automate these steps for my Archer C7 v2, so I can quickly build a new snapshot firmware.","title":"OpenWRT"},{"location":"tips/openwrt.html#openwrt","text":"","title":"OpenWRT"},{"location":"tips/openwrt.html#image-builder","text":"The Image Builder (previously called the Image Generator) is a pre-compiled environment suitable for creating custom images without the need for compiling them from source. It downloads pre-compiled packages and integrates them in a single flashable image. Look for the openwrt-imagebuilder-<target>-<type>.Linux-x86_64.tar.xz in the firmware image folder for your device. Download and extract it somewhere. Get a list of available profiles with make info . For my TP-Link Archer C7 v2: imagebuilder: openwrt-imagebuilder-ar71xx-generic.Linux-x86_64.tar.xz profile: archer-c7-v2 You can include extra packages by configuring PACKAGES= . make image PROFILE=\"archer-c7-v2\" PACKAGES=\"-ppp -ppp-mod-pppoe luci-ssl wireguard\" The result will be stored in ./bin/targets/<target>/<type>/ . Link I wrote a small script to automate these steps for my Archer C7 v2, so I can quickly build a new snapshot firmware.","title":"Image Builder"},{"location":"tips/python.html","text":"Python \u00b6 Embedding version information in packages with setuptools \u00b6 TODO, see https://github.com/ansemjo/tinyssh-keyconvert/compare/0.3.1\u20260.3.2 Basically: use version.sh script modify release seperator to \u2018-dev\u2019 use only \u2018version\u2019 output, not \u2018describe\u2019 to conform to pep 440 read version when packaging with a subprocess cmd, use directly in hash write that version into a simple file that exports __version__ in your package in your script try to import said __version__ in a try-except-clause and fallback now you can use the same version in the script, yay","title":"Python"},{"location":"tips/python.html#python","text":"","title":"Python"},{"location":"tips/python.html#embedding-version-information-in-packages-with-setuptools","text":"TODO, see https://github.com/ansemjo/tinyssh-keyconvert/compare/0.3.1\u20260.3.2 Basically: use version.sh script modify release seperator to \u2018-dev\u2019 use only \u2018version\u2019 output, not \u2018describe\u2019 to conform to pep 440 read version when packaging with a subprocess cmd, use directly in hash write that version into a simple file that exports __version__ in your package in your script try to import said __version__ in a try-except-clause and fallback now you can use the same version in the script, yay","title":"Embedding version information in packages with setuptools"},{"location":"tips/restic.html","text":"Restic \u00b6 Most of these tips are assuming that you have your RESTIC_REPOSITORY and necessary API keys, e.g. B2_ACCOUNT_{ID|KEY} , set in your environment. This enables you to simply use restic [command] instead of specifying the repository with -r <repo> and entering the password interactively. Restore a single file \u00b6 List your snapshots: restic snapshots Fetch a list of files within a snapshot or search for one: restic ls -l {latest|snapshot-id} restic find Restore a single file matching a pattern: restic restore \\ --target /restore/path \\ --include filename_or_pattern \\ {latest|snapshot-id} Mount and browse a snapshot \u00b6 Or mount a snapshot and browse inside interactively: restic mount /mount/path ls -la /mount/path","title":"Restic"},{"location":"tips/restic.html#restic","text":"Most of these tips are assuming that you have your RESTIC_REPOSITORY and necessary API keys, e.g. B2_ACCOUNT_{ID|KEY} , set in your environment. This enables you to simply use restic [command] instead of specifying the repository with -r <repo> and entering the password interactively.","title":"Restic"},{"location":"tips/restic.html#restore-a-single-file","text":"List your snapshots: restic snapshots Fetch a list of files within a snapshot or search for one: restic ls -l {latest|snapshot-id} restic find Restore a single file matching a pattern: restic restore \\ --target /restore/path \\ --include filename_or_pattern \\ {latest|snapshot-id}","title":"Restore a single file"},{"location":"tips/restic.html#mount-and-browse-a-snapshot","text":"Or mount a snapshot and browse inside interactively: restic mount /mount/path ls -la /mount/path","title":"Mount and browse a snapshot"},{"location":"tips/serialdevices.html","text":"Serial \u00b6 picocom autocompletion \u00b6 Here is a simple config I put in my .bashrc to enable useful defaults and autocompletion for picocom : # picocom config for usb serial if iscommand picocom; then alias picocom='picocom --baud 115200 --omap delbs --quiet' _picocom_serials() { COMPREPLY=($(compgen -W \"$(ls /dev/{ttyUSB,serial/by-{path,id}/}* 2>/dev/null)\" \"${COMP_WORDS[1]}\")); } complete -F _picocom_serials picocom fi udev rule to create symlink for devices \u00b6 As soon as you regularly have more than one USB serial adapter, for example a Startech RS232 cable and a Sparkfun FTDI breakout board \u2026, the /dev/ttyUSB* naming gets frustrating. There\u2019s symlinks in /dev/serial/by-id/* which are stable. But do you really want to remember the serial yourself? A page in the siduction wiki describes how you can create symlinks with udev rules. Find your device\u2019s attributes with udevadm info --attribute-walk --name=/dev/ttyUSB* . For example my Startech RS232 adapter lists: /* ... */ looking at device '[...]/ttyUSB0/tty/ttyUSB0': KERNEL==\"ttyUSB0\" SUBSYSTEM==\"tty\" DRIVER==\"\" /* ... */ looking at parent device '/devices/pci0000:00/0000:00:14.0/usb1/1-3/1-3.3': KERNELS==\"1-3.3\" SUBSYSTEMS==\"usb\" DRIVERS==\"usb\" /* ... */ ATTRS{idProduct}==\"6001\" ATTRS{idVendor}==\"0403\" /* ... */ ATTRS{manufacturer}==\"FTDI\" ATTRS{product}==\"FT232R USB UART\" ATTRS{serial}==\"AI05A7NY\" /* ... */ /* ... */ Create a file in /etc/udev/rules.d/ named like 20-serial-mydev.rules with the following content to create a custom symlink whenever the matching device is attached: # create an alias for the startech rs232 serial cable SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"0403\", ATTRS{idProduct}==\"6001\", ATTRS{serial}==\"AI05A7NY\", SYMLINK+=\"ttyUSBStartechRS232\" Use picocom with the new alias: picocom /dev/ttyUSBStartechRS232","title":"Serial"},{"location":"tips/serialdevices.html#serial","text":"","title":"Serial"},{"location":"tips/serialdevices.html#picocom-autocompletion","text":"Here is a simple config I put in my .bashrc to enable useful defaults and autocompletion for picocom : # picocom config for usb serial if iscommand picocom; then alias picocom='picocom --baud 115200 --omap delbs --quiet' _picocom_serials() { COMPREPLY=($(compgen -W \"$(ls /dev/{ttyUSB,serial/by-{path,id}/}* 2>/dev/null)\" \"${COMP_WORDS[1]}\")); } complete -F _picocom_serials picocom fi","title":"picocom autocompletion"},{"location":"tips/serialdevices.html#udev-rule-to-create-symlink-for-devices","text":"As soon as you regularly have more than one USB serial adapter, for example a Startech RS232 cable and a Sparkfun FTDI breakout board \u2026, the /dev/ttyUSB* naming gets frustrating. There\u2019s symlinks in /dev/serial/by-id/* which are stable. But do you really want to remember the serial yourself? A page in the siduction wiki describes how you can create symlinks with udev rules. Find your device\u2019s attributes with udevadm info --attribute-walk --name=/dev/ttyUSB* . For example my Startech RS232 adapter lists: /* ... */ looking at device '[...]/ttyUSB0/tty/ttyUSB0': KERNEL==\"ttyUSB0\" SUBSYSTEM==\"tty\" DRIVER==\"\" /* ... */ looking at parent device '/devices/pci0000:00/0000:00:14.0/usb1/1-3/1-3.3': KERNELS==\"1-3.3\" SUBSYSTEMS==\"usb\" DRIVERS==\"usb\" /* ... */ ATTRS{idProduct}==\"6001\" ATTRS{idVendor}==\"0403\" /* ... */ ATTRS{manufacturer}==\"FTDI\" ATTRS{product}==\"FT232R USB UART\" ATTRS{serial}==\"AI05A7NY\" /* ... */ /* ... */ Create a file in /etc/udev/rules.d/ named like 20-serial-mydev.rules with the following content to create a custom symlink whenever the matching device is attached: # create an alias for the startech rs232 serial cable SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"0403\", ATTRS{idProduct}==\"6001\", ATTRS{serial}==\"AI05A7NY\", SYMLINK+=\"ttyUSBStartechRS232\" Use picocom with the new alias: picocom /dev/ttyUSBStartechRS232","title":"udev rule to create symlink for devices"},{"location":"tips/tor.html","text":"Tor \u00b6 Local SOCKS Proxy \u00b6 Running a local Tor client / proxy is currently the default if none of ORPort , DirPort or ControlPort are defined. Since most distributions should ship a default config with those commented out you only need to download / install tor and start it. A possible obstacle is the configured DataDirectory if you want to run it as a user. In this case use this simple configuration: SocksPort 9050 Log notice stderr DataDirectory ~/.local/share/tor Start tor with tor -f ~/.config/torrc or whereever you saved that config.","title":"Tor"},{"location":"tips/tor.html#tor","text":"","title":"Tor"},{"location":"tips/tor.html#local-socks-proxy","text":"Running a local Tor client / proxy is currently the default if none of ORPort , DirPort or ControlPort are defined. Since most distributions should ship a default config with those commented out you only need to download / install tor and start it. A possible obstacle is the configured DataDirectory if you want to run it as a user. In this case use this simple configuration: SocksPort 9050 Log notice stderr DataDirectory ~/.local/share/tor Start tor with tor -f ~/.config/torrc or whereever you saved that config.","title":"Local SOCKS Proxy"}]}